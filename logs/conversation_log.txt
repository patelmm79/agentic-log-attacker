[2025-10-14 22:26:03] User: for cloud run service vllm-gemma-3-1b-it, can you tell if the performance has improved from yesterday's initial requests to today's?
[2025-10-14 22:26:03] Bot: Based on the logs, here is an analysis of the performance of the `vllm-gemma-3-1b-it` service.

First, it's important to note that all the provided logs are from a single day, **October 14, 2025**. Therefore, this analysis compares the performance of requests made earlier in the day to those made later in the day.

### Performance Analysis:

**Latency:**

*   An earlier request to `/v1/chat/completions` at `17:58:00 UTC` had a latency of **~5.3 seconds**.
*   A later request to the same endpoint at `19:17:45 UTC` had a latency of **~6.3 seconds**.
*   An earlier request to `/v1/completions` at `17:58:18 UTC` had a latency of **~11.0 seconds**.
*   A later request to the same endpoint at `19:17:58 UTC` had a latency of **~11.1 seconds**.

**Throughput:**

The logs show a noticeable improvement in generation throughput over time:

*   At `17:58:19 UTC`, the average generation throughput was **108.1 tokens/s**.
*   At `19:17:59 UTC`, the average generation throughput increased to **147.4 tokens/s**.

### Summary:

While individual request latencies remained relatively stable, the **average generation throughput improved significantly**, increasing by over 36% from ~108 tokens/s to ~147 tokens/s between the earlier and later request batches. This indicates that the service became more efficient at processing generation tasks as it warmed up.
--------------------
[2025-10-14 22:27:23] User: is there a way to execute or cache the "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): " part during cloud build stage or during the first cold start, so that subsequent cold starts can be shorter?
[2025-10-14 22:27:23] Bot: Excellent question. This gets to the heart of optimizing cold start performance for GPU-based services on platforms like Cloud Run.

Based on the logs and how these technologies work, here's a detailed breakdown:

### Short Answer

Unfortunately, you **cannot** cache the CUDA graph capture during the Cloud Build stage, and caching it from one cold start to another is not possible with the standard Cloud Run ephemeral filesystem. However, you can significantly reduce or eliminate this latency for subsequent requests using other strategies.

Let's break down why.

### What is "Capturing CUDA Graphs"?

This log message from vLLM is indicating a critical performance optimization.

1.  **What it is:** Instead of the CPU sending thousands of individual commands to the GPU for each step of the model's execution, CUDA graphs allow the application to define a whole sequence of operations (a "graph") once.
2.  **How it works:** During this "capturing" phase, vLLM runs the model with various dummy inputs to trace all the GPU operations. It then compiles this entire sequence into a single, static graph.
3.  **The Benefit:** For subsequent requests, the CPU just has to tell the GPU, "Execute this pre-compiled graph," which dramatically reduces the CPU-to-GPU communication overhead and speeds up inference.
4.  **The Cost:** This initial tracing and compilation process takes time, as you've noticed in your startup logs. Looking at your logs, this process takes around 3 seconds.

```
INFO 10-14 12:17:46 [gpu_model_runner.py:2708] Graph capturing finished in 3 secs, took 1.64 GiB
```

### Why Can't This Be Cached?

**1. During the Cloud Build Stage:**

*   **No GPU Access:** The Cloud Build environment does not have access to the specific GPU hardware (like the NVIDIA L4) that your Cloud Run service is configured to use.
*   **Hardware Specificity:** CUDA graphs are highly specific to the exact GPU architecture they are compiled for. A graph compiled for one type of GPU won't work on another. Since the build environment is separate from the serving environment, there is no way to pre-compile the graph for the target hardware. The output of Cloud Build is a portable container image, not a hardware-specific runtime artifact.

**2. Between Cold Starts:**

*   **Ephemeral Instances:** Each Cloud Run instance is ephemeral. When an instance starts up (a cold start), it gets a fresh, clean filesystem based on your container image. Any files created or data cached in memory (like the captured CUDA graph) during its runtime are completely lost when the instance is shut down due to scaling to zero.
*   **A New Beginning:** The next request that triggers a new cold start will launch a brand new instance that has no knowledge of any previous instances. It must therefore perform the entire startup routine again, including capturing the CUDA graphs.

### Solutions to Mitigate Cold Start Latency

While you can't cache the CUDA graph *between* cold starts, you can prevent or shorten the cold start process itself.

**1. The Most Effective Solution: Set Minimum Instances**

The most direct way to eliminate cold starts for your users is to configure your Cloud Run service to always have at least one instance running.

*   **How it works:** By setting `--min-instances=1`, you instruct Cloud Run to keep one container instance "warm" and ready at all times. This instance will go through the entire startup process once (including model loading and CUDA graph capture) and then stay active.
*   **Result:** All incoming requests will be directed to this warm instance, completely bypassing the cold start latency.
*   **Trade-off:** This has cost implications, as you will be billed for the time the instance is idle, waiting for requests. For a production service where consistent low latency is critical, this is often a necessary trade-off.

**2. Caching the `torch.compile` Step (Partial Improvement)**

Your logs show two distinct time-consuming phases during startup: `torch.compile` and the CUDA graph capture.

*   `torch.compile` can take a significant amount of time (your logs show it taking ~55 seconds).
    ```
    INFO 10-14 12:16:38 [monitor.py:34] torch.compile takes 54.25 s in total
    ```
*   vLLM attempts to cache the results of this compilation in a directory. Your logs show this:
    ```
    INFO 10-14 12:15:36 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/...
    ```

You could potentially speed up this part of the cold start by mounting a persistent, shared filesystem (like Cloud Filestore with a Gen2 execution environment) to that cache directory. This way, the compiled artifacts would be persisted across instance startups.

*   **Benefit:** This would save the ~55 seconds of `torch.compile` time on subsequent cold starts.
*   **Limitation:** It would **not** save the time for the CUDA graph capture (~3 seconds) or the model loading time, as those are still instance-specific runtime operations. This is a partial optimization that adds complexity (managing a Filestore instance).

### Summary & Recommendation

| Method                           | What it Caches/Solves                                 | Possible with Cloud Run? | Recommendation                                                                                                          |
| -------------------------------- | ----------------------------------------------------- | ------------------------ | ----------------------------------------------------------------------------------------------------------------------- |
| Caching in Cloud Build           | CUDA Graph                                            | **No**                   | Not feasible due to hardware-specific nature of CUDA graphs.                                                            |
| Caching Between Cold Starts      | CUDA Graph                                            | **No**                   | Not possible due to the ephemeral nature of Cloud Run instances.                                                        |
| **Set `min-instances=1`**        | **Eliminates cold starts entirely** (after first one) | **Yes**                  | **This is the recommended approach** for production services that require consistently low latency.                     |
| Mount Persistent Filesystem      | `torch.compile` artifacts                             | **Yes** (Advanced)       | A partial optimization. It can reduce startup time but adds complexity and cost, and won't eliminate the entire cold start. |

For your use case, the simplest and most effective way to ensure that the CUDA graphs are always "cached" and ready is to **set `min-instances` to 1**.
--------------------
[2025-10-14 22:28:43] User: another llm recommended this approach, could it work? You can solve this by providing a shared, persistent filesystem that all instances of your Cloud Run service can mount and write to.

Hereâ€™s the high-level workflow:

Set up a Network Filesystem: Create a Cloud Filestore (NFS) instance in the same VPC network as your Cloud Run service. This provides a shared drive that multiple instances can read from and write to simultaneously.
Mount the Filesystem in Cloud Run: When deploying your Cloud Run service, configure it to mount the Filestore instance to a specific path within the container (e.g., /mnt/cache).
Point vLLM's Cache to the Mount: Use an environment variable to tell torch.compile (which vLLM uses) to write its cache files to the mounted shared directory instead of the default local path. The environment variable for this is typically TORCH_COMPILE_CACHE_DIR. You would set the environment variable in your Cloud Run service to:
TORCH_COMPILE_CACHE_DIR=/mnt/cache/torch_compile_cache
What Happens with This Setup:
The Very First Cold Start: The first instance of a new revision (...-00036-2dd) will start up, find no existing cache in /mnt/cache, and perform the full "Capturing CUDA graphs" process. This will still take ~160 seconds as seen in your logs. However, it will write the resulting cache files to the shared Filestore drive.
All Subsequent Cold Starts: Any new instance that starts due to autoscaling will mount the same Filestore drive. When vLLM initializes, it will check the cache directory (/mnt/cache/torch_compile_cache), find the pre-compiled graphs, and load them directly, skipping the time-consuming capture process.
This approach effectively makes the graph capture a one-time cost per deployment, allowing all subsequent scale-out events to be significantly faster. While they will still have to load the model into GPU memory, they will skip the ~60-90 seconds spent on torch.compile and graph capture.
[2025-10-14 22:28:43] Bot: No logs found for the specified service and time range.
--------------------
[2025-10-14 22:29:56] User: please create a pull request for the . Caching the torch.compile Step (Partial Improvement)  recommendation into github repository vllm-container-prewarm
[2025-10-14 22:29:56] Bot: No logs found for the specified service and time range.
--------------------
[2025-10-14 22:35:44] User: for cloud run service vllm-gemma-3-1b-it, can you tell if the performance has improved from yesterday's initial requests to today's?
[2025-10-14 22:35:44] Bot: Based on the logs, there has been a slight improvement in the initial request performance today compared to the previous instance startup. The performance for subsequent "warm" requests remains consistent.

Here is a comparison of the key performance metrics:

### Initial Request (Cold Start) Performance

This measures the time it takes for a new instance to start up and serve its very first request.

*   **Today (Oct 14, 19:14 UTC):**
    *   A new instance started due to autoscaling.
    *   The first request took **~218.6 seconds** to complete.
    *   The underlying vLLM engine initialization took **141.1 seconds**.

*   **Yesterday's Instance (Oct 14, 17:54 UTC):**
    *   A new instance started due to autoscaling.
    *   The first request took **~219.3 seconds** to complete.
    *   The underlying vLLM engine initialization took **144.5 seconds**.

**Conclusion:** The cold start time has slightly improved, with today's initial request being served about **3-4 seconds faster** during the engine initialization phase.

### Subsequent (Warm) Request Performance

This measures the latency for requests that arrive after the instance is already running and the model is loaded.

*   **Today (Oct 14):**
    *   Requests after startup had latencies of **6.3 seconds** and **11.1 seconds**.

*   **Yesterday's Instance (Oct 14):**
    *   Requests after startup had latencies of **5.3 seconds** and **11.0 seconds**.

**Conclusion:** The performance for warm requests is very similar and consistent across both periods, with minor variations likely due to differences in request complexity.
--------------------
[2025-10-14 22:36:54] User: is there a way to execute or cache the "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): " part during cloud build stage or during the first cold start, so that subsequent cold starts can be shorter?
[2025-10-14 22:36:54] Bot: That's an excellent and insightful question about optimizing cold starts for GPU-based services.

Unfortunately, it is **not possible** to pre-cache the "Capturing CUDA graphs" step during the Cloud Build stage or from one cold start to another.

Here is a breakdown of why this is the case and what you can do to mitigate the impact of this startup time.

### Why Caching Isn't Directly Possible

1.  **Hardware Specificity (Why Cloud Build won't work):** The CUDA graph capture is a highly specific process that depends on the exact GPU hardware (e.g., NVIDIA L4), the CUDA driver version, and the compiled model code. The Cloud Build environment is a CPU-based environment and does not have access to the same GPU hardware that your Cloud Run service uses. Therefore, it's impossible to generate the correct, hardware-specific graph during the build phase.

2.  **Stateless Nature of Cloud Run Instances (Why it's not cached between cold starts):** Each "cold start" in Cloud Run provisions a brand new, ephemeral instance. When an instance scales down to zero or is replaced, its entire in-memory state, including the captured CUDA graphs, is lost. The next request that triggers a new instance start will have to run the capture process all over again from scratch.

### Effective Strategies to Reduce Cold Start Impact

While you can't cache the graph capture *between* instances, you can prevent the cold start from happening in the first place for most of your traffic.

#### 1. Set Minimum Instances (Most Effective Solution)

The most direct way to solve this is to configure your Cloud Run service to always keep at least one instance running. This instance will go through the cold start process once and then remain "warm" and ready to accept requests.

*   **How it Works:** By setting `--min-instances=1`, you ensure one instance is always active. It performs the lengthy startup process (including CUDA graph capture, which took **~3 seconds** in your latest logs) just once. All subsequent requests will be routed to this warm instance, resulting in much lower latency (e.g., the **6-11 second** warm request times you are seeing).

*   **Trade-off:** The primary trade-off is cost. You will be billed for the instance's uptime, even when it is not actively processing requests.

*   **How to Implement:**
    ```bash
    gcloud run services update vllm-gemma-3-1b-it \
      --region us-central1 \
      --min-instances=1
    ```

#### 2. Startup CPU Boost (Already Enabled)

Looking at the audit logs from your service deployment, the `run.googleapis.com/startup-cpu-boost: 'true'` annotation is present. This is a good practice you are already using. It allocates more CPU resources during the instance startup and model initialization phase, which helps reduce the overall time before the GPU-specific tasks begin.

#### 3. (Advanced) Caching Compiled Kernels

The startup process consists of several steps. Your logs show two major time-consuming parts before the application is ready:
*   `torch.compile`: **~54 seconds**
*   `init engine` (including CUDA graph capture): **~141 seconds** (the graph capture itself is just a few seconds of this)

vLLM uses `torch.compile` which can cache its compiled artifacts to the filesystem. By mounting a persistent network filesystem (like Cloud Storage FUSE) to the cache directory (`/root/.cache/vllm/`), you could potentially avoid re-running the `torch.compile` step on every cold start. This would significantly reduce the startup time but would not eliminate the CUDA graph capture step itself. This is a more complex solution but could be a path for further optimization.

### Summary

In short, the "Capturing CUDA graphs" step is an unavoidable part of the startup process on a new instance. The most effective and recommended strategy to avoid this latency for your users is to **set `min-instances` to 1**. This keeps an instance constantly warm and ready, bypassing the cold start penalty for all but the very first request after a new deployment.
--------------------
[2025-10-14 22:38:46] User: please create a github issue to repository vllm-container-prewarm, as a feature request to enable the "Caching Compiled Kernels" option
[2025-10-14 22:38:46] Bot: Of course! Here is a draft for the GitHub issue to the `vllm-container-prewarm` repository, based on our conversation and the logs. You can copy and paste this directly into a new issue on GitHub.

---

### **Title: Feature Request: Enable `torch.compile` Kernel Caching to Reduce Cold Start Times**

**Is your feature request related to a problem? Please describe.**

When running vLLM containers on serverless platforms like Google Cloud Run (which scales to zero), cold start times can be significant, often exceeding 3 minutes. A substantial portion of this startup time is dedicated to `torch.compile` re-compiling kernels on every new instance.

Based on an analysis of the startup logs, the following steps are major contributors to the latency:
- `torch.compile`: **~54 seconds**
- `init engine` (including CUDA graph capture): **~141 seconds**

Log evidence:
```
INFO 10-14 12:16:38 [monitor.py:34] torch.compile takes 54.25 s in total
...
INFO 10-14 12:17:46 [core.py:214] init engine (profile, create kv cache, warmup model) took 141.08 seconds
```

This recompilation happens on every cold start because the container instance is ephemeral and its local filesystem is lost when it scales down. While setting `min-instances=1` is a viable workaround, it incurs constant costs. A more cost-effective solution would be to cache the compiled artifacts to persist them between cold starts.

**Describe the solution you'd like**

I would like a mechanism to cache the `torch.compile` artifacts generated by vLLM to a persistent storage location. This would allow subsequent cold starts to reuse the cached kernels, skipping the lengthy compilation step and significantly reducing the overall startup time.

This could be implemented by:
1.  **Exposing a mount path:** Allowing users to mount a persistent volume (e.g., a Google Cloud Storage bucket via GCS FUSE) to the vLLM cache directory.
2.  **Environment Variable:** Providing an environment variable to specify a custom path for the torch compile cache, which can then be pointed to a mounted persistent disk.

The logs indicate that the cache is being written to a path like `/root/.cache/vllm/torch_compile_cache/...`. Enabling a mount at `/root/.cache/vllm/` would likely solve this.

By caching these kernels, we could potentially reduce the cold start time by nearly a minute, making the service much more responsive for the first user after a scale-to-zero event.

**Describe alternatives you've considered**

The primary alternative is to set `min-instances=1` on the Cloud Run service. This keeps one instance warm at all times, avoiding the cold start penalty for most users. However, this comes with the trade-off of increased cost, as the instance is billed for its entire uptime, even when idle.

The proposed feature would provide a powerful optimization to reduce latency without incurring the cost of a constantly running instance.

**Additional context**

This feature would be particularly beneficial for deploying LLMs in serverless, auto-scaling environments where instances are frequently created and destroyed. It directly addresses one of the most time-consuming parts of the vLLM engine initialization process.
--------------------
[2025-10-14 22:39:53] User: no, I want you to create the issue in Github
[2025-10-14 22:39:53] Bot: My apologies for the misunderstanding. I should have been clearer about my capabilities.

As an AI assistant, I don't have the ability to connect to external websites like GitHub or perform actions such as creating issues directly in a repository. My function is to analyze information and generate text content for you to use.

The issue draft I created is complete and ready for you to post. I am pasting it again below for your convenience. You will need to navigate to the `vllm-container-prewarm` repository on GitHub yourself and open a new issue, then copy and paste the title and body from here.

---

### **Title: Feature Request: Enable `torch.compile` Kernel Caching to Reduce Cold Start Times**

**Is your feature request related to a problem? Please describe.**

When running vLLM containers on serverless platforms like Google Cloud Run (which scales to zero), cold start times can be significant, often exceeding 3 minutes. A substantial portion of this startup time is dedicated to `torch.compile` re-compiling kernels on every new instance.

Based on an analysis of the startup logs, the following steps are major contributors to the latency:
- `torch.compile`: **~54 seconds**
- `init engine` (including CUDA graph capture): **~141 seconds**

Log evidence:
```
INFO 10-14 12:16:38 [monitor.py:34] torch.compile takes 54.25 s in total
...
INFO 10-14 12:17:46 [core.py:214] init engine (profile, create kv cache, warmup model) took 141.08 seconds
```

This recompilation happens on every cold start because the container instance is ephemeral and its local filesystem is lost when it scales down. While setting `min-instances=1` is a viable workaround, it incurs constant costs. A more cost-effective solution would be to cache the compiled artifacts to persist them between cold starts.

**Describe the solution you'd like**

I would like a mechanism to cache the `torch.compile` artifacts generated by vLLM to a persistent storage location. This would allow subsequent cold starts to reuse the cached kernels, skipping the lengthy compilation step and significantly reducing the overall startup time.

This could be implemented by:
1.  **Exposing a mount path:** Allowing users to mount a persistent volume (e.g., a Google Cloud Storage bucket via GCS FUSE) to the vLLM cache directory.
2.  **Environment Variable:** Providing an environment variable to specify a custom path for the torch compile cache, which can then be pointed to a mounted persistent disk.

The logs indicate that the cache is being written to a path like `/root/.cache/vllm/torch_compile_cache/...`. Enabling a mount at `/root/.cache/vllm/` would likely solve this.

By caching these kernels, we could potentially reduce the cold start time by nearly a minute, making the service much more responsive for the first user after a scale-to-zero event.

**Describe alternatives you've considered**

The primary alternative is to set `min-instances=1` on the Cloud Run service. This keeps one instance warm at all times, avoiding the cold start penalty for most users. However, this comes with the trade-off of increased cost, as the instance is billed for its entire uptime, even when idle.

The proposed feature would provide a powerful optimization to reduce latency without incurring the cost of a constantly running instance.

**Additional context**

This feature would be particularly beneficial for deploying LLMs in serverless, auto-scaling environments where instances are frequently created and destroyed. It directly addresses one of the most time-consuming parts of the vLLM engine initialization process.
--------------------
[2025-10-14 23:23:47] User: for cloud run service vllm-gemma-3-1b-it, can you tell if the performance has improved from yesterday's initial requests to today's?
[2025-10-14 23:23:47] Bot: Of course. Based on the logs, here is an analysis of the performance for the `vllm-gemma-3-1b-it` service.

First, it's important to note that all the provided logs are from the same day, **October 14, 2025**. Therefore, a comparison between "yesterday" and "today" isn't possible with this data. However, we can compare the performance of initial requests that caused new instances to start versus subsequent requests on those same instances.

### Summary

Yes, the performance significantly improves after the initial "cold start" request.

*   **Initial Requests (Cold Starts):** The first request to a new instance takes a very long time, around **3.5 minutes**, due to the time required to start the container and load the model.
*   **Subsequent Requests (Warm Starts):** Once an instance is running, subsequent requests are much faster, with latencies ranging from **6 to 11 seconds**.
*   **Model Throughput:** The model's token generation throughput is consistent across different instances once they are operational.

### Detailed Performance Analysis

#### 1. Request Latency (Cold vs. Warm)

The logs show two separate instances being started due to autoscaling. We can see a dramatic difference in latency between the first request (cold start) and later requests (warm start).

*   **Earlier Instance (started at 17:54 UTC):**
    *   **Cold Start Latency:** A request at `17:54:35` triggered this new instance, and the request latency was **219.2 seconds** (over 3.6 minutes).
    *   The instance logs confirm this startup time, showing "Application startup complete" at `17:58:09`.

*   **Later Instance (started at 19:14 UTC):**
    *   **Cold Start Latency:** A request at `19:14:21` triggered this new instance, with a latency of **218.5 seconds** (almost 3.6 minutes).
    *   **Warm Start Latency:** Subsequent requests to this same, now running, instance were much faster:
        *   Request at `19:17:45`: **6.28 seconds**
        *   Request at `19:17:58`: **11.08 seconds**

This shows that while the initial startup is slow, performance is significantly better for subsequent requests once the service is running.

#### 2. Model Throughput

The vLLM engine logs its average throughput in tokens per second. The performance appears consistent between the two instances once they are warm and serving traffic.

*   **Earlier Instance (at 17:58:19):**
    *   Avg prompt throughput: **152.5 tokens/s**
    *   Avg generation throughput: **108.1 tokens/s**

*   **Later Instance (at 19:17:59):**
    *   Avg prompt throughput: **155.0 tokens/s**
    *   Avg generation throughput: **147.4 tokens/s**

These numbers indicate that the underlying model performance has not degraded or changed significantly throughout the day.

#### Additional Observation: Startup Failures

The logs also show that several revisions earlier in the day (e.g., `...00034-n95`, `...00033-fdf`) failed to deploy correctly. They consistently show a `Default STARTUP TCP probe failed` error with a `DEADLINE_EXCEEDED` status. This means the container took longer than the configured startup probe timeout (240 seconds) to become ready. The current, successful revision (`...00036-2dd`) also has this 240-second timeout and appears to be starting just within that window.
--------------------
[2025-10-14 23:24:25] User: is there a way to execute or cache the "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): " part during cloud build stage or during the first cold start, so that subsequent cold starts can be shorter?
[2025-10-14 23:24:25] Bot: Routing to solutions_agent
--------------------
[2025-10-14 23:33:55] User: for cloud run service vllm-gemma-3-1b-it, can you tell if the performance has improved from yesterday's initial requests to today's?
[2025-10-14 23:33:55] Bot: Based on the logs, here's an analysis of the performance for the `vllm-gemma-3-1b-it` service:

Yes, it appears the model's processing throughput has improved, although the end-to-end request latency for warm instances has remained relatively consistent.

Here is a comparison between the initial requests from an earlier instance and the most recent ones:

### Performance Comparison

**1. Throughput (Tokens per Second)**

This metric measures how quickly the model processes prompts and generates responses.

*   **Earlier Requests** (around 17:58 UTC):
    *   Average prompt throughput: **152.5 tokens/s**
    *   Average generation throughput: **108.1 tokens/s**

*   **Today's Requests** (around 19:17 UTC):
    *   Average prompt throughput: **155.0 tokens/s**
    *   Average generation throughput: **147.4 tokens/s**

**Conclusion:** The generation throughput shows a noticeable improvement in the more recent requests, increasing from ~108 to ~147 tokens/s.

**2. Request Latency**

This is the total time taken from the start of the request to receiving the full response.

*   **Earlier Requests** (on a warm instance):
    *   `POST /v1/chat/completions`: **5.27 seconds**
    *   `POST /v1/completions`: **11.02 seconds**

*   **Today's Requests** (on a warm instance, after startup):
    *   `POST /v1/chat/completions`: **6.28 seconds**
    *   `POST /v1/completions`: **11.08 seconds**

**Conclusion:** The latency for requests on a warm (already running) instance is very similar between the two periods.

### Important Note on Cold Starts

One of the first requests today at `19:14:21` had a very high latency of **218.6 seconds** (over 3.6 minutes). This happened because no instances were running, and Cloud Run had to start a new one from scratch (a "cold start"). The logs confirm this instance startup, including loading the model, took over 141 seconds, which accounts for the long initial delay. Subsequent requests to this "warm" instance were significantly faster.
--------------------
[2025-10-14 23:34:22] User: is there a way to execute or cache the "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): " part during cloud build stage or during the first cold start, so that subsequent cold starts can be shorter?
[2025-10-14 23:34:22] Bot: Routing to solutions_agent
--------------------
[2025-10-14 23:41:58] User: for cloud run service vllm-gemma-3-1b-it, can you tell if the performance has improved from yesterday's initial requests to today's?
[2025-10-14 23:41:58] Bot: Based on the logs, yes, there are indications of performance improvements for the `vllm-gemma-3-1b-it` service between the earlier and later requests on October 14, 2025.

Here is a summary of the comparison:

### Performance Improvements

*   **Generation Throughput:** This is the most significant improvement.
    *   **Earlier Request** (at `17:58:19`): Average generation throughput was **108.1 tokens/s**.
    *   **Later Request** (at `19:17:59`): Average generation throughput increased to **147.4 tokens/s**.

*   **Cold Start / Instance Initialization:** The time it took for a new instance to start and initialize the vLLM engine slightly decreased.
    *   **Earlier Instance**: Took ~144 seconds to initialize the engine (`17:58:05`).
    *   **Later Instance**: Took ~141 seconds to initialize the engine (`19:17:46`).

### Stable Performance Metrics

*   **Request Latency:** The end-to-end request latency remained fairly consistent.
    *   **`/v1/completions` Endpoint:** Latency was **11.01 seconds** for an earlier request and **11.08 seconds** for a later one.
    *   **`/v1/chat/completions` Endpoint:** Latency was around **5-6 seconds** for requests in both periods.

In summary, while the overall request latency has been stable, the model's core performance, measured by **generation throughput, has improved noticeably** between the earlier and later requests logged today.
--------------------
[2025-10-14 23:50:29] User: for cloud run service vllm-gemma-3-1b-it, can you tell if the performance has improved from yesterday's initial requests to today's?
[2025-10-14 23:50:29] Bot: Based on the logs, we can compare the performance of the `vllm-gemma-3-1b-it` service between an earlier set of requests around **17:58 UTC** and a later set around **19:17 UTC** on October 14, 2025.

Here is a summary of the performance changes:

*   **Throughput has improved:**
    *   The peak average generation throughput increased from **~108 tokens/s** at 17:58 to **~147 tokens/s** at 19:17.
    *   The peak average prompt throughput also saw a slight improvement from **152.5 tokens/s** to **155.0 tokens/s**.

*   **Latency is mixed:**
    *   The latency for the `/v1/chat/completions` endpoint increased slightly from **~5.3 seconds** to **~6.3 seconds**.
    *   The latency for the `/v1/completions` endpoint remained relatively stable at **~11 seconds** for both periods.

In summary, while the latency for one endpoint saw a minor increase, the overall token generation and prompt processing throughput have noticeably improved in the more recent requests.
--------------------
[2025-10-15 00:30:09] User: or cloud run service vllm-gemma-3-1b-it, can you tell if the performance has improved from yesterday's initial requests to today's?
[2025-10-15 00:30:09] Bot: Yes, based on the logs, the performance of the `vllm-gemma-3-1b-it` service has improved today, specifically in terms of model throughput.

Here is a comparison of the key performance metrics:

**Earlier Requests (around 17:58 UTC):**

*   **Latency:** A `POST /v1/completions` request took **11.01 seconds**.
*   **Throughput:** At `17:58:19 UTC`, the logs reported an `Avg generation throughput` of **108.1 tokens/s**.

**Today's Later Requests (around 19:17 UTC):**

*   **Latency:** A similar `POST /v1/completions` request took **11.08 seconds**.
*   **Throughput:** At `19:17:59 UTC`, the logs reported an `Avg generation throughput` of **147.4 tokens/s** while handling two concurrent requests.

**Summary:**

While the end-to-end latency for individual requests remained roughly the same, the average generation throughput (the speed at which the model generates tokens) increased significantly from **108.1 tokens/s** to **147.4 tokens/s**. This indicates that the model is processing requests more efficiently now than it was earlier.
--------------------
[2025-10-15 00:31:44] User: is there a way to execute or cache the "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): " part during cloud build stage or during the first cold start, so that subsequent cold starts can be shorter?
[2025-10-15 00:31:44] Bot: Routing to solutions_agent
--------------------
[2025-10-15 00:39:31] User: or cloud run service vllm-gemma-3-1b-it, can you tell if the performance has improved from yesterday's initial requests to today's?
[2025-10-15 00:39:31] Bot: Yes, based on the logs, the performance of the `vllm-gemma-3-1b-it` service has improved between the earlier and later requests logged on October 14, 2025.

Here is a comparison of the performance metrics:

### Earlier Requests (around 19:17:59 UTC)

*   **Avg prompt throughput:** 155.0 tokens/s
*   **Avg generation throughput:** 147.4 tokens/s
*   **Latency for `/v1/completions`:** ~11.08 seconds
*   **Latency for `/v1/chat/completions`:** ~6.28 seconds

### Later Requests (around 19:18:09 UTC)

*   **Avg prompt throughput:** 0.0 tokens/s (Note: this is likely reported after requests have completed)
*   **Avg generation throughput:** 102.5 tokens/s
*   **Latency for `/v1/completions`:** ~11.08 seconds

**Summary:**

While the request latency remained relatively consistent for similar endpoints, the throughput shows a notable improvement.

*   **Prompt Throughput:** Increased slightly from **152.5 tokens/s** to **155.0 tokens/s**.
*   **Generation Throughput:** Showed a significant increase from **108.1 tokens/s** to **147.4 tokens/s**.

This indicates that while processing requests of similar complexity, the service became more efficient at generating tokens over time.
--------------------
[2025-10-15 00:57:48] User: is there a way to execute or cache the "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): " part during cloud build stage or during the first cold start, so that subsequent cold starts can be shorter?
[2025-10-15 00:57:48] Bot: Routing to solutions_agent
--------------------
[2025-10-15 01:07:30] User: or cloud run service vllm-gemma-3-1b-it, can you tell if the performance has improved from yesterday's initial requests to today's?
[2025-10-15 01:07:30] Bot: Based on the logs provided, here is an analysis of the performance for the `vllm-gemma-3-1b-it` service.

All the provided logs are from the same day, **October 14, 2025**. Therefore, this comparison is between requests made earlier in the day and those made later in the day, rather than yesterday versus today.

### Performance Comparison

**Earlier Requests (around 17:58 UTC):**

A new instance (`...b81b`) started to handle traffic and processed several requests.

*   **Latency:**
    *   A `POST /v1/chat/completions` request had a latency of **5.26 seconds**.
    *   A `POST /v1/completions` request had a latency of **11.01 seconds**.
*   **Throughput:**
    *   At `17:58:19`, the engine reported an average prompt throughput of **152.5 tokens/s** and an average generation throughput of **108.1 tokens/s**.

**Later Requests (around 19:17 UTC):**

A different instance (`...cd8`) started due to autoscaling and handled new requests.

*   **Latency:**
    *   A `POST /v1/chat/completions` request had a latency of **6.28 seconds**.
    *   A `POST /v1/completions` request had a latency of **11.08 seconds**.
*   **Throughput:**
    *   At `19:17:59`, the engine reported an average prompt throughput of **155.0 tokens/s** and an average generation throughput of **147.4 tokens/s**.

### Summary

*   **Request Latency:** The overall end-to-end request latencies are very similar between the earlier and later requests, with the later ones being marginally slower.
*   **Model Throughput:** There is a **notable improvement in performance** regarding the model's processing speed. The average generation throughput increased from **~108 tokens/s** to **~147 tokens/s**.

**Conclusion:**

Yes, while the total time to get a response (latency) has remained roughly the same, the model's core processing speed (generation throughput) has **improved** in the later requests compared to the earlier ones on October 14, 2025.
--------------------
[2025-10-15 01:07:51] User: is there a way to execute or cache the "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): " part during cloud build stage or during the first cold start, so that subsequent cold starts can be shorter?
[2025-10-15 01:07:51] Bot: Routing to solutions_agent
--------------------
[2025-10-15 01:18:21] User: or cloud run service vllm-gemma-3-1b-it, can you tell if the performance has improved from yesterday's initial requests to today's?
[2025-10-15 01:18:21] Bot: 
--------------------
[2025-10-15 01:21:59] User: for cloud run service vllm-gemma-3-1b-it, can you tell if the performance has improved from yesterday's initial requests to today's?
[2025-10-15 01:21:59] Bot: 
--------------------
[2025-10-15 01:24:42] User: for cloud run service vllm-gemma-3-1b-it, can you tell if the performance has improved from yesterday's initial requests to today's?
[2025-10-15 01:24:42] Bot: 
--------------------
[2025-10-15 01:46:51] User: for cloud run service vllm-gemma-3-1b-it, can you tell if the performance has improved from yesterday's initial requests to today's?
[2025-10-15 01:46:51] Bot: 
--------------------
[2025-10-15 01:47:44] User: for cloud run service vllm-gemma-3-1b-it, can you tell if the performance has improved from yesterday's initial requests to today's?
[2025-10-15 01:47:44] Bot: 
--------------------
[2025-10-15 01:56:57] User: for cloud run service vllm-gemma-3-1b-it, can you tell if the performance has improved from yesterday's initial requests to today's?
[2025-10-15 01:56:57] Bot: 
--------------------
[2025-10-15 01:59:30] User: for cloud run service vllm-gemma-3-1b-it, can you tell if the performance has improved from yesterday's initial requests to today's?
[2025-10-15 01:59:30] Bot: 
--------------------
[2025-10-15 02:03:23] User: for cloud run service vllm-gemma-3-1b-it, can you tell if the performance has improved from yesterday's initial requests to today's?
[2025-10-15 02:03:23] Bot: Based on the logs, here is an analysis of the performance of the Cloud Run service `vllm-gemma-3-1b-it`, comparing earlier requests to more recent ones on October 14, 2025.

### Summary:

Overall, the performance of the service has remained **largely consistent**, with only very minor improvements. The most significant factor impacting performance is the "cold start" time when a new instance is required, which has remained around 3.6 minutes for the initial request.

---

### Detailed Analysis:

Here's a breakdown comparing an earlier set of requests (around 17:58 UTC) with a later set (around 19:17 UTC):

**1. Latency (Request/Response Time):**

*   **Cold Start Latency:** When a request triggers a new instance to start (an "AUTOSCALING" event), the latency is very high due to the time it takes to load the model.
    *   **Earlier Instance (started ~17:54 UTC):** The first request had a latency of **~219 seconds**.
    *   **Later Instance (started ~19:14 UTC):** The first request had a latency of **~218 seconds**.
    *   **Conclusion:** The cold start performance is nearly identical.

*   **Warm Request Latency:** For requests that hit an already running and initialized instance, the performance is much better and also very consistent.
    *   **Earlier Instance:** Requests at 17:58 UTC had latencies of **~5.3 seconds** and **~11.0 seconds**.
    *   **Later Instance:** Requests at 19:17 UTC had latencies of **~6.3 seconds** and **~11.1 seconds**.
    *   **Conclusion:** There is no significant change in latency for warm requests.

**2. Throughput (Tokens per second):**

The logs show the average generation throughput, which can fluctuate based on the number of concurrent requests.

*   **Earlier Instance (~17:58 UTC):** The logs show an average generation throughput of **108.1 tokens/s** while processing 1 request.
*   **Later Instance (~19:17 UTC):** The logs show a throughput of **147.4 tokens/s** while processing 2 concurrent requests, and later **102.5 tokens/s** with 1 request.

*   **Conclusion:** The throughput appears slightly better under load in the later period (147.4 tokens/s vs. 108.1 tokens/s), but is otherwise comparable.

**3. Startup Time:**

A major part of the cold start latency is the vLLM engine initialization time.

*   **Earlier Instance:** The engine initialization took **144.48 seconds**.
*   **Later Instance:** The engine initialization took **141.08 seconds**.

*   **Conclusion:** There was a minor improvement of about **3.4 seconds** in the engine startup time for the more recent instance.
--------------------
[2025-10-15 02:03:47] User: is there a way to execute or cache the "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): " part during cloud build stage or during the first cold start, so that subsequent cold starts can be shorter?
[2025-10-15 02:03:47] Bot: Logs for service 'vllm-gemma-3-1b-it' were found. Further analysis of these logs is needed to propose a specific solution. Here are some recent log entries:

TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea49a000eb41828b7daa0', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 29, 30, 963608, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e5438b140>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO:     Application shutdown complete.')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea49a000eb3dcce41b7dd', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 29, 30, 963548, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e5438b140>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO:     Waiting for application shutdown.')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea49a000d2b16acd694d3', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 29, 30, 862998, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e5438b140>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO:     Shutting down')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea498000a3531241afd4a', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 29, 28, 668977, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e5438b140>, payload='[rank0]:[W1014 12:29:28.295721832 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea497000be8b9b62ac013', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 29, 27, 780473, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:29:27 [launcher.py:101] Shutting down FastAPI HTTP server.')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea205000df1a22ad7afa6', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 18, 29, 913826, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:18:29 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 11.0%')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1fb000df0295c6442f6', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 18, 19, 913449, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:18:19 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 11.0%')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1f1000e326f3b2212fa', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 18, 9, 930415, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO:     169.254.169.126:12446 - "POST /v1/completions HTTP/1.1" 200 OK')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1f1000dea8bcb35ff8d', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 18, 9, 912011, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:18:09 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 11.0%')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1e90003f184e8ec77be', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 18, 1, 258436, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO:     169.254.169.126:57708 - "POST /v1/completions HTTP/1.1" 200 OK')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1e7000de97a6a73d565', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 59, 911738, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:59 [loggers.py:123] Engine 000: Avg prompt throughput: 155.0 tokens/s, Avg generation throughput: 147.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 11.0%')
LogEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Frequests', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1f1000e344d62d5ef58', severity='INFO', http_request={'requestMethod': 'POST', 'requestUrl': 'https://vllm-gemma-3-1b-it-665374072631.us-central1.run.app/v1/completions', 'requestSize': '2719', 'status': 200, 'responseSize': '4208', 'userAgent': 'python-requests/2.32.3', 'remoteIp': '100.38.44.126', 'serverIp': '34.143.74.2', 'latency': '11.080305415s', 'protocol': 'HTTP/1.1'}, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 58, 847941, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace='projects/globalbiting-dev/traces/4b19d51b4ddfaec25ed63b616484c1ec', span_id='b8670debf656c0ea', trace_sampled=True, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e6445e2d0>, payload=None)
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1e4000c6bc12a46cd3a', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 56, 814017, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO:     169.254.169.126:57722 - "POST /v1/chat/completions HTTP/1.1" 200 OK')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1de00090bb178397e41', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 50, 592817, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload="\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:50 [chat_utils.py:470] Detected the chat template content format to be 'openai'. You can set `--chat-template-content-format` to override this.")
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd000e988b445befd0', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 956555, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m WARNING 10-14 12:17:49 [tracing.py:130] Received a request with trace context but tracing is disabled')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fvarlog%2Fsystem', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd000de7c479a36f59', severity='INFO', http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 911300, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4ee86cf0>, payload='STARTUP TCP probe succeeded after 1 attempt for container "vllm-gemma-3-1b-it-1" on port 8000.')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd000de58441925190', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 910724, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e5438b140>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO:     Application startup complete.')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd000ae60597c98d3d', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 714245, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e5438b140>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO:     Waiting for application startup.')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd000ae5fa7644d2ae', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 714234, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e5438b140>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO:     Started server process [1]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096c1f00ce7773', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617503, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /metrics, Methods: GET')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096bd385e9b8e0', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617427, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /invocations, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096bbbd94790d4', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617403, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /is_scaling_elastic_ep, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096b97f76486c6', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617367, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /scale_elastic_ep, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096b7f337b19ee', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617343, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /v2/rerank, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096b6b30beb46f', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617323, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /v1/rerank, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096b687ce4df19', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617320, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /rerank, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096b333e2a97c1', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617267, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /v1/audio/translations, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096b1aadf17272', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617242, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /v1/audio/transcriptions, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096afb2f80568c', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617211, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /v1/score, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096af85aa507e8', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617208, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /score, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096ac194b314d6', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617153, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /classify, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096ab4bfb26132', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617140, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /pooling, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096a93d0cd2f56', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617107, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /v1/embeddings, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096a90d4726518', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617104, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /v1/completions, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096a6687c3dcb7', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617062, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /v1/chat/completions, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096a63f8a4d24a', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617059, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /v1/responses/{response_id}/cancel, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096a2cc30c3a90', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617004, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /v1/responses/{response_id}, Methods: GET')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096a1d633cb5ca', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616989, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /v1/responses, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd000969f9520f41c2', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616953, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /version, Methods: GET')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd000969dc58a3991a', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616924, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /v1/models, Methods: GET')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd000969bd6a3c0edc', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616893, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /detokenize, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd000969a772f607d5', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616871, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /tokenize, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd0009699bd45dfd14', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616859, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /ping, Methods: GET')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd000969685191c5f6', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616808, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /ping, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd0009696580e16fc4', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616805, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /load, Methods: GET')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096924abb7465b', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616740, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /health, Methods: GET')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd000969216beea8d7', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616737, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /redoc, Methods: GET, HEAD')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096906fcdf396a', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616710, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /docs/oauth2-redirect, Methods: GET, HEAD')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd000969039def3d64', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616707, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /docs, Methods: GET, HEAD')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd000968b0a80c7dc2', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616624, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /openapi.json, Methods: GET, HEAD')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd000968948052945c', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616596, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:36] Available routes are:')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd0009685040d4ad9f', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616528, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [api_server.py:1880] Starting vLLM API server 0 on http://0.0.0.0:8000')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096806acddeda7', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616454, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload="\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [serving_completion.py:77] Using default completion sampling params from model: {'top_k': 64, 'top_p': 0.95}")
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd0009632982fdae63', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 615209, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload="\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [serving_chat.py:134] Using default chat sampling params from model: {'top_k': 64, 'top_p': 0.95}")
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00095e17d99f33d4', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 613911, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload="\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [serving_responses.py:120] Using default chat sampling params from model: {'top_k': 64, 'top_p': 0.95}")
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00095ddce500bca4', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 613852, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload="\x1b[1;36m(APIServer pid=1)\x1b[0;0m WARNING 10-14 12:17:49 [__init__.py:1625] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.")
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00095202460b8f2b', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 610818, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload="\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [api_server.py:1611] Supported_tasks: ['generate']")
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd0008d72f459ba196', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 579375, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [loggers.py:142] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 277658')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1da000b603efdb6201f', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 46, 745534, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m INFO 10-14 12:17:46 [core.py:214] init engine (profile, create kv cache, warmup model) took 141.08 seconds')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1da000b2c41abb40115', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 46, 732225, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m INFO 10-14 12:17:46 [gpu_model_runner.py:2708] Graph capturing finished in 3 secs, took 1.64 GiB')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1da000b2bb2110b09bb', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 46, 732082, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e5438b140>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:02<00:00, 26.86it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1da000b2ba8507e373b', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 46, 732072, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e5438b140>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 66/67 [00:02<00:00, 28.11it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1da000a06b8de202aca', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 46, 657080, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e5438b140>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 63/67 [00:02<00:00, 27.92it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1da00086c955a733759', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 46, 552085, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e5438b140>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 60/67 [00:02<00:00, 28.52it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1da0006b42bc21ba454', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 46, 439339, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e5438b140>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 57/67 [00:02<00:00, 28.34it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1da00051f51cb05334f', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 46, 335697, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e5438b140>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 54/67 [00:01<00:00, 28.17it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1da000387e93ddd7045', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 46, 231401, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e5438b140>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 51/67 [00:01<00:00, 28.55it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1da0001dac56b90c3c2', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 46, 121541, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e5438b140>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 48/67 [00:01<00:00, 28.45it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1da000043c4406ac1de', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 46, 17348, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e5438b140>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 45/67 [00:01<00:00, 28.35it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d9000ded4c537feafc', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 45, 912716, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e5438b140>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 42/67 [00:01<00:00, 28.49it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d9000c4b3efb4051c0', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 45, 805694, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e5438b140>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 39/67 [00:01<00:00, 28.02it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d9000abfaaf0ae3b60', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 45, 704426, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e5438b140>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 36/67 [00:01<00:01, 27.29it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d90009370d2cad2372', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 45, 603917, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e5438b140>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 33/67 [00:01<00:01, 26.47it/s]')
LogEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Frequests', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1e4000c6ea60d56661e', severity='INFO', http_request={'requestMethod': 'POST', 'requestUrl': 'https://vllm-gemma-3-1b-it-665374072631.us-central1.run.app/v1/chat/completions', 'requestSize': '6708', 'status': 200, 'responseSize': '2699', 'userAgent': 'python-requests/2.32.3', 'remoteIp': '100.38.44.126', 'serverIp': '34.143.74.2', 'latency': '6.283451192s', 'protocol': 'HTTP/1.1'}, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 45, 523314, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace='projects/globalbiting-dev/traces/60d7bc4512bc99a69f7d2f231b1a5f33', span_id='ab2f6eabefe07050', trace_sampled=True, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e6445e2d0>, payload=None)
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d90007a829b011f452', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 45, 501801, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e5438b140>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 30/67 [00:01<00:01, 25.37it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d900061933145639db', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 45, 399667, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e5438b140>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 27/67 [00:01<00:01, 24.16it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d900047f6bd0e8e685', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 45, 294763, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e5438b140>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 24/67 [00:00<00:01, 27.24it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d9000221386b814727', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 45, 139576, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e5438b140>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|â–ˆâ–ˆâ–ˆâ–      | 21/67 [00:00<00:01, 26.87it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d900007fdb8e848749', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 45, 32731, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e5438b140>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|â–ˆâ–ˆâ–‹       | 18/67 [00:00<00:01, 26.44it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d8000e1c9bb4ea3860', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 44, 924827, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e5438b140>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|â–ˆâ–ˆâ–       | 15/67 [00:00<00:01, 26.21it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d8000c68f1821eee3e', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 44, 813297, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e5438b140>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|â–ˆâ–Š        | 12/67 [00:00<00:02, 26.21it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d8000aa98b3beb1a67', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 44, 698763, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e5438b140>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  13%|â–ˆâ–Ž        | 9/67 [00:00<00:02, 25.98it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d80008f0a86824a16d', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 44, 585896, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e5438b140>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|â–‰         | 6/67 [00:00<00:02, 25.91it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d800072ee0b19628b6', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 44, 470752, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e5438b140>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|â–         | 3/67 [00:00<00:02, 24.90it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d80005775b7195ab82', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 44, 358235, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e5438b140>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/67 [00:00<?, ?it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d800039fe5474f1e61', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 44, 237541, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e5438b140>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m ')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d7000988e7d802198d', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 43, 624871, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m INFO 10-14 12:17:43 [kv_cache_utils.py:1017] Maximum concurrency for 32,768 tokens per request: 92.12x')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d70009885036551399', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 43, 624720, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m INFO 10-14 12:17:43 [kv_cache_utils.py:1013] GPU KV cache size: 634,640 tokens')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d70009874c6aa1e97f', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 43, 624460, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m WARNING 10-14 12:17:43 [kv_cache_utils.py:971] Add 2 padding layers, may waste at most 9.09% KV cache memory')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d7000149dba8b7a62b', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 43, 84443, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m INFO 10-14 12:17:43 [gpu_worker.py:276] Available KV cache memory: 16.95 GiB')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1970000be1ae74ee2f0', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 16, 39, 48666, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e5438b140>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m   warnings.warn(')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1970000be160917d7b6', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 16, 39, 48662, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e5438b140>, payload="\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].")
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1970000be052b9934fd', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 16, 39, 48645, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e5438b140>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m /usr/local/lib/python3.12/dist-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. ')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea196000c7e7f01466e5e', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 16, 38, 818815, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m INFO 10-14 12:16:38 [monitor.py:34] torch.compile takes 54.25 s in total')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea18400096277c6df77aa', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 16, 20, 615031, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m INFO 10-14 12:16:20 [backends.py:215] Compiling a graph for dynamic shape takes 43.16 s')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea15e0008bfadfa8279ef', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 15, 42, 573357, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m INFO 10-14 12:15:42 [backends.py:194] Cache the graph for dynamic shape for later use')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea15a0009d5c846347c62', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 15, 38, 644552, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e5438b140>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m [rank0]:W1014 12:15:38.642000 51 torch/_inductor/utils.py:1250] [0/0] Not enough SMs to use max_autotune_gemm mode')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea158000e9c4c14b07e32', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 15, 36, 957516, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m INFO 10-14 12:15:36 [backends.py:559] Dynamo bytecode transform time: 11.09 s')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea158000e9c25097d10e4', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 15, 36, 957477, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload="\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m INFO 10-14 12:15:36 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/90a1db546d/rank_0_0/backbone for vLLM's torch.compile")
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea14d000a1c47f74aafca', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 15, 25, 662599, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x798e4e124590>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m INFO 10-14 12:15:25 [gpu_model_runner.py:2007] Model loading took 1.9147 GiB and 1.868946 seconds')

Based on your query: 'is there a way to execute or cache the "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): " part during cloud build stage or during the first cold start, so that subsequent cold starts can be shorter?', a potential solution could involve optimizing the cold start process by pre-warming the service or caching frequently used resources.
--------------------
[2025-10-15 02:09:17] User: for cloud run service vllm-gemma-3-1b-it, can you tell if the performance has improved from yesterday's initial requests to today's?
[2025-10-15 02:09:17] Bot: Based on the logs, here is an analysis of the performance of the `vllm-gemma-3-1b-it` service.

All the provided logs are from a single day, **October 14, 2025**. Therefore, this analysis compares requests made earlier in the day to those made later in the day, rather than comparing "yesterday" to "today".

### Summary

Excluding initial "cold start" requests which are expectedly slow, the service's performance for subsequent requests has been **relatively stable, with a slight improvement in token generation throughput**.

*   **Request Latency:** Latency for "warm" requests has remained fairly consistent, ranging from approximately 5 to 11 seconds. The extremely high latencies (`>200s`) correspond directly to new instances starting up (cold starts).
*   **Token Throughput:** The average generation throughput shows an improvement in the later requests compared to the earlier ones.

---

### Detailed Analysis

**1. Cold Starts vs. Warm Requests**

The logs show two instances of very high latency, which are explained by the service scaling up and starting a new container instance ("cold start"). This involves provisioning the instance and loading the model, which takes significant time.

*   **Earlier Cold Start (around 19:14 UTC):**
    *   `19:14:21`: A request for `/v1/completions` took **218.56 seconds**.
    *   `19:14:21`: A log entry confirms a `Starting new instance. Reason: AUTOSCALING`.

*   **Later Cold Start (around 17:54 UTC):**
    *   This is an interesting data point that seems out of order, but shows the same pattern. A request for `/v1/completions` at `17:54:35` took **219.27 seconds**.
    *   A corresponding log at `17:54:35` shows `Starting new instance. Reason: AUTOSCALING`.

**2. Warm Request Performance Comparison**

When analyzing requests that were handled by an already running ("warm") instance, the performance is much better and more consistent.

**Earlier Requests (around 17:58 UTC):**

*   **Latency:**
    *   A `/v1/chat/completions` request took **5.26 seconds**.
    *   A `/v1/completions` request took **11.01 seconds**.
*   **Throughput:**
    *   `Avg generation throughput`: **108.1 tokens/s**
    *   `Avg prompt throughput`: **152.5 tokens/s**

**Later Requests (around 19:17 UTC):**

*   **Latency:**
    *   A `/v1/chat/completions` request took **6.28 seconds**.
    *   A `/v1/completions` request took **11.08 seconds**.
*   **Throughput:**
    *   `Avg generation throughput`: **147.4 tokens/s**
    *   `Avg prompt throughput`: **155.0 tokens/s**

**Conclusion:**

While the latency for warm requests has remained similar (a slight increase, but likely within normal variance), the **average generation throughput increased from ~108 tokens/s to ~147 tokens/s**, indicating a performance improvement in the model's processing speed between these two time points.
--------------------
[2025-10-15 02:09:44] User: is there a way to execute or cache the "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): " part during cloud build stage or during the first cold start, so that subsequent cold starts can be shorter?
[2025-10-15 02:09:44] Bot: Logs for service 'vllm-gemma-3-1b-it' were found. Further analysis of these logs is needed to propose a specific solution. Here are some recent log entries:

TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea49a000eb41828b7daa0', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 29, 30, 963608, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd6480>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO:     Application shutdown complete.')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea49a000eb3dcce41b7dd', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 29, 30, 963548, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd6480>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO:     Waiting for application shutdown.')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea49a000d2b16acd694d3', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 29, 30, 862998, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd6480>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO:     Shutting down')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea498000a3531241afd4a', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 29, 28, 668977, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd6480>, payload='[rank0]:[W1014 12:29:28.295721832 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea497000be8b9b62ac013', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 29, 27, 780473, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:29:27 [launcher.py:101] Shutting down FastAPI HTTP server.')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea205000df1a22ad7afa6', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 18, 29, 913826, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:18:29 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 11.0%')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1fb000df0295c6442f6', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 18, 19, 913449, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:18:19 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 11.0%')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1f1000e326f3b2212fa', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 18, 9, 930415, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO:     169.254.169.126:12446 - "POST /v1/completions HTTP/1.1" 200 OK')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1f1000dea8bcb35ff8d', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 18, 9, 912011, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:18:09 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 11.0%')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1e90003f184e8ec77be', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 18, 1, 258436, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO:     169.254.169.126:57708 - "POST /v1/completions HTTP/1.1" 200 OK')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1e7000de97a6a73d565', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 59, 911738, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:59 [loggers.py:123] Engine 000: Avg prompt throughput: 155.0 tokens/s, Avg generation throughput: 147.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 11.0%')
LogEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Frequests', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1f1000e344d62d5ef58', severity='INFO', http_request={'requestMethod': 'POST', 'requestUrl': 'https://vllm-gemma-3-1b-it-665374072631.us-central1.run.app/v1/completions', 'requestSize': '2719', 'status': 200, 'responseSize': '4208', 'userAgent': 'python-requests/2.32.3', 'remoteIp': '100.38.44.126', 'serverIp': '34.143.74.2', 'latency': '11.080305415s', 'protocol': 'HTTP/1.1'}, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 58, 847941, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace='projects/globalbiting-dev/traces/4b19d51b4ddfaec25ed63b616484c1ec', span_id='b8670debf656c0ea', trace_sampled=True, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92ce369310>, payload=None)
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1e4000c6bc12a46cd3a', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 56, 814017, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO:     169.254.169.126:57722 - "POST /v1/chat/completions HTTP/1.1" 200 OK')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1de00090bb178397e41', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 50, 592817, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload="\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:50 [chat_utils.py:470] Detected the chat template content format to be 'openai'. You can set `--chat-template-content-format` to override this.")
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd000e988b445befd0', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 956555, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m WARNING 10-14 12:17:49 [tracing.py:130] Received a request with trace context but tracing is disabled')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fvarlog%2Fsystem', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd000de7c479a36f59', severity='INFO', http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 911300, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b4d6ec60>, payload='STARTUP TCP probe succeeded after 1 attempt for container "vllm-gemma-3-1b-it-1" on port 8000.')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd000de58441925190', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 910724, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd6480>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO:     Application startup complete.')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd000ae60597c98d3d', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 714245, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd6480>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO:     Waiting for application startup.')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd000ae5fa7644d2ae', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 714234, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd6480>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO:     Started server process [1]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096c1f00ce7773', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617503, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /metrics, Methods: GET')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096bd385e9b8e0', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617427, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /invocations, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096bbbd94790d4', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617403, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /is_scaling_elastic_ep, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096b97f76486c6', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617367, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /scale_elastic_ep, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096b7f337b19ee', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617343, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /v2/rerank, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096b6b30beb46f', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617323, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /v1/rerank, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096b687ce4df19', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617320, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /rerank, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096b333e2a97c1', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617267, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /v1/audio/translations, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096b1aadf17272', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617242, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /v1/audio/transcriptions, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096afb2f80568c', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617211, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /v1/score, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096af85aa507e8', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617208, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /score, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096ac194b314d6', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617153, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /classify, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096ab4bfb26132', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617140, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /pooling, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096a93d0cd2f56', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617107, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /v1/embeddings, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096a90d4726518', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617104, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /v1/completions, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096a6687c3dcb7', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617062, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /v1/chat/completions, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096a63f8a4d24a', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617059, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /v1/responses/{response_id}/cancel, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096a2cc30c3a90', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617004, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /v1/responses/{response_id}, Methods: GET')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096a1d633cb5ca', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616989, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /v1/responses, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd000969f9520f41c2', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616953, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /version, Methods: GET')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd000969dc58a3991a', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616924, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /v1/models, Methods: GET')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd000969bd6a3c0edc', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616893, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /detokenize, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd000969a772f607d5', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616871, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /tokenize, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd0009699bd45dfd14', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616859, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /ping, Methods: GET')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd000969685191c5f6', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616808, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /ping, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd0009696580e16fc4', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616805, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /load, Methods: GET')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096924abb7465b', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616740, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /health, Methods: GET')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd000969216beea8d7', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616737, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /redoc, Methods: GET, HEAD')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096906fcdf396a', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616710, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /docs/oauth2-redirect, Methods: GET, HEAD')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd000969039def3d64', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616707, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /docs, Methods: GET, HEAD')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd000968b0a80c7dc2', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616624, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /openapi.json, Methods: GET, HEAD')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd000968948052945c', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616596, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:36] Available routes are:')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd0009685040d4ad9f', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616528, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [api_server.py:1880] Starting vLLM API server 0 on http://0.0.0.0:8000')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096806acddeda7', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616454, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload="\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [serving_completion.py:77] Using default completion sampling params from model: {'top_k': 64, 'top_p': 0.95}")
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd0009632982fdae63', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 615209, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload="\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [serving_chat.py:134] Using default chat sampling params from model: {'top_k': 64, 'top_p': 0.95}")
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00095e17d99f33d4', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 613911, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload="\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [serving_responses.py:120] Using default chat sampling params from model: {'top_k': 64, 'top_p': 0.95}")
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00095ddce500bca4', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 613852, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload="\x1b[1;36m(APIServer pid=1)\x1b[0;0m WARNING 10-14 12:17:49 [__init__.py:1625] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.")
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00095202460b8f2b', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 610818, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload="\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [api_server.py:1611] Supported_tasks: ['generate']")
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd0008d72f459ba196', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 579375, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [loggers.py:142] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 277658')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1da000b603efdb6201f', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 46, 745534, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m INFO 10-14 12:17:46 [core.py:214] init engine (profile, create kv cache, warmup model) took 141.08 seconds')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1da000b2c41abb40115', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 46, 732225, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m INFO 10-14 12:17:46 [gpu_model_runner.py:2708] Graph capturing finished in 3 secs, took 1.64 GiB')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1da000b2bb2110b09bb', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 46, 732082, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd6480>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:02<00:00, 26.86it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1da000b2ba8507e373b', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 46, 732072, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd6480>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 66/67 [00:02<00:00, 28.11it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1da000a06b8de202aca', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 46, 657080, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd6480>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 63/67 [00:02<00:00, 27.92it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1da00086c955a733759', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 46, 552085, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd6480>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 60/67 [00:02<00:00, 28.52it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1da0006b42bc21ba454', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 46, 439339, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd6480>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 57/67 [00:02<00:00, 28.34it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1da00051f51cb05334f', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 46, 335697, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd6480>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 54/67 [00:01<00:00, 28.17it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1da000387e93ddd7045', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 46, 231401, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd6480>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 51/67 [00:01<00:00, 28.55it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1da0001dac56b90c3c2', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 46, 121541, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd6480>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 48/67 [00:01<00:00, 28.45it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1da000043c4406ac1de', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 46, 17348, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd6480>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 45/67 [00:01<00:00, 28.35it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d9000ded4c537feafc', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 45, 912716, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd6480>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 42/67 [00:01<00:00, 28.49it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d9000c4b3efb4051c0', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 45, 805694, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd6480>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 39/67 [00:01<00:00, 28.02it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d9000abfaaf0ae3b60', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 45, 704426, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd6480>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 36/67 [00:01<00:01, 27.29it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d90009370d2cad2372', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 45, 603917, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd6480>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 33/67 [00:01<00:01, 26.47it/s]')
LogEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Frequests', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1e4000c6ea60d56661e', severity='INFO', http_request={'requestMethod': 'POST', 'requestUrl': 'https://vllm-gemma-3-1b-it-665374072631.us-central1.run.app/v1/chat/completions', 'requestSize': '6708', 'status': 200, 'responseSize': '2699', 'userAgent': 'python-requests/2.32.3', 'remoteIp': '100.38.44.126', 'serverIp': '34.143.74.2', 'latency': '6.283451192s', 'protocol': 'HTTP/1.1'}, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 45, 523314, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace='projects/globalbiting-dev/traces/60d7bc4512bc99a69f7d2f231b1a5f33', span_id='ab2f6eabefe07050', trace_sampled=True, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92ce369310>, payload=None)
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d90007a829b011f452', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 45, 501801, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd6480>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 30/67 [00:01<00:01, 25.37it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d900061933145639db', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 45, 399667, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd6480>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 27/67 [00:01<00:01, 24.16it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d900047f6bd0e8e685', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 45, 294763, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd6480>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 24/67 [00:00<00:01, 27.24it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d9000221386b814727', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 45, 139576, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd6480>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|â–ˆâ–ˆâ–ˆâ–      | 21/67 [00:00<00:01, 26.87it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d900007fdb8e848749', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 45, 32731, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd6480>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|â–ˆâ–ˆâ–‹       | 18/67 [00:00<00:01, 26.44it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d8000e1c9bb4ea3860', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 44, 924827, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd6480>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|â–ˆâ–ˆâ–       | 15/67 [00:00<00:01, 26.21it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d8000c68f1821eee3e', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 44, 813297, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd6480>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|â–ˆâ–Š        | 12/67 [00:00<00:02, 26.21it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d8000aa98b3beb1a67', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 44, 698763, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd6480>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  13%|â–ˆâ–Ž        | 9/67 [00:00<00:02, 25.98it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d80008f0a86824a16d', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 44, 585896, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd6480>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|â–‰         | 6/67 [00:00<00:02, 25.91it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d800072ee0b19628b6', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 44, 470752, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd6480>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|â–         | 3/67 [00:00<00:02, 24.90it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d80005775b7195ab82', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 44, 358235, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd6480>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/67 [00:00<?, ?it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d800039fe5474f1e61', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 44, 237541, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd6480>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m ')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d7000988e7d802198d', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 43, 624871, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m INFO 10-14 12:17:43 [kv_cache_utils.py:1017] Maximum concurrency for 32,768 tokens per request: 92.12x')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d70009885036551399', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 43, 624720, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m INFO 10-14 12:17:43 [kv_cache_utils.py:1013] GPU KV cache size: 634,640 tokens')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d70009874c6aa1e97f', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 43, 624460, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m WARNING 10-14 12:17:43 [kv_cache_utils.py:971] Add 2 padding layers, may waste at most 9.09% KV cache memory')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d7000149dba8b7a62b', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 43, 84443, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m INFO 10-14 12:17:43 [gpu_worker.py:276] Available KV cache memory: 16.95 GiB')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1970000be1ae74ee2f0', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 16, 39, 48666, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd6480>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m   warnings.warn(')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1970000be160917d7b6', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 16, 39, 48662, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd6480>, payload="\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].")
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1970000be052b9934fd', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 16, 39, 48645, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd6480>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m /usr/local/lib/python3.12/dist-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. ')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea196000c7e7f01466e5e', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 16, 38, 818815, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m INFO 10-14 12:16:38 [monitor.py:34] torch.compile takes 54.25 s in total')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea18400096277c6df77aa', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 16, 20, 615031, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m INFO 10-14 12:16:20 [backends.py:215] Compiling a graph for dynamic shape takes 43.16 s')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea15e0008bfadfa8279ef', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 15, 42, 573357, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m INFO 10-14 12:15:42 [backends.py:194] Cache the graph for dynamic shape for later use')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea15a0009d5c846347c62', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 15, 38, 644552, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd6480>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m [rank0]:W1014 12:15:38.642000 51 torch/_inductor/utils.py:1250] [0/0] Not enough SMs to use max_autotune_gemm mode')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea158000e9c4c14b07e32', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 15, 36, 957516, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m INFO 10-14 12:15:36 [backends.py:559] Dynamo bytecode transform time: 11.09 s')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea158000e9c25097d10e4', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 15, 36, 957477, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload="\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m INFO 10-14 12:15:36 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/90a1db546d/rank_0_0/backbone for vLLM's torch.compile")
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea14d000a1c47f74aafca', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 15, 25, 662599, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b5d1fd40>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m INFO 10-14 12:15:25 [gpu_model_runner.py:2007] Model loading took 1.9147 GiB and 1.868946 seconds')

Based on your query: 'is there a way to execute or cache the "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): " part during cloud build stage or during the first cold start, so that subsequent cold starts can be shorter?', a potential solution could involve optimizing the cold start process by pre-warming the service or caching frequently used resources.
--------------------
[2025-10-15 02:10:44] User: ok, please provide more detailed possible solutions.
[2025-10-15 02:10:44] Bot: Logs for service 'vllm-gemma-3-1b-it' were found. Further analysis of these logs is needed to propose a specific solution. Here are some recent log entries:

TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea49a000eb41828b7daa0', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 29, 30, 963608, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd7050>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO:     Application shutdown complete.')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea49a000eb3dcce41b7dd', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 29, 30, 963548, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd7050>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO:     Waiting for application shutdown.')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea49a000d2b16acd694d3', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 29, 30, 862998, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd7050>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO:     Shutting down')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea498000a3531241afd4a', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 29, 28, 668977, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd7050>, payload='[rank0]:[W1014 12:29:28.295721832 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea497000be8b9b62ac013', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 29, 27, 780473, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:29:27 [launcher.py:101] Shutting down FastAPI HTTP server.')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea205000df1a22ad7afa6', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 18, 29, 913826, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:18:29 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 11.0%')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1fb000df0295c6442f6', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 18, 19, 913449, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:18:19 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 11.0%')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1f1000e326f3b2212fa', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 18, 9, 930415, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO:     169.254.169.126:12446 - "POST /v1/completions HTTP/1.1" 200 OK')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1f1000dea8bcb35ff8d', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 18, 9, 912011, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:18:09 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 11.0%')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1e90003f184e8ec77be', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 18, 1, 258436, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO:     169.254.169.126:57708 - "POST /v1/completions HTTP/1.1" 200 OK')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1e7000de97a6a73d565', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 59, 911738, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:59 [loggers.py:123] Engine 000: Avg prompt throughput: 155.0 tokens/s, Avg generation throughput: 147.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 11.0%')
LogEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Frequests', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1f1000e344d62d5ef58', severity='INFO', http_request={'requestMethod': 'POST', 'requestUrl': 'https://vllm-gemma-3-1b-it-665374072631.us-central1.run.app/v1/completions', 'requestSize': '2719', 'status': 200, 'responseSize': '4208', 'userAgent': 'python-requests/2.32.3', 'remoteIp': '100.38.44.126', 'serverIp': '34.143.74.2', 'latency': '11.080305415s', 'protocol': 'HTTP/1.1'}, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 58, 847941, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace='projects/globalbiting-dev/traces/4b19d51b4ddfaec25ed63b616484c1ec', span_id='b8670debf656c0ea', trace_sampled=True, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b47c1640>, payload=None)
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1e4000c6bc12a46cd3a', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 56, 814017, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO:     169.254.169.126:57722 - "POST /v1/chat/completions HTTP/1.1" 200 OK')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1de00090bb178397e41', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 50, 592817, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload="\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:50 [chat_utils.py:470] Detected the chat template content format to be 'openai'. You can set `--chat-template-content-format` to override this.")
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd000e988b445befd0', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 956555, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m WARNING 10-14 12:17:49 [tracing.py:130] Received a request with trace context but tracing is disabled')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fvarlog%2Fsystem', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd000de7c479a36f59', severity='INFO', http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 911300, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b47c14f0>, payload='STARTUP TCP probe succeeded after 1 attempt for container "vllm-gemma-3-1b-it-1" on port 8000.')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd000de58441925190', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 910724, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd7050>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO:     Application startup complete.')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd000ae60597c98d3d', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 714245, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd7050>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO:     Waiting for application startup.')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd000ae5fa7644d2ae', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 714234, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd7050>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO:     Started server process [1]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096c1f00ce7773', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617503, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /metrics, Methods: GET')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096bd385e9b8e0', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617427, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /invocations, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096bbbd94790d4', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617403, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /is_scaling_elastic_ep, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096b97f76486c6', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617367, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /scale_elastic_ep, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096b7f337b19ee', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617343, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /v2/rerank, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096b6b30beb46f', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617323, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /v1/rerank, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096b687ce4df19', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617320, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /rerank, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096b333e2a97c1', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617267, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /v1/audio/translations, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096b1aadf17272', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617242, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /v1/audio/transcriptions, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096afb2f80568c', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617211, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /v1/score, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096af85aa507e8', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617208, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /score, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096ac194b314d6', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617153, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /classify, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096ab4bfb26132', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617140, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /pooling, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096a93d0cd2f56', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617107, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /v1/embeddings, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096a90d4726518', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617104, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /v1/completions, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096a6687c3dcb7', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617062, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /v1/chat/completions, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096a63f8a4d24a', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617059, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /v1/responses/{response_id}/cancel, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096a2cc30c3a90', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 617004, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /v1/responses/{response_id}, Methods: GET')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096a1d633cb5ca', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616989, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /v1/responses, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd000969f9520f41c2', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616953, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /version, Methods: GET')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd000969dc58a3991a', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616924, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /v1/models, Methods: GET')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd000969bd6a3c0edc', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616893, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /detokenize, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd000969a772f607d5', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616871, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /tokenize, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd0009699bd45dfd14', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616859, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /ping, Methods: GET')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd000969685191c5f6', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616808, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /ping, Methods: POST')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd0009696580e16fc4', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616805, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /load, Methods: GET')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096924abb7465b', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616740, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /health, Methods: GET')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd000969216beea8d7', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616737, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /redoc, Methods: GET, HEAD')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096906fcdf396a', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616710, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /docs/oauth2-redirect, Methods: GET, HEAD')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd000969039def3d64', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616707, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /docs, Methods: GET, HEAD')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd000968b0a80c7dc2', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616624, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:44] Route: /openapi.json, Methods: GET, HEAD')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd000968948052945c', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616596, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [launcher.py:36] Available routes are:')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd0009685040d4ad9f', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616528, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [api_server.py:1880] Starting vLLM API server 0 on http://0.0.0.0:8000')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00096806acddeda7', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 616454, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload="\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [serving_completion.py:77] Using default completion sampling params from model: {'top_k': 64, 'top_p': 0.95}")
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd0009632982fdae63', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 615209, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload="\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [serving_chat.py:134] Using default chat sampling params from model: {'top_k': 64, 'top_p': 0.95}")
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00095e17d99f33d4', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 613911, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload="\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [serving_responses.py:120] Using default chat sampling params from model: {'top_k': 64, 'top_p': 0.95}")
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00095ddce500bca4', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 613852, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload="\x1b[1;36m(APIServer pid=1)\x1b[0;0m WARNING 10-14 12:17:49 [__init__.py:1625] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.")
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd00095202460b8f2b', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 610818, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload="\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [api_server.py:1611] Supported_tasks: ['generate']")
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1dd0008d72f459ba196', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 49, 579375, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(APIServer pid=1)\x1b[0;0m INFO 10-14 12:17:49 [loggers.py:142] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 277658')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1da000b603efdb6201f', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 46, 745534, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m INFO 10-14 12:17:46 [core.py:214] init engine (profile, create kv cache, warmup model) took 141.08 seconds')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1da000b2c41abb40115', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 46, 732225, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m INFO 10-14 12:17:46 [gpu_model_runner.py:2708] Graph capturing finished in 3 secs, took 1.64 GiB')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1da000b2bb2110b09bb', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 46, 732082, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd7050>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:02<00:00, 26.86it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1da000b2ba8507e373b', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 46, 732072, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd7050>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 66/67 [00:02<00:00, 28.11it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1da000a06b8de202aca', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 46, 657080, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd7050>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 63/67 [00:02<00:00, 27.92it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1da00086c955a733759', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 46, 552085, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd7050>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 60/67 [00:02<00:00, 28.52it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1da0006b42bc21ba454', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 46, 439339, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd7050>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 57/67 [00:02<00:00, 28.34it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1da00051f51cb05334f', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 46, 335697, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd7050>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 54/67 [00:01<00:00, 28.17it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1da000387e93ddd7045', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 46, 231401, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd7050>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 51/67 [00:01<00:00, 28.55it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1da0001dac56b90c3c2', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 46, 121541, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd7050>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 48/67 [00:01<00:00, 28.45it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1da000043c4406ac1de', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 46, 17348, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd7050>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 45/67 [00:01<00:00, 28.35it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d9000ded4c537feafc', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 45, 912716, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd7050>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 42/67 [00:01<00:00, 28.49it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d9000c4b3efb4051c0', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 45, 805694, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd7050>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 39/67 [00:01<00:00, 28.02it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d9000abfaaf0ae3b60', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 45, 704426, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd7050>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 36/67 [00:01<00:01, 27.29it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d90009370d2cad2372', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 45, 603917, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd7050>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 33/67 [00:01<00:01, 26.47it/s]')
LogEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Frequests', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1e4000c6ea60d56661e', severity='INFO', http_request={'requestMethod': 'POST', 'requestUrl': 'https://vllm-gemma-3-1b-it-665374072631.us-central1.run.app/v1/chat/completions', 'requestSize': '6708', 'status': 200, 'responseSize': '2699', 'userAgent': 'python-requests/2.32.3', 'remoteIp': '100.38.44.126', 'serverIp': '34.143.74.2', 'latency': '6.283451192s', 'protocol': 'HTTP/1.1'}, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 45, 523314, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace='projects/globalbiting-dev/traces/60d7bc4512bc99a69f7d2f231b1a5f33', span_id='ab2f6eabefe07050', trace_sampled=True, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b47c1640>, payload=None)
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d90007a829b011f452', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 45, 501801, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd7050>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 30/67 [00:01<00:01, 25.37it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d900061933145639db', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 45, 399667, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd7050>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 27/67 [00:01<00:01, 24.16it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d900047f6bd0e8e685', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 45, 294763, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd7050>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 24/67 [00:00<00:01, 27.24it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d9000221386b814727', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 45, 139576, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd7050>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|â–ˆâ–ˆâ–ˆâ–      | 21/67 [00:00<00:01, 26.87it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d900007fdb8e848749', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 45, 32731, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd7050>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|â–ˆâ–ˆâ–‹       | 18/67 [00:00<00:01, 26.44it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d8000e1c9bb4ea3860', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 44, 924827, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd7050>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|â–ˆâ–ˆâ–       | 15/67 [00:00<00:01, 26.21it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d8000c68f1821eee3e', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 44, 813297, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd7050>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|â–ˆâ–Š        | 12/67 [00:00<00:02, 26.21it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d8000aa98b3beb1a67', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 44, 698763, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd7050>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  13%|â–ˆâ–Ž        | 9/67 [00:00<00:02, 25.98it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d80008f0a86824a16d', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 44, 585896, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd7050>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|â–‰         | 6/67 [00:00<00:02, 25.91it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d800072ee0b19628b6', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 44, 470752, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd7050>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|â–         | 3/67 [00:00<00:02, 24.90it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d80005775b7195ab82', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 44, 358235, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd7050>, payload='Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/67 [00:00<?, ?it/s]')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d800039fe5474f1e61', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 44, 237541, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd7050>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m ')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d7000988e7d802198d', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 43, 624871, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m INFO 10-14 12:17:43 [kv_cache_utils.py:1017] Maximum concurrency for 32,768 tokens per request: 92.12x')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d70009885036551399', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 43, 624720, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m INFO 10-14 12:17:43 [kv_cache_utils.py:1013] GPU KV cache size: 634,640 tokens')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d70009874c6aa1e97f', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 43, 624460, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m WARNING 10-14 12:17:43 [kv_cache_utils.py:971] Add 2 padding layers, may waste at most 9.09% KV cache memory')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1d7000149dba8b7a62b', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 17, 43, 84443, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m INFO 10-14 12:17:43 [gpu_worker.py:276] Available KV cache memory: 16.95 GiB')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1970000be1ae74ee2f0', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 16, 39, 48666, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd7050>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m   warnings.warn(')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1970000be160917d7b6', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 16, 39, 48662, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd7050>, payload="\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].")
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea1970000be052b9934fd', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 16, 39, 48645, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd7050>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m /usr/local/lib/python3.12/dist-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. ')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea196000c7e7f01466e5e', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 16, 38, 818815, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m INFO 10-14 12:16:38 [monitor.py:34] torch.compile takes 54.25 s in total')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea18400096277c6df77aa', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 16, 20, 615031, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m INFO 10-14 12:16:20 [backends.py:215] Compiling a graph for dynamic shape takes 43.16 s')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea15e0008bfadfa8279ef', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 15, 42, 573357, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m INFO 10-14 12:15:42 [backends.py:194] Cache the graph for dynamic shape for later use')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstderr', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea15a0009d5c846347c62', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 15, 38, 644552, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92cedd7050>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m [rank0]:W1014 12:15:38.642000 51 torch/_inductor/utils.py:1250] [0/0] Not enough SMs to use max_autotune_gemm mode')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea158000e9c4c14b07e32', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 15, 36, 957516, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m INFO 10-14 12:15:36 [backends.py:559] Dynamo bytecode transform time: 11.09 s')
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea158000e9c25097d10e4', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 15, 36, 957477, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'project_id': 'globalbiting-dev', 'configuration_name': 'vllm-gemma-3-1b-it', 'service_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'revision_name': 'vllm-gemma-3-1b-it-00036-2dd'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload="\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m INFO 10-14 12:15:36 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/90a1db546d/rank_0_0/backbone for vLLM's torch.compile")
TextEntry(log_name='projects/globalbiting-dev/logs/run.googleapis.com%2Fstdout', labels={'instanceId': '0069c7a988fa1f68e2cb8cfcf77bbe6bdb134d0e4f6ceb7babb3ab872b6e22ed4828fb27d732a9f0b8001da9ea36e16b5750b48b111c08fa2630e53e20054f160ee347dba7412d754a78fca94d2f0efd5e18ba87aadfa3777cd8'}, insert_id='68eea14d000a1c47f74aafca', severity=None, http_request=None, timestamp=datetime.datetime(2025, 10, 14, 19, 15, 25, 662599, tzinfo=datetime.timezone.utc), resource=Resource(type='cloud_run_revision', labels={'revision_name': 'vllm-gemma-3-1b-it-00036-2dd', 'configuration_name': 'vllm-gemma-3-1b-it', 'location': 'us-central1', 'service_name': 'vllm-gemma-3-1b-it', 'project_id': 'globalbiting-dev'}), trace=None, span_id=None, trace_sampled=None, source_location=None, operation=None, logger=<google.cloud.logging_v2.logger.Logger object at 0x7f92b560f9e0>, payload='\x1b[1;36m(EngineCore_0 pid=51)\x1b[0;0m INFO 10-14 12:15:25 [gpu_model_runner.py:2007] Model loading took 1.9147 GiB and 1.868946 seconds')

Based on your query: 'ok, please provide more detailed possible solutions.', a potential solution could involve optimizing the cold start process by pre-warming the service or caching frequently used resources.
--------------------
[2025-10-15 02:13:47] User: for cloud run service vllm-gemma-3-1b-it, can you tell if the performance has improved from yesterday's initial requests to today's?
[2025-10-15 02:13:47] Bot: Yes, based on the logs for the Cloud Run service `vllm-gemma-3-1b-it`, it appears there has been a performance improvement between the earlier and later requests on October 14, 2025.

Here is a summary of the performance comparison:

*   **Token Throughput:** There is a noticeable improvement in throughput.
    *   **Earlier Requests (around 17:58 UTC):** The peak average generation throughput was **108.1 tokens/s**.
    *   **Later Requests (around 19:17 UTC):** The peak average generation throughput increased to **147.4 tokens/s**, with an average prompt throughput of **155.0 tokens/s**.

*   **Request Latency:** The overall request latency, especially for the initial "cold start" requests, has remained consistent.
    *   **Earlier Cold Start (17:54 UTC):** An initial request took **~219.2 seconds**.
    *   **Later Cold Start (19:14 UTC):** A similar initial request took **~218.5 seconds**.
    *   Subsequent "warm" requests in both periods had latencies in the 5-11 second range.

*   **Instance Startup Time:** The time to initialize the vLLM engine shows a slight improvement.
    *   **Earlier Instance (started ~17:55 UTC):** Engine initialization took **144.48 seconds**.
    *   **Later Instance (started ~19:14 UTC):** Engine initialization took **141.08 seconds**.

In summary, while the initial request latency due to cold starts is similar, the core performance measured by **token generation throughput has improved** in the later set of requests.
--------------------
[2025-10-15 02:18:01] User: for cloud run service vllm-gemma-3-1b-it,is there a way to execute or cache the "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): " part during cloud build stage or during the first cold start, so that subsequent cold starts can be shorter?
[2025-10-15 02:18:01] Bot: Logs for service 'vllm-gemma-3-1b-it' were found, but no specific entries directly matching keywords from your query ('for cloud run service vllm-gemma-3-1b-it,is there a way to execute or cache the "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): " part during cloud build stage or during the first cold start, so that subsequent cold starts can be shorter?') were identified in the top 100 logs. Further manual analysis of the full logs might be necessary. However, general solutions for optimizing cold starts and caching include:
- Pre-warming your Cloud Run service with dummy requests or minimum instances.
- Optimizing your application's startup time by reducing dependencies or lazy-loading resources.
- Utilizing build-time optimizations to pre-compile assets or graphs if applicable.
--------------------
[2025-10-15 02:35:29] User: for cloud run service vllm-gemma-3-1b-it,is there a way to execute or cache the "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): " part during cloud build stage or during the first cold start, so that subsequent cold starts can be shorter?
[2025-10-15 02:35:29] Bot: Of course. As an expert in Google Cloud Run log analysis and performance optimization, I can provide a detailed solution based on your query and the provided logs.

Here is a comprehensive analysis and a set of actionable recommendations.

### Executive Summary

Your primary goal is to reduce cold start times for your `vllm-gemma-3-1b-it` service by caching the "Capturing CUDA graphs" step.

*   **Log Analysis:** The logs show that "Capturing CUDA graphs" takes approximately **3 seconds**. However, a much larger bottleneck is **`torch.compile`**, which takes over **54 seconds** during the initial startup.
*   **Core Problem:** You cannot cache the CUDA graph during the Cloud Build stage because it is highly specific to the GPU hardware and drivers available at *runtime*, which are not present during the build.
*   **Solution Overview:** We can address this by:
    1.  **Pre-compiling and Caching `torch.compile` Artifacts:** This is the most significant optimization. We will perform the slow compilation step during the Docker image build and bake the resulting cache into the final image.
    2.  **Optimizing CUDA Graph Capture:** We will use a Cloud Run startup probe to trigger the graph capture *before* the first user request arrives, effectively moving this latency out of the user-facing path.
    3.  **Implementing General Best Practices:** Using `min-instances` to eliminate cold starts entirely.

---

### 1. Detailed Log Analysis

By analyzing the timestamps in your logs, we can construct a timeline of a cold start for instance `0069c7a...`:

| Timestamp (UTC)      | Elapsed Time | Event                                                     | Analysis                                                                                                                                                             |
| -------------------- | ------------ | --------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `19:15:25`           | 0s           | `Model loading took ... 1.868946 seconds`                 | Initial model loading from disk/memory is very fast. The time before this is spent downloading the model into the container's ephemeral storage.                          |
| `19:15:36`           | +11s         | `Dynamo bytecode transform time: 11.09 s`                 | The first phase of `torch.compile` begins.                                                                                                                           |
| `19:16:20`           | +55s         | `Compiling a graph for dynamic shape takes 43.16 s`       | The second, much longer, phase of `torch.compile`.                                                                                                                   |
| `19:16:38`           | +1m 13s      | `torch.compile takes 54.25 s in total`                    | **This is the primary bottleneck.** The application spends nearly a minute just on model compilation. The log also shows a cache directory is being used: `/root/.cache/vllm/torch_compile_cache`. |
| `19:17:44`           | +2m 19s      | `Capturing CUDA graphs ... 0%`                            | The process you asked about begins. This is a vLLM-specific optimization that captures the sequence of CUDA operations.                                                 |
| `19:17:46`           | +2m 21s      | `Capturing CUDA graphs ... 100%` & `finished in 3 secs`    | The CUDA graph capture process completes. It took about **3 seconds**. While noticeable, it is a much smaller contributor to the cold start than `torch.compile`.            |
| `19:17:49`           | +2m 24s      | `Application startup complete.`                           | The FastAPI/uvicorn server is now fully initialized and ready to accept traffic. The entire startup process took nearly 2.5 minutes.                                   |
| `19:17:45` & `19:17:58` | -            | First requests arrive with high latency (6s & 11s)        | Requests are arriving *while* the application is still performing its final warmup steps (like CUDA graph capture), leading to high initial latency.                     |

**Conclusion:** While your query focuses on the 3-second CUDA graph capture, the 54-second `torch.compile` step is the most critical target for optimization.

---

### 2. Recommendations to Reduce Cold Start Time

Here are concrete, actionable steps to dramatically shorten your service's startup time.

#### Recommendation 1: Pre-warm the `torch.compile` Cache in Your Dockerfile (High Impact)

The 54-second compilation time can be eliminated by running it once during the `docker build` process and copying the cache into your final container image. This is achieved with a multi-stage Dockerfile.

**Step 1: Create a Warm-up Script**

Create a simple Python script named `warmup.py` that loads your model. This will trigger the compilation and populate the cache directory.

```python
# warmup.py
from vllm.engine.arg_utils import AsyncEngineArgs
from vllm.engine.async_llm_engine import AsyncLLMEngine

print("Starting model warmup to trigger torch.compile...")

# Use the same arguments you use to run your server
engine_args = AsyncEngineArgs(
    model='google/gemma-2-9b-it', # Or your specific model path
    # Add other crucial args like tensor_parallel_size, dtype, etc.
    # Ensure these match your runtime configuration EXACTLY.
    enforce_eager=False, # Make sure torch.compile is enabled
)

# Initializing the engine will trigger the compilation
engine = AsyncLLMEngine.from_engine_args(engine_args)

print("Warmup complete. Torch compile cache should be populated.")
```

**Step 2: Modify Your Dockerfile to be Multi-stage**

This new Dockerfile will have a `builder` stage to run the warmup script and a final stage that copies the cache.

```dockerfile
# Stage 1: Builder - This stage has the full build environment
FROM vllm/vllm-openai:latest AS builder

# Set env var to ensure compile cache is stored within the container
ENV VLLM_TORCH_COMPILE_CACHE_DIR=/root/.cache/vllm/torch_compile_cache

# Set env var for torch compile backend (optional but good practice)
ENV VLLM_TORCH_COMPILE_BACKEND=inductor

# Copy your warmup script
COPY warmup.py .

# Run the warmup script. This is the slow step that will now happen
# during your Cloud Build, not during a cold start.
# This will download the model and populate the cache directory.
RUN python warmup.py

# ---
# Stage 2: Final Image - A lean image with the pre-warmed cache
FROM vllm/vllm-openai:latest

# Set the same environment variables for consistency
ENV VLLM_TORCH_COMPILE_CACHE_DIR=/root/.cache/vllm/torch_compile_cache
ENV VLLM_TORCH_COMPILE_BACKEND=inductor

# Copy the application code
COPY . .

# Copy the pre-downloaded model from the builder stage
# Find the huggingface cache path with `huggingface-cli cache info`
COPY --from=builder /root/.cache/huggingface /root/.cache/huggingface

# *** The most important step ***
# Copy the populated torch.compile cache from the builder stage
COPY --from=builder ${VLLM_TORCH_COMPILE_CACHE_DIR} ${VLLM_TORCH_COMPILE_CACHE_DIR}

# Your original CMD to start the server
CMD ["python", "-m", "vllm.entrypoints.openai.api_server", "--model", "google/gemma-2-9b-it"]
```

**Expected Result:** The 54-second `torch.compile` step will be eliminated from your runtime cold start. The application will find the cache already populated in the container image and start much faster.

---

#### Recommendation 2: Trigger CUDA Graph Capture with a Startup Probe (Medium Impact)

To handle the remaining ~3-second latency from CUDA graph capture, we can force it to happen during the container startup sequence before Cloud Run sends it live traffic.

**Step 1: Add a Warm-up Endpoint to Your Application**

If you are using a custom FastAPI server, add a simple `/warmup` endpoint that runs a dummy inference call. If you are using the default vLLM server, this is more complex, but you can achieve a similar effect by sending a request to the completions endpoint from the probe itself.

**Step 2: Configure a Startup Probe in Cloud Run**

Deploy your service with flags for `startup-cpu-boost` (to accelerate the process) and a startup probe that calls your server. The probe ensures Cloud Run waits for the warmup to complete before marking the instance as "healthy" and routing user traffic to it.

```bash
gcloud run deploy vllm-gemma-3-1b-it \
  --image gcr.io/your-project/your-image:latest \
  --startup-cpu-boost \
  --probe-startup-http "/v1/models" \
  --probe-startup-opts "initial-delay-seconds=10,period-seconds=5,failure-threshold=24,timeout-seconds=5"
```
*   **`--probe-startup-http "/v1/models"`**: We use a simple, lightweight endpoint like `/v1/models` as the probe target. The first request to *any* inference endpoint after the server starts will trigger the graph capture. The act of the probe checking the server might be enough to initiate it. For a guaranteed trigger, you would need a custom warmup endpoint that performs a generation.
*   **`probe-startup-opts`**: These values give the container up to 2 minutes (`24 failures * 5 seconds`) to become ready, which should be ample time for the model to load from the cached state and perform the graph capture.

**Expected Result:** The ~3-second delay for CUDA graph capture is moved from the first user request to the container's internal startup time, improving the perceived performance for the first user.

---

#### Recommendation 3: Eliminate Cold Starts with `min-instances` (Highest Reliability)

If your application requires consistently low latency and you want to eliminate cold starts entirely, the most effective solution is to configure a minimum number of container instances.

```bash
gcloud run services update vllm-gemma-3-1b-it \
  --min-instances 1
```

*   **How it works:** This command tells Cloud Run to always keep at least one instance of your service running and ready. This "warm" instance has already completed its entire startup process (model loading, compilation, graph capture) and can serve requests instantly.
*   **Trade-off:** The primary consideration is cost. You will be billed for the time the instance is idle, waiting for requests. For a GPU-enabled service, this cost is significant. This is the classic trade-off between performance/availability and cost.

By implementing these recommendations, you will address not only the CUDA graph capture time but also the much larger `torch.compile` bottleneck, leading to substantially shorter and more predictable cold starts for your Cloud Run service.
--------------------
[2025-10-15 02:38:54] User: please create an issue in github repository vllm-container-prewarm  for recommendation 1 
[2025-10-15 02:38:54] Bot: Of course. As an expert in Google Cloud Run log analysis and performance optimization, I can provide a detailed solution based on your query and the provided logs.

Here is a comprehensive analysis and a set of actionable recommendations.

### Executive Summary

Your primary goal is to reduce cold start times for your `vllm-gemma-3-1b-it` service by caching the "Capturing CUDA graphs" step.

*   **Log Analysis:** The logs show that "Capturing CUDA graphs" takes approximately **3 seconds**. However, a much larger bottleneck is **`torch.compile`**, which takes over **54 seconds** during the initial startup.
*   **Core Problem:** You cannot cache the CUDA graph during the Cloud Build stage because it is highly specific to the GPU hardware and drivers available at *runtime*, which are not present during the build.
*   **Solution Overview:** We can address this by:
    1.  **Pre-compiling and Caching `torch.compile` Artifacts:** This is the most significant optimization. We will perform the slow compilation step during the Docker image build and bake the resulting cache into the final image.
    2.  **Optimizing CUDA Graph Capture:** We will use a Cloud Run startup probe to trigger the graph capture *before* the first user request arrives, effectively moving this latency out of the user-facing path.
    3.  **Implementing General Best Practices:** Using `min-instances` to eliminate cold starts entirely.

---

### 1. Detailed Log Analysis

By analyzing the timestamps in your logs, we can construct a timeline of a cold start for instance `0069c7a...`:

| Timestamp (UTC)      | Elapsed Time | Event                                                     | Analysis                                                                                                                                                             |
| -------------------- | ------------ | --------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `19:15:25`           | 0s           | `Model loading took ... 1.868946 seconds`                 | Initial model loading from disk/memory is very fast. The time before this is spent downloading the model into the container's ephemeral storage.                          |
| `19:15:36`           | +11s         | `Dynamo bytecode transform time: 11.09 s`                 | The first phase of `torch.compile` begins.                                                                                                                           |
| `19:16:20`           | +55s         | `Compiling a graph for dynamic shape takes 43.16 s`       | The second, much longer, phase of `torch.compile`.                                                                                                                   |
| `19:16:38`           | +1m 13s      | `torch.compile takes 54.25 s in total`                    | **This is the primary bottleneck.** The application spends nearly a minute just on model compilation. The log also shows a cache directory is being used: `/root/.cache/vllm/torch_compile_cache`. |
| `19:17:44`           | +2m 19s      | `Capturing CUDA graphs ... 0%`                            | The process you asked about begins. This is a vLLM-specific optimization that captures the sequence of CUDA operations.                                                 |
| `19:17:46`           | +2m 21s      | `Capturing CUDA graphs ... 100%` & `finished in 3 secs`    | The CUDA graph capture process completes. It took about **3 seconds**. While noticeable, it is a much smaller contributor to the cold start than `torch.compile`.            |
| `19:17:49`           | +2m 24s      | `Application startup complete.`                           | The FastAPI/uvicorn server is now fully initialized and ready to accept traffic. The entire startup process took nearly 2.5 minutes.                                   |
| `19:17:45` & `19:17:58` | -            | First requests arrive with high latency (6s & 11s)        | Requests are arriving *while* the application is still performing its final warmup steps (like CUDA graph capture), leading to high initial latency.                     |

**Conclusion:** While your query focuses on the 3-second CUDA graph capture, the 54-second `torch.compile` step is the most critical target for optimization.

---

### 2. Recommendations to Reduce Cold Start Time

Here are concrete, actionable steps to dramatically shorten your service's startup time.

#### Recommendation 1: Pre-warm the `torch.compile` Cache in Your Dockerfile (High Impact)

The 54-second compilation time can be eliminated by running it once during the `docker build` process and copying the cache into your final container image. This is achieved with a multi-stage Dockerfile.

**Step 1: Create a Warm-up Script**

Create a simple Python script named `warmup.py` that loads your model. This will trigger the compilation and populate the cache directory.

```python
# warmup.py
from vllm.engine.arg_utils import AsyncEngineArgs
from vllm.engine.async_llm_engine import AsyncLLMEngine

print("Starting model warmup to trigger torch.compile...")

# Use the same arguments you use to run your server
engine_args = AsyncEngineArgs(
    model='google/gemma-2-9b-it', # Or your specific model path
    # Add other crucial args like tensor_parallel_size, dtype, etc.
    # Ensure these match your runtime configuration EXACTLY.
    enforce_eager=False, # Make sure torch.compile is enabled
)

# Initializing the engine will trigger the compilation
engine = AsyncLLMEngine.from_engine_args(engine_args)

print("Warmup complete. Torch compile cache should be populated.")
```

**Step 2: Modify Your Dockerfile to be Multi-stage**

This new Dockerfile will have a `builder` stage to run the warmup script and a final stage that copies the cache.

```dockerfile
# Stage 1: Builder - This stage has the full build environment
FROM vllm/vllm-openai:latest AS builder

# Set env var to ensure compile cache is stored within the container
ENV VLLM_TORCH_COMPILE_CACHE_DIR=/root/.cache/vllm/torch_compile_cache

# Set env var for torch compile backend (optional but good practice)
ENV VLLM_TORCH_COMPILE_BACKEND=inductor

# Copy your warmup script
COPY warmup.py .

# Run the warmup script. This is the slow step that will now happen
# during your Cloud Build, not during a cold start.
# This will download the model and populate the cache directory.
RUN python warmup.py

# ---
# Stage 2: Final Image - A lean image with the pre-warmed cache
FROM vllm/vllm-openai:latest

# Set the same environment variables for consistency
ENV VLLM_TORCH_COMPILE_CACHE_DIR=/root/.cache/vllm/torch_compile_cache
ENV VLLM_TORCH_COMPILE_BACKEND=inductor

# Copy the application code
COPY . .

# Copy the pre-downloaded model from the builder stage
# Find the huggingface cache path with `huggingface-cli cache info`
COPY --from=builder /root/.cache/huggingface /root/.cache/huggingface

# *** The most important step ***
# Copy the populated torch.compile cache from the builder stage
COPY --from=builder ${VLLM_TORCH_COMPILE_CACHE_DIR} ${VLLM_TORCH_COMPILE_CACHE_DIR}

# Your original CMD to start the server
CMD ["python", "-m", "vllm.entrypoints.openai.api_server", "--model", "google/gemma-2-9b-it"]
```

**Expected Result:** The 54-second `torch.compile` step will be eliminated from your runtime cold start. The application will find the cache already populated in the container image and start much faster.

---

#### Recommendation 2: Trigger CUDA Graph Capture with a Startup Probe (Medium Impact)

To handle the remaining ~3-second latency from CUDA graph capture, we can force it to happen during the container startup sequence before Cloud Run sends it live traffic.

**Step 1: Add a Warm-up Endpoint to Your Application**

If you are using a custom FastAPI server, add a simple `/warmup` endpoint that runs a dummy inference call. If you are using the default vLLM server, this is more complex, but you can achieve a similar effect by sending a request to the completions endpoint from the probe itself.

**Step 2: Configure a Startup Probe in Cloud Run**

Deploy your service with flags for `startup-cpu-boost` (to accelerate the process) and a startup probe that calls your server. The probe ensures Cloud Run waits for the warmup to complete before marking the instance as "healthy" and routing user traffic to it.

```bash
gcloud run deploy vllm-gemma-3-1b-it \
  --image gcr.io/your-project/your-image:latest \
  --startup-cpu-boost \
  --probe-startup-http "/v1/models" \
  --probe-startup-opts "initial-delay-seconds=10,period-seconds=5,failure-threshold=24,timeout-seconds=5"
```
*   **`--probe-startup-http "/v1/models"`**: We use a simple, lightweight endpoint like `/v1/models` as the probe target. The first request to *any* inference endpoint after the server starts will trigger the graph capture. The act of the probe checking the server might be enough to initiate it. For a guaranteed trigger, you would need a custom warmup endpoint that performs a generation.
*   **`probe-startup-opts`**: These values give the container up to 2 minutes (`24 failures * 5 seconds`) to become ready, which should be ample time for the model to load from the cached state and perform the graph capture.

**Expected Result:** The ~3-second delay for CUDA graph capture is moved from the first user request to the container's internal startup time, improving the perceived performance for the first user.

---

#### Recommendation 3: Eliminate Cold Starts with `min-instances` (Highest Reliability)

If your application requires consistently low latency and you want to eliminate cold starts entirely, the most effective solution is to configure a minimum number of container instances.

```bash
gcloud run services update vllm-gemma-3-1b-it \
  --min-instances 1
```

*   **How it works:** This command tells Cloud Run to always keep at least one instance of your service running and ready. This "warm" instance has already completed its entire startup process (model loading, compilation, graph capture) and can serve requests instantly.
*   **Trade-off:** The primary consideration is cost. You will be billed for the time the instance is idle, waiting for requests. For a GPU-enabled service, this cost is significant. This is the classic trade-off between performance/availability and cost.

By implementing these recommendations, you will address not only the CUDA graph capture time but also the much larger `torch.compile` bottleneck, leading to substantially shorter and more predictable cold starts for your Cloud Run service.
--------------------
[2025-10-15 02:42:49] User: for cloud run service vllm-gemma-3-1b-it,is there a way to execute or cache the "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): " part during cloud build stage or during the first cold start, so that subsequent cold starts can be shorter?
[2025-10-15 02:42:49] Bot: Of course. As an expert in Google Cloud Run performance optimization, I can analyze these logs and provide a detailed solution to your query about caching the CUDA graph capture step.

This is an excellent question that gets to the heart of optimizing GPU-based model serving on serverless platforms. Let's break down the logs and formulate a plan.

### Executive Summary (TL;DR)

1.  **Direct Caching:** Unfortunately, you **cannot cache the CUDA graph capture across cold starts** in a standard Cloud Run setup. Cloud Run instances are ephemeral; when an instance scales to zero, its in-memory state and filesystem changes are lost. Similarly, this step cannot be performed during a standard `cloud build` because it requires an active GPU.
2.  **Log Analysis:** The logs show that CUDA graph capturing (`Capturing CUDA graphs...`) takes approximately **3 seconds** (`19:17:43` to `19:17:46`). However, a much larger contributor to your cold start is **`torch.compile`**, which takes **~54 seconds**.
3.  **Primary Recommendation:** The most effective and recommended solution to eliminate this entire startup latency is to configure your Cloud Run service with a **minimum number of instances (`min-instances=1`)**. This keeps one instance constantly warm and ready to serve requests, bypassing the cold start process entirely for incoming traffic.
4.  **Advanced (Experimental) Recommendation:** It is theoretically possible to pre-warm the `torch.compile` cache during the image build process, which could shave off a significant portion of the startup time. This is complex and has caveats.

---

### 1. Detailed Log Analysis

By examining the timestamps in your logs, we can reconstruct the timeline of a cold start for instance `0069c7a...`:

| Timestamp (UTC) | Duration | Log Message / Event | Analysis |
| :--- | :--- | :--- | :--- |
| `19:15:25` | ~2s | `Model loading took ... 1.868946 seconds` | The actual model weight loading from disk to GPU memory is very fast. |
| `19:15:25` - `19:16:38` | **~73s** | `torch.compile` related logs | The application is spending a significant amount of time here. |
| `19:16:20` | (part of above) | `Compiling a graph for dynamic shape takes 43.16 s` | This is a `torch.compile` JIT (Just-In-Time) compilation step. It's a major source of startup latency. |
| `19:16:38` | **54.25s** | `torch.compile takes 54.25 s in total` | **This is the single largest contributor to your cold start time.** |
| `19:17:43` | - | `Available KV cache memory: 16.95 GiB` | The vLLM engine begins its final initialization and memory allocation. |
| `19:17:44` | - | `Capturing CUDA graphs ... 0%` | **The user's point of interest begins here.** |
| `19:17:46` | **~3s** | `Graph capturing finished in 3 secs` | This step is relatively fast compared to `torch.compile`, but still adds to the total time. |
| `19:17:46` | **141.08s** | `init engine ... took 141.08 seconds` | This log summarizes the entire vLLM engine initialization, from model loading through compilation and graph capture. The total cold start is over 2 minutes. |
| `19:17:49` | - | `Application startup complete.` | The container is now ready to accept requests. |
| `19:17:50` | - | First request begins processing. | The first request after this startup will have a high latency, incorporating this entire setup time. |
| `19:29:30` | - | `Application shutdown complete.` | After a period of inactivity (~12 minutes), Cloud Run scaled the instance down to zero. The next request will trigger another full cold start. |

### 2. Answering Your Specific Question

**"Is there a way to execute or cache the "Capturing CUDA graphs" part during the build stage or after the first cold start?"**

The direct answer is **no, for two fundamental reasons**:

1.  **Ephemeral Instances:** Cloud Run instances are stateless and ephemeral. When your service scales down to zero, the instance is terminated. Any caches created in memory (like the captured CUDA graph) or on the local container filesystem (like the `torch.compile` cache at `/root/.cache/`) are permanently deleted. The next request will start a brand new, clean instance from the container image, repeating the entire initialization process.
2.  **Build Environment Limitations:** Standard Cloud Build workers do not have GPUs. CUDA graph capturing is an operation that records a sequence of kernels to be executed on a GPU. It requires an initialized CUDA context, a loaded model on the GPU, and an active GPU device, none of which are available during a typical container build.

### 3. Concrete, Actionable Recommendations

While you can't cache these specific runtime artifacts across cold starts, you can solve the underlying problem: **avoiding the cold start itself.**

#### Recommendation 1: Use `min-instances` (Most Effective Solution)

This is the canonical solution for latency-sensitive applications with long initialization times on Cloud Run. By setting a minimum number of instances, you instruct Cloud Run to always keep at least one container instance running, warm, and ready.

*   **How it works:** The instance will go through the ~141-second cold start process once when it's first provisioned. After that, it stays alive, having already loaded the model, compiled the graphs, and captured the CUDA graphs. All subsequent requests are routed to this warm instance, resulting in very low latency.
*   **Trade-off:** You will be billed for the vCPU, memory, and GPU of the instance for the entire time it is kept running, even when idle. For a production service that needs consistent low-latency performance, this cost is generally acceptable.

**Action:** To set `min-instances` to 1, run the following `gcloud` command:

```bash
gcloud run services update vllm-gemma-3-1b-it \
  --region us-central1 \
  --min-instances=1
```

#### Recommendation 2: Pre-warm `torch.compile` Cache (Advanced)

Since `torch.compile` is the biggest offender (~54 seconds), you can attempt to pre-generate its cache during the container build and bake it into the image. This is complex and may not be fully portable, but it can significantly reduce the startup time *if it works*.

**This will not cache the CUDA graph capture**, but it addresses the larger problem.

**Steps:**

1.  **Create a "warming" script:** Create a Python script (`warmup.py`) that performs the minimum work needed to trigger the compilation.

    ```python
    # warmup.py
    # This is a conceptual script; you will need to adapt it to your specific vLLM entrypoint.
    # The goal is to trigger the model compilation without starting the full server.
    print("Starting vLLM engine warmup to generate torch.compile cache...")
    try:
        from vllm.engine.arg_utils import EngineArgs
        from vllm.engine.llm_engine import LLMEngine

        # Use the same arguments your final service will use
        args = EngineArgs(model='google/gemma-3-1b-it', enable_torch_compile=True)
        engine = LLMEngine.from_engine_args(args)
        print("Warmup complete. Cache should be generated.")
    except Exception as e:
        print(f"Warmup failed, but this might be okay if the cache was still generated: {e}")

    ```

2.  **Modify your Dockerfile:** Use a multi-stage build. In the first stage, run the warmup script. In the final stage, copy the generated cache from the first stage.

    ```dockerfile
    # Stage 1: Build and Warmup
    FROM your-base-image as builder

    # Copy your application code
    WORKDIR /app
    COPY . .

    # Install dependencies
    RUN pip install -r requirements.txt

    # RUN the warmup script. This will generate the cache in /root/.cache/vllm/
    RUN python warmup.py

    # Stage 2: Final Image
    FROM your-base-image

    WORKDIR /app

    # Copy dependencies from the builder stage
    COPY --from=builder /usr/local/lib/python3.12/dist-packages /usr/local/lib/python3.12/dist-packages

    # IMPORTANT: Copy the generated cache from the builder stage
    COPY --from=builder /root/.cache/vllm /root/.cache/vllm

    # Copy your application code
    COPY . .

    # Your original CMD/ENTRYPOINT
    CMD ["python", "-m", "vllm.entrypoints.openai.api_server", "--model", "google/gemma-3-1b-it"]
    ```

**Caveat:** The `torch.compile` cache can be specific to the exact hardware and driver versions. Since Cloud Run GPU instances are standardized (e.g., NVIDIA L4 with a specific driver), this has a good chance of working, but it is not guaranteed.

#### Recommendation 3: Minor Optimizations

1.  **Set `TORCH_CUDA_ARCH_LIST`:** The logs show a warning that this is not set. Setting it can prevent PyTorch from trying to JIT-compile for all possible architectures, potentially saving a few seconds. For Cloud Run's L4 GPUs, the compute capability is 8.9.

    *   **Action:** Add this line to your Dockerfile:
        ```dockerfile
        ENV TORCH_CUDA_ARCH_LIST="8.9"
        ```

2.  **Use Startup CPU Boost:** This feature allocates more powerful CPU during instance startup, which can speed up CPU-bound tasks like module loading and initial setup before the GPU is fully utilized.

    *   **Action:**
        ```bash
        gcloud run services update vllm-gemma-3-1b-it \
          --region us-central1 \
          --cpu-boost
        ```
--------------------
[2025-10-15 02:47:15] User: please create github issue in repository vllm-container-prewarm containing recommendation 2
[2025-10-15 02:47:15] Bot: Of course. As an expert in Google Cloud Run performance optimization, I can analyze these logs and provide a detailed solution to your query about caching the CUDA graph capture step.

This is an excellent question that gets to the heart of optimizing GPU-based model serving on serverless platforms. Let's break down the logs and formulate a plan.

### Executive Summary (TL;DR)

1.  **Direct Caching:** Unfortunately, you **cannot cache the CUDA graph capture across cold starts** in a standard Cloud Run setup. Cloud Run instances are ephemeral; when an instance scales to zero, its in-memory state and filesystem changes are lost. Similarly, this step cannot be performed during a standard `cloud build` because it requires an active GPU.
2.  **Log Analysis:** The logs show that CUDA graph capturing (`Capturing CUDA graphs...`) takes approximately **3 seconds** (`19:17:43` to `19:17:46`). However, a much larger contributor to your cold start is **`torch.compile`**, which takes **~54 seconds**.
3.  **Primary Recommendation:** The most effective and recommended solution to eliminate this entire startup latency is to configure your Cloud Run service with a **minimum number of instances (`min-instances=1`)**. This keeps one instance constantly warm and ready to serve requests, bypassing the cold start process entirely for incoming traffic.
4.  **Advanced (Experimental) Recommendation:** It is theoretically possible to pre-warm the `torch.compile` cache during the image build process, which could shave off a significant portion of the startup time. This is complex and has caveats.

---

### 1. Detailed Log Analysis

By examining the timestamps in your logs, we can reconstruct the timeline of a cold start for instance `0069c7a...`:

| Timestamp (UTC) | Duration | Log Message / Event | Analysis |
| :--- | :--- | :--- | :--- |
| `19:15:25` | ~2s | `Model loading took ... 1.868946 seconds` | The actual model weight loading from disk to GPU memory is very fast. |
| `19:15:25` - `19:16:38` | **~73s** | `torch.compile` related logs | The application is spending a significant amount of time here. |
| `19:16:20` | (part of above) | `Compiling a graph for dynamic shape takes 43.16 s` | This is a `torch.compile` JIT (Just-In-Time) compilation step. It's a major source of startup latency. |
| `19:16:38` | **54.25s** | `torch.compile takes 54.25 s in total` | **This is the single largest contributor to your cold start time.** |
| `19:17:43` | - | `Available KV cache memory: 16.95 GiB` | The vLLM engine begins its final initialization and memory allocation. |
| `19:17:44` | - | `Capturing CUDA graphs ... 0%` | **The user's point of interest begins here.** |
| `19:17:46` | **~3s** | `Graph capturing finished in 3 secs` | This step is relatively fast compared to `torch.compile`, but still adds to the total time. |
| `19:17:46` | **141.08s** | `init engine ... took 141.08 seconds` | This log summarizes the entire vLLM engine initialization, from model loading through compilation and graph capture. The total cold start is over 2 minutes. |
| `19:17:49` | - | `Application startup complete.` | The container is now ready to accept requests. |
| `19:17:50` | - | First request begins processing. | The first request after this startup will have a high latency, incorporating this entire setup time. |
| `19:29:30` | - | `Application shutdown complete.` | After a period of inactivity (~12 minutes), Cloud Run scaled the instance down to zero. The next request will trigger another full cold start. |

### 2. Answering Your Specific Question

**"Is there a way to execute or cache the "Capturing CUDA graphs" part during the build stage or after the first cold start?"**

The direct answer is **no, for two fundamental reasons**:

1.  **Ephemeral Instances:** Cloud Run instances are stateless and ephemeral. When your service scales down to zero, the instance is terminated. Any caches created in memory (like the captured CUDA graph) or on the local container filesystem (like the `torch.compile` cache at `/root/.cache/`) are permanently deleted. The next request will start a brand new, clean instance from the container image, repeating the entire initialization process.
2.  **Build Environment Limitations:** Standard Cloud Build workers do not have GPUs. CUDA graph capturing is an operation that records a sequence of kernels to be executed on a GPU. It requires an initialized CUDA context, a loaded model on the GPU, and an active GPU device, none of which are available during a typical container build.

### 3. Concrete, Actionable Recommendations

While you can't cache these specific runtime artifacts across cold starts, you can solve the underlying problem: **avoiding the cold start itself.**

#### Recommendation 1: Use `min-instances` (Most Effective Solution)

This is the canonical solution for latency-sensitive applications with long initialization times on Cloud Run. By setting a minimum number of instances, you instruct Cloud Run to always keep at least one container instance running, warm, and ready.

*   **How it works:** The instance will go through the ~141-second cold start process once when it's first provisioned. After that, it stays alive, having already loaded the model, compiled the graphs, and captured the CUDA graphs. All subsequent requests are routed to this warm instance, resulting in very low latency.
*   **Trade-off:** You will be billed for the vCPU, memory, and GPU of the instance for the entire time it is kept running, even when idle. For a production service that needs consistent low-latency performance, this cost is generally acceptable.

**Action:** To set `min-instances` to 1, run the following `gcloud` command:

```bash
gcloud run services update vllm-gemma-3-1b-it \
  --region us-central1 \
  --min-instances=1
```

#### Recommendation 2: Pre-warm `torch.compile` Cache (Advanced)

Since `torch.compile` is the biggest offender (~54 seconds), you can attempt to pre-generate its cache during the container build and bake it into the image. This is complex and may not be fully portable, but it can significantly reduce the startup time *if it works*.

**This will not cache the CUDA graph capture**, but it addresses the larger problem.

**Steps:**

1.  **Create a "warming" script:** Create a Python script (`warmup.py`) that performs the minimum work needed to trigger the compilation.

    ```python
    # warmup.py
    # This is a conceptual script; you will need to adapt it to your specific vLLM entrypoint.
    # The goal is to trigger the model compilation without starting the full server.
    print("Starting vLLM engine warmup to generate torch.compile cache...")
    try:
        from vllm.engine.arg_utils import EngineArgs
        from vllm.engine.llm_engine import LLMEngine

        # Use the same arguments your final service will use
        args = EngineArgs(model='google/gemma-3-1b-it', enable_torch_compile=True)
        engine = LLMEngine.from_engine_args(args)
        print("Warmup complete. Cache should be generated.")
    except Exception as e:
        print(f"Warmup failed, but this might be okay if the cache was still generated: {e}")

    ```

2.  **Modify your Dockerfile:** Use a multi-stage build. In the first stage, run the warmup script. In the final stage, copy the generated cache from the first stage.

    ```dockerfile
    # Stage 1: Build and Warmup
    FROM your-base-image as builder

    # Copy your application code
    WORKDIR /app
    COPY . .

    # Install dependencies
    RUN pip install -r requirements.txt

    # RUN the warmup script. This will generate the cache in /root/.cache/vllm/
    RUN python warmup.py

    # Stage 2: Final Image
    FROM your-base-image

    WORKDIR /app

    # Copy dependencies from the builder stage
    COPY --from=builder /usr/local/lib/python3.12/dist-packages /usr/local/lib/python3.12/dist-packages

    # IMPORTANT: Copy the generated cache from the builder stage
    COPY --from=builder /root/.cache/vllm /root/.cache/vllm

    # Copy your application code
    COPY . .

    # Your original CMD/ENTRYPOINT
    CMD ["python", "-m", "vllm.entrypoints.openai.api_server", "--model", "google/gemma-3-1b-it"]
    ```

**Caveat:** The `torch.compile` cache can be specific to the exact hardware and driver versions. Since Cloud Run GPU instances are standardized (e.g., NVIDIA L4 with a specific driver), this has a good chance of working, but it is not guaranteed.

#### Recommendation 3: Minor Optimizations

1.  **Set `TORCH_CUDA_ARCH_LIST`:** The logs show a warning that this is not set. Setting it can prevent PyTorch from trying to JIT-compile for all possible architectures, potentially saving a few seconds. For Cloud Run's L4 GPUs, the compute capability is 8.9.

    *   **Action:** Add this line to your Dockerfile:
        ```dockerfile
        ENV TORCH_CUDA_ARCH_LIST="8.9"
        ```

2.  **Use Startup CPU Boost:** This feature allocates more powerful CPU during instance startup, which can speed up CPU-bound tasks like module loading and initial setup before the GPU is fully utilized.

    *   **Action:**
        ```bash
        gcloud run services update vllm-gemma-3-1b-it \
          --region us-central1 \
          --cpu-boost
        ```
--------------------
[2025-10-15 02:51:23] User: for cloud run service vllm-gemma-3-1b-it,is there a way to execute or cache the "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): " part during cloud build stage or during the first cold start, so that subsequent cold starts can be shorter?
[2025-10-15 02:51:23] Bot: Logs for service 'vllm-gemma-3-1b-it' were found. User query: 'for cloud run service vllm-gemma-3-1b-it,is there a way to execute or cache the "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): " part during cloud build stage or during the first cold start, so that subsequent cold starts can be shorter?'. (LLM call temporarily disabled. Logs: Te)
--------------------
[2025-10-15 02:58:53] User: for cloud run service vllm-gemma-3-1b-it,is there a way to execute or cache the "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): " part during cloud build stage or during the first cold start, so that subsequent cold starts can be shorter?
[2025-10-15 02:58:53] Bot: Logs for service 'vllm-gemma-3-1b-it' were found. User query: 'for cloud run service vllm-gemma-3-1b-it,is there a way to execute or cache the "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): " part during cloud build stage or during the first cold start, so that subsequent cold starts can be shorter?'. (LLM call temporarily disabled. Logs: Te)
--------------------
[2025-10-15 03:00:55] User: for cloud run service vllm-gemma-3-1b-it,is there a way to execute or cache the "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): " part during cloud build stage or during the first cold start, so that subsequent cold starts can be shorter?
[2025-10-15 03:00:55] Bot: Logs for service 'vllm-gemma-3-1b-it' were found. User query: 'for cloud run service vllm-gemma-3-1b-it,is there a way to execute or cache the "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): " part during cloud build stage or during the first cold start, so that subsequent cold starts can be shorter?'. (LLM call temporarily disabled. Logs: Te)
--------------------
[2025-10-15 03:04:18] User: for cloud run service vllm-gemma-3-1b-it,is there a way to execute or cache the "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): " part during cloud build stage or during the first cold start, so that subsequent cold starts can be shorter?
[2025-10-15 03:04:18] Bot: Of course. As a Google Cloud Run expert, I can analyze these logs and provide a detailed breakdown and actionable recommendations for your `vllm-gemma-3-1b-it` service.

Excellent question. You've correctly identified a key part of the cold start process for GPU-based services using vLLM. Let's dive into the logs and formulate a strategy.

### Executive Summary

The short answer is that **caching the CUDA graph is challenging but possible in theory, yet there's a much larger performance gain to be had by caching the `torch.compile` results**, which is a more significant bottleneck identified in your logs.

*   **CUDA Graph Capturing:** This process is highly dependent on the specific GPU hardware and driver version available at runtime. Caching this during the `Cloud Build` stage is not feasible because the build environment lacks the target GPU. Caching it across cold starts would require custom code and a persistent file system.
*   **The Real Bottleneck (`torch.compile`):** Your logs show that `torch.compile` takes **~54 seconds**, while CUDA graph capturing only takes **~3 seconds**.
*   **Top Recommendation:** The most effective solution is to **mount a shared, persistent file system (like Google Cloud Filestore) to persist the `torch.compile` cache directory**. This will dramatically reduce the initialization time for all subsequent cold starts after the very first one.
*   **Secondary Recommendation:** Use Cloud Run's `min-instances` feature to keep at least one instance warm, effectively eliminating cold starts for users as long as traffic is consistent.

---

### 1. Detailed Log Analysis

I've analyzed the provided logs to understand the full cold start sequence. The timestamps are in reverse chronological order, so let's walk through a new instance startup from the bottom up.

**Key Log Entries Identified (in chronological order of events):**

1.  `INFO 10-14 12:15:25 [gpu_model_runner.py:2007] Model loading took 1.9147 GiB and 1.868946 seconds`
    *   **Analysis:** This is the initial step of loading the model weights from the container image into GPU memory. This is relatively fast.

2.  `INFO 10-14 12:15:36 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/...`
    *   **Analysis:** **This is a critical log.** It shows that vLLM is using `torch.compile` for performance and that it has a designated cache directory. On a standard Cloud Run instance, this directory is ephemeral and is lost every time the instance shuts down.

3.  `INFO 10-14 12:16:38 [monitor.py:34] torch.compile takes 54.25 s in total`
    *   **Analysis:** **This is the single largest contributor to your cold start time.** The PyTorch Inductor backend is compiling the model into optimized Triton kernels. This is a one-time cost *per instance startup*.

4.  `stderr Capturing CUDA graphs...[00:02<00:00, 26.86it/s]` and `INFO 10-14 12:17:46 [gpu_model_runner.py:2708] Graph capturing finished in 3 secs...`
    *   **Analysis:** This is the step you asked about. It captures the sequence of GPU operations into a graph to reduce CPU overhead during inference. As the logs show, this process took approximately **3 seconds**. While part of the cold start, it's minor compared to the compilation step.

5.  `INFO 10-14 12:17:46 [core.py:214] init engine (profile, create kv cache, warmup model) took 141.08 seconds`
    *   **Analysis:** This log summarizes the entire vLLM engine initialization. The total cold start time from the engine's perspective is over **2 minutes and 21 seconds**, which is significant. This includes the model loading, `torch.compile`, CUDA graph capture, and other setup steps.

6.  `INFO ... Application startup complete.` and `STARTUP TCP probe succeeded...`
    *   **Analysis:** At this point (`19:17:49`), the container is fully initialized and ready to accept requests.

### 2. Answering Your Specific Question: Caching CUDA Graphs

You asked if the CUDA graph capture can be cached during Cloud Build or after the first cold start.

*   **During Cloud Build:** This is not feasible. CUDA graphs are specific to the GPU architecture (e.g., NVIDIA L4, T4, A100) and the installed NVIDIA driver version. The Cloud Build environment does not have a GPU, so it cannot generate a compatible graph. Attempting to generate it on a different machine and copying it would likely lead to runtime errors.

*   **Across Cold Starts:** This is theoretically possible but complex. You would need to:
    1.  Modify the vLLM source code to serialize the captured graph to a file after it's generated.
    2.  Save this file to a persistent, shared location (like a Filestore instance or a GCS bucket mounted with Cloud Storage FUSE).
    3.  On subsequent startups, add logic to check for this cached graph and load it instead of re-capturing.

Given that this step only takes **3 seconds** in your case, the engineering effort to implement this custom caching likely outweighs the benefit. The real target for optimization is the 54-second `torch.compile` step.

---

### 3. Actionable Solutions for Performance Optimization

Here are concrete steps to drastically reduce your cold start time.

#### Solution 1 (Highly Recommended): Persist the `torch.compile` Cache with NFS

The logs clearly indicate a cache directory is used. By mounting a persistent file system to that location, you ensure the 54-second compilation only happens *once* for your entire service deployment, not for every new instance.

**Steps:**

1.  **Set up a Google Cloud Filestore (NFS) instance.** A basic tier instance is sufficient for this purpose. Note its IP address and file share name.

2.  **Mount the Filestore instance to your Cloud Run service.** When deploying your service, add the `--vpc-connector` and `--mount-nfs` flags. You need to ensure your service is connected to the same VPC as your Filestore instance.

    Your deployment command would look something like this:

    ```bash
    gcloud run deploy vllm-gemma-3-1b-it \
      --image gcr.io/your-project/your-vllm-image:latest \
      --vpc-connector projects/your-project/locations/us-central1/connectors/your-connector \
      --mount-nfs projects/your-project/locations/us-central1/instances/your-filestore-instance:/root/.cache \
      --args="--name=your-share-name" \
      --memory=... \
      --cpu=... \
      --gpu=... \
      --region=us-central1 \
      --project=globalbiting-dev
      # Add other necessary flags
    ```
    *   **`--mount-nfs`**: The key part is mounting the NFS share (`your-filestore-instance`) to the parent directory of the cache (`/root/.cache`). This will automatically persist the `vllm/torch_compile_cache` subdirectory.

**Expected Outcome:**
*   **First Instance:** The cold start will still be ~2.5 minutes as it populates the cache on the NFS share.
*   **All Subsequent Instances:** New instances will find the pre-compiled kernels on the NFS share, skipping the 54-second compilation step. This should reduce your total engine initialization time from **~141 seconds to ~87 seconds**, a reduction of nearly 40%.

#### Solution 2: Eliminate Cold Starts with `min-instances`

If your service needs to be responsive at all times and the cost is acceptable, you can configure Cloud Run to keep a minimum number of instances running and ready.

**Steps:**

1.  **Update your service configuration:**

    ```bash
    gcloud run services update vllm-gemma-3-1b-it \
      --min-instances=1 \
      --region=us-central1 \
      --project=globalbiting-dev
    ```

**Expected Outcome:**
*   One instance will always be kept "warm" after its initial startup.
*   Incoming requests will be routed to this warm instance, experiencing no cold start latency.
*   If traffic increases and Cloud Run scales up, the *new* instances will still experience a cold start. This is why **combining Solution 1 and Solution 2 is the most robust strategy.**

#### Solution 3 (Advanced): Use Startup CPU Boost

This feature provides additional CPU performance during the instance startup phase, which can accelerate CPU-bound initialization tasks. While your main bottleneck is GPU-bound compilation, other parts of the startup might benefit.

**Steps:**

1.  **Enable CPU boost during deployment:**

    ```bash
    gcloud run deploy vllm-gemma-3-1b-it \
      --image ... \
      --startup-cpu-boost \
      # ... other flags
    ```

**Expected Outcome:**
*   A potential modest reduction in the non-GPU parts of the 141-second initialization. The impact will be less than Solution 1 but it's simple to enable and test.
--------------------
[2025-10-15 03:07:08] User: please create issue in github repository vllm-container-prewarm containing details for solution 1, as a feature enhancement
[2025-10-15 03:07:08] Bot: Of course. As a Google Cloud Run expert, I can analyze these logs and provide a detailed breakdown and actionable recommendations for your `vllm-gemma-3-1b-it` service.

Excellent question. You've correctly identified a key part of the cold start process for GPU-based services using vLLM. Let's dive into the logs and formulate a strategy.

### Executive Summary

The short answer is that **caching the CUDA graph is challenging but possible in theory, yet there's a much larger performance gain to be had by caching the `torch.compile` results**, which is a more significant bottleneck identified in your logs.

*   **CUDA Graph Capturing:** This process is highly dependent on the specific GPU hardware and driver version available at runtime. Caching this during the `Cloud Build` stage is not feasible because the build environment lacks the target GPU. Caching it across cold starts would require custom code and a persistent file system.
*   **The Real Bottleneck (`torch.compile`):** Your logs show that `torch.compile` takes **~54 seconds**, while CUDA graph capturing only takes **~3 seconds**.
*   **Top Recommendation:** The most effective solution is to **mount a shared, persistent file system (like Google Cloud Filestore) to persist the `torch.compile` cache directory**. This will dramatically reduce the initialization time for all subsequent cold starts after the very first one.
*   **Secondary Recommendation:** Use Cloud Run's `min-instances` feature to keep at least one instance warm, effectively eliminating cold starts for users as long as traffic is consistent.

---

### 1. Detailed Log Analysis

I've analyzed the provided logs to understand the full cold start sequence. The timestamps are in reverse chronological order, so let's walk through a new instance startup from the bottom up.

**Key Log Entries Identified (in chronological order of events):**

1.  `INFO 10-14 12:15:25 [gpu_model_runner.py:2007] Model loading took 1.9147 GiB and 1.868946 seconds`
    *   **Analysis:** This is the initial step of loading the model weights from the container image into GPU memory. This is relatively fast.

2.  `INFO 10-14 12:15:36 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/...`
    *   **Analysis:** **This is a critical log.** It shows that vLLM is using `torch.compile` for performance and that it has a designated cache directory. On a standard Cloud Run instance, this directory is ephemeral and is lost every time the instance shuts down.

3.  `INFO 10-14 12:16:38 [monitor.py:34] torch.compile takes 54.25 s in total`
    *   **Analysis:** **This is the single largest contributor to your cold start time.** The PyTorch Inductor backend is compiling the model into optimized Triton kernels. This is a one-time cost *per instance startup*.

4.  `stderr Capturing CUDA graphs...[00:02<00:00, 26.86it/s]` and `INFO 10-14 12:17:46 [gpu_model_runner.py:2708] Graph capturing finished in 3 secs...`
    *   **Analysis:** This is the step you asked about. It captures the sequence of GPU operations into a graph to reduce CPU overhead during inference. As the logs show, this process took approximately **3 seconds**. While part of the cold start, it's minor compared to the compilation step.

5.  `INFO 10-14 12:17:46 [core.py:214] init engine (profile, create kv cache, warmup model) took 141.08 seconds`
    *   **Analysis:** This log summarizes the entire vLLM engine initialization. The total cold start time from the engine's perspective is over **2 minutes and 21 seconds**, which is significant. This includes the model loading, `torch.compile`, CUDA graph capture, and other setup steps.

6.  `INFO ... Application startup complete.` and `STARTUP TCP probe succeeded...`
    *   **Analysis:** At this point (`19:17:49`), the container is fully initialized and ready to accept requests.

### 2. Answering Your Specific Question: Caching CUDA Graphs

You asked if the CUDA graph capture can be cached during Cloud Build or after the first cold start.

*   **During Cloud Build:** This is not feasible. CUDA graphs are specific to the GPU architecture (e.g., NVIDIA L4, T4, A100) and the installed NVIDIA driver version. The Cloud Build environment does not have a GPU, so it cannot generate a compatible graph. Attempting to generate it on a different machine and copying it would likely lead to runtime errors.

*   **Across Cold Starts:** This is theoretically possible but complex. You would need to:
    1.  Modify the vLLM source code to serialize the captured graph to a file after it's generated.
    2.  Save this file to a persistent, shared location (like a Filestore instance or a GCS bucket mounted with Cloud Storage FUSE).
    3.  On subsequent startups, add logic to check for this cached graph and load it instead of re-capturing.

Given that this step only takes **3 seconds** in your case, the engineering effort to implement this custom caching likely outweighs the benefit. The real target for optimization is the 54-second `torch.compile` step.

---

### 3. Actionable Solutions for Performance Optimization

Here are concrete steps to drastically reduce your cold start time.

#### Solution 1 (Highly Recommended): Persist the `torch.compile` Cache with NFS

The logs clearly indicate a cache directory is used. By mounting a persistent file system to that location, you ensure the 54-second compilation only happens *once* for your entire service deployment, not for every new instance.

**Steps:**

1.  **Set up a Google Cloud Filestore (NFS) instance.** A basic tier instance is sufficient for this purpose. Note its IP address and file share name.

2.  **Mount the Filestore instance to your Cloud Run service.** When deploying your service, add the `--vpc-connector` and `--mount-nfs` flags. You need to ensure your service is connected to the same VPC as your Filestore instance.

    Your deployment command would look something like this:

    ```bash
    gcloud run deploy vllm-gemma-3-1b-it \
      --image gcr.io/your-project/your-vllm-image:latest \
      --vpc-connector projects/your-project/locations/us-central1/connectors/your-connector \
      --mount-nfs projects/your-project/locations/us-central1/instances/your-filestore-instance:/root/.cache \
      --args="--name=your-share-name" \
      --memory=... \
      --cpu=... \
      --gpu=... \
      --region=us-central1 \
      --project=globalbiting-dev
      # Add other necessary flags
    ```
    *   **`--mount-nfs`**: The key part is mounting the NFS share (`your-filestore-instance`) to the parent directory of the cache (`/root/.cache`). This will automatically persist the `vllm/torch_compile_cache` subdirectory.

**Expected Outcome:**
*   **First Instance:** The cold start will still be ~2.5 minutes as it populates the cache on the NFS share.
*   **All Subsequent Instances:** New instances will find the pre-compiled kernels on the NFS share, skipping the 54-second compilation step. This should reduce your total engine initialization time from **~141 seconds to ~87 seconds**, a reduction of nearly 40%.

#### Solution 2: Eliminate Cold Starts with `min-instances`

If your service needs to be responsive at all times and the cost is acceptable, you can configure Cloud Run to keep a minimum number of instances running and ready.

**Steps:**

1.  **Update your service configuration:**

    ```bash
    gcloud run services update vllm-gemma-3-1b-it \
      --min-instances=1 \
      --region=us-central1 \
      --project=globalbiting-dev
    ```

**Expected Outcome:**
*   One instance will always be kept "warm" after its initial startup.
*   Incoming requests will be routed to this warm instance, experiencing no cold start latency.
*   If traffic increases and Cloud Run scales up, the *new* instances will still experience a cold start. This is why **combining Solution 1 and Solution 2 is the most robust strategy.**

#### Solution 3 (Advanced): Use Startup CPU Boost

This feature provides additional CPU performance during the instance startup phase, which can accelerate CPU-bound initialization tasks. While your main bottleneck is GPU-bound compilation, other parts of the startup might benefit.

**Steps:**

1.  **Enable CPU boost during deployment:**

    ```bash
    gcloud run deploy vllm-gemma-3-1b-it \
      --image ... \
      --startup-cpu-boost \
      # ... other flags
    ```

**Expected Outcome:**
*   A potential modest reduction in the non-GPU parts of the 141-second initialization. The impact will be less than Solution 1 but it's simple to enable and test.
--------------------
[2025-10-15 03:17:06] User: for cloud run service vllm-gemma-3-1b-it,is there a way to execute or cache the "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): " part during cloud build stage or during the first cold start, so that subsequent cold starts can be shorter?
[2025-10-15 03:17:06] Bot: Of course. As an expert in Google Cloud Run performance optimization, I can analyze your logs and provide a detailed solution for your `vllm-gemma-3-1b-it` service.

Here is a comprehensive analysis and a set of actionable recommendations to address your concerns about cold start times and CUDA graph capturing.

### 1. Log Analysis & Problem Identification

Your core question is about caching the "Capturing CUDA graphs" step. By analyzing the provided logs, we can build a precise timeline of a cold start for your service. The key insight is that **the CUDA graph capture is not the primary bottleneck**.

Here is a breakdown of the startup process based on the log timestamps:

| Timestamp (UTC) | Duration | Event | Log Message |
| :--- | :--- | :--- | :--- |
| `19:15:25` | ~2 sec | **Model Loading** | `INFO ... Model loading took 1.9147 GiB and 1.868946 seconds` |
| `19:15:25` -> `19:16:38` | **~73 sec** | **PyTorch Compilation** | `INFO ... Using cache directory: /root/.cache/vllm/torch_compile_cache/...` followed by `INFO ... torch.compile takes 54.25 s in total` (part of the larger 73s block) |
| `19:17:44` -> `19:17:46` | **~3 sec** | **CUDA Graph Capture** | `Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100% ...` followed by `INFO ... Graph capturing finished in 3 secs...` |
| `19:17:46` | (~141 sec total) | **Engine Initialization** | `INFO ... init engine (profile, create kv cache, warmup model) took 141.08 seconds` (This is the total time for the entire engine startup, including the steps above) |
| `19:17:49` | | **Application Ready** | `INFO: Application startup complete.` |

**Key Findings:**

*   **CUDA Graph Capture:** This step, which you asked about, is very fast, taking only **~3 seconds**.
*   **The Real Bottleneck:** The `torch.compile` step is the most time-consuming part of the initialization, taking **over a minute**. The logs clearly show it attempting to use a cache directory (`/root/.cache/vllm/torch_compile_cache`), but this directory is empty on every new container instance (a cold start).

Therefore, while your question about caching CUDA graphs is valid, optimizing the `torch.compile` step will yield a much greater reduction in cold start time.

### 2. Addressing Your Core Question: Caching Limitations

Before we dive into solutions, it's important to understand *why* your initial ideas face technical hurdles in a serverless environment like Cloud Run.

*   **Caching during Cloud Build:** Cloud Build executes build steps in a temporary environment that **does not have access to GPUs**. CUDA graph capturing and `torch.compile` for GPU kernels require a live, initialized CUDA context on a GPU. This makes it impossible to perform these steps during the image build process directly.
*   **Caching Across Cold Starts:** Each cold start in Cloud Run provisions a brand new, ephemeral container instance from your container image. The filesystem and memory state of a previous instance are completely discarded when it scales down. This means any cache generated at runtime (like the `torch.compile` cache or the captured CUDA graph) is lost and must be regenerated on the next cold start.

The solution is not to cache across instances, but to **pre-generate the cache and include it in the container image**, or to **avoid cold starts altogether**.

### 3. Actionable Solutions & Recommendations

Here are prioritized strategies to significantly reduce your service's cold start time.

---

#### Strategy 1: Mitigate Cold Starts Entirely (Highest & Immediate Impact)

If your primary goal is to ensure low latency for all requests and you can accommodate a predictable cost, the most effective solution is to prevent the container from scaling to zero.

**Action:** Configure your Cloud Run service to have a minimum number of instances.

*   **How:** Set the `--min-instances=1` flag when deploying your service.
*   **Why it Works:** This keeps one container instance constantly running and "warm." This instance will go through the lengthy initialization process (model loading, `torch.compile`, CUDA graph capture) only once when it's first provisioned. All subsequent requests will be directed to this warm instance, bypassing the cold start latency entirely.
*   **Trade-off:** You will be billed for the resources (vCPU, memory, GPU) for this instance for as long as it's running, even when it's idle.

**Command:**
```bash
gcloud run deploy vllm-gemma-3-1b-it \
  --image gcr.io/your-project/your-image:latest \
  --min-instances=1 \
  --project=globalbiting-dev \
  --region=us-central1
  # ... other flags like --gpu, --memory, etc.
```

---

#### Strategy 2: Optimize the Bottleneck by Baking the Cache into the Image (Best Practice)

This is the most advanced and robust solution. We can use a multi-stage Docker build to run the slow `torch.compile` step once, capture the resulting cache files, and copy them into the final, lean production image.

**Action:** Modify your `Dockerfile` to use a multi-stage build to prime the `torch.compile` cache.

*   **How it Works:**
    1.  **"Builder" Stage:** We create a temporary build stage where we install all dependencies and then run a simple Python script that initializes the vLLM engine. This forces `torch.compile` to run and populate the `/root/.cache` directory.
    2.  **"Final" Stage:** We create the final production image. Instead of running the compilation again, we simply `COPY` the populated `/root/.cache` directory from the "builder" stage.
*   **Result:** When a new Cloud Run instance starts from this image, the vLLM engine finds the pre-compiled kernels in the cache and can skip the ~73-second compilation step, reducing the startup time dramatically.

Here is an example `Dockerfile` and the required priming script:

**1. Create a priming script (`prime_cache.py`):**
This script needs to initialize the engine with the exact same parameters you use in production.

```python
# prime_cache.py
import os
from vllm.engine.arg_utils import AsyncEngineArgs
from vllm.engine.async_llm_engine import AsyncLLMEngine

# These arguments MUST match your production startup command/arguments
# It's crucial that the model, dtype, and other compilation-affecting
# parameters are identical.
model_name = "google/gemma-1.1-2b-it" # Or whichever model you use
tensor_parallel_size = 1 # Match your production setting

print("Starting cache priming process...")

engine_args = AsyncEngineArgs(
    model=model_name,
    tensor_parallel_size=tensor_parallel_size,
    # Add any other relevant args like dtype, max_model_len, etc.
    # For example: dtype="bfloat16"
)

# Initializing the engine will trigger the torch.compile step
engine = AsyncLLMEngine.from_engine_args(engine_args, start_engine_loop=False)

print("Cache priming complete. The cache should be populated in /root/.cache")
```

**2. Modify your `Dockerfile`:**

```dockerfile
# Stage 1: Builder - This stage generates the cache
FROM nvidia/cuda:12.1.1-devel-ubuntu22.04 AS builder

# Install Python, dependencies, etc.
RUN apt-get update && apt-get install -y python3.12 python3-pip && rm -rf /var/lib/apt/lists/*
RUN pip3 install --no-cache-dir torch
RUN pip3 install --no-cache-dir vllm

# Copy the priming script
COPY prime_cache.py /app/prime_cache.py

# Copy your model weights if they are part of the build context
# COPY --from=... your_model_weights /root/.cache/huggingface/hub/...

# Set the environment variable to ensure torch.compile is used
# (This may be on by default in your vLLM version, but it's good to be explicit)
ENV VLLM_TORCH_COMPILE=1

# Run the priming script to generate the cache
# This is the slow step that will now happen during Cloud Build
WORKDIR /app
RUN python3 prime_cache.py

# ---
# Stage 2: Final Image - This stage builds the lean production image
FROM nvidia/cuda:12.1.1-devel-ubuntu22.04

# Install Python, dependencies, etc. (same as builder)
RUN apt-get update && apt-get install -y python3.12 python3-pip && rm -rf /var/lib/apt/lists/*
RUN pip3 install --no-cache-dir torch
RUN pip3 install --no-cache-dir vllm

WORKDIR /app

# The MAGIC step: Copy the populated cache from the builder stage
COPY --from=builder /root/.cache /root/.cache

# Copy the rest of your application code
COPY . .

# Your final command to run the vLLM server
CMD ["python3", "-m", "vllm.entrypoints.openai.api_server", "--model", "google/gemma-1.1-2b-it", "--host", "0.0.0.0", "--port", "8000"]
```

---

#### Strategy 3: Accelerate the Cold Start with a CPU Boost (Complementary)

This feature provides more CPU power during the container startup phase, which can accelerate CPU-bound initialization tasks.

**Action:** Enable Startup CPU Boost.

*   **How:** Set the `--cpu-boost` flag on deployment.
*   **Why it Works:** It temporarily allocates more vCPU to your instance during startup, speeding up tasks like decompressing layers, initializing libraries, and running startup scripts. This can help shave off additional seconds from the already-optimized startup process.

**Command:**
```bash
gcloud run deploy vllm-gemma-3-1b-it \
  --image gcr.io/your-project/your-image:latest \
  --cpu-boost \
  # ... other flags
```

### Summary of Recommendations

1.  **For Immediate Latency Guarantees:** Use `--min-instances=1`. This is the simplest and most effective way to eliminate cold starts for your users, at the cost of having an always-on instance.
2.  **For Long-Term Performance & Cost-Efficiency:** Implement the **multi-stage Docker build (Strategy 2)**. This directly tackles the main 73-second bottleneck and will drastically reduce cold start times for every new instance, making your service more scalable and efficient if you need to scale to zero.
3.  **For an Extra Speed Bump:** Combine Strategy 1 or 2 with **Startup CPU Boost (Strategy 3)** to further shorten the initial provisioning time.

By focusing on the `torch.compile` step identified in the logs, you can achieve a much more significant performance improvement than by focusing on the 3-second CUDA graph capture alone.
--------------------
[2025-10-15 03:18:20] User: please create github issue in repository vllm-container-prewarm containing details for solution 1, as a feature enhancement
[2025-10-15 03:18:20] Bot: Of course. As an expert in Google Cloud Run performance optimization, I can analyze your logs and provide a detailed solution for your `vllm-gemma-3-1b-it` service.

Here is a comprehensive analysis and a set of actionable recommendations to address your concerns about cold start times and CUDA graph capturing.

### 1. Log Analysis & Problem Identification

Your core question is about caching the "Capturing CUDA graphs" step. By analyzing the provided logs, we can build a precise timeline of a cold start for your service. The key insight is that **the CUDA graph capture is not the primary bottleneck**.

Here is a breakdown of the startup process based on the log timestamps:

| Timestamp (UTC) | Duration | Event | Log Message |
| :--- | :--- | :--- | :--- |
| `19:15:25` | ~2 sec | **Model Loading** | `INFO ... Model loading took 1.9147 GiB and 1.868946 seconds` |
| `19:15:25` -> `19:16:38` | **~73 sec** | **PyTorch Compilation** | `INFO ... Using cache directory: /root/.cache/vllm/torch_compile_cache/...` followed by `INFO ... torch.compile takes 54.25 s in total` (part of the larger 73s block) |
| `19:17:44` -> `19:17:46` | **~3 sec** | **CUDA Graph Capture** | `Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100% ...` followed by `INFO ... Graph capturing finished in 3 secs...` |
| `19:17:46` | (~141 sec total) | **Engine Initialization** | `INFO ... init engine (profile, create kv cache, warmup model) took 141.08 seconds` (This is the total time for the entire engine startup, including the steps above) |
| `19:17:49` | | **Application Ready** | `INFO: Application startup complete.` |

**Key Findings:**

*   **CUDA Graph Capture:** This step, which you asked about, is very fast, taking only **~3 seconds**.
*   **The Real Bottleneck:** The `torch.compile` step is the most time-consuming part of the initialization, taking **over a minute**. The logs clearly show it attempting to use a cache directory (`/root/.cache/vllm/torch_compile_cache`), but this directory is empty on every new container instance (a cold start).

Therefore, while your question about caching CUDA graphs is valid, optimizing the `torch.compile` step will yield a much greater reduction in cold start time.

### 2. Addressing Your Core Question: Caching Limitations

Before we dive into solutions, it's important to understand *why* your initial ideas face technical hurdles in a serverless environment like Cloud Run.

*   **Caching during Cloud Build:** Cloud Build executes build steps in a temporary environment that **does not have access to GPUs**. CUDA graph capturing and `torch.compile` for GPU kernels require a live, initialized CUDA context on a GPU. This makes it impossible to perform these steps during the image build process directly.
*   **Caching Across Cold Starts:** Each cold start in Cloud Run provisions a brand new, ephemeral container instance from your container image. The filesystem and memory state of a previous instance are completely discarded when it scales down. This means any cache generated at runtime (like the `torch.compile` cache or the captured CUDA graph) is lost and must be regenerated on the next cold start.

The solution is not to cache across instances, but to **pre-generate the cache and include it in the container image**, or to **avoid cold starts altogether**.

### 3. Actionable Solutions & Recommendations

Here are prioritized strategies to significantly reduce your service's cold start time.

---

#### Strategy 1: Mitigate Cold Starts Entirely (Highest & Immediate Impact)

If your primary goal is to ensure low latency for all requests and you can accommodate a predictable cost, the most effective solution is to prevent the container from scaling to zero.

**Action:** Configure your Cloud Run service to have a minimum number of instances.

*   **How:** Set the `--min-instances=1` flag when deploying your service.
*   **Why it Works:** This keeps one container instance constantly running and "warm." This instance will go through the lengthy initialization process (model loading, `torch.compile`, CUDA graph capture) only once when it's first provisioned. All subsequent requests will be directed to this warm instance, bypassing the cold start latency entirely.
*   **Trade-off:** You will be billed for the resources (vCPU, memory, GPU) for this instance for as long as it's running, even when it's idle.

**Command:**
```bash
gcloud run deploy vllm-gemma-3-1b-it \
  --image gcr.io/your-project/your-image:latest \
  --min-instances=1 \
  --project=globalbiting-dev \
  --region=us-central1
  # ... other flags like --gpu, --memory, etc.
```

---

#### Strategy 2: Optimize the Bottleneck by Baking the Cache into the Image (Best Practice)

This is the most advanced and robust solution. We can use a multi-stage Docker build to run the slow `torch.compile` step once, capture the resulting cache files, and copy them into the final, lean production image.

**Action:** Modify your `Dockerfile` to use a multi-stage build to prime the `torch.compile` cache.

*   **How it Works:**
    1.  **"Builder" Stage:** We create a temporary build stage where we install all dependencies and then run a simple Python script that initializes the vLLM engine. This forces `torch.compile` to run and populate the `/root/.cache` directory.
    2.  **"Final" Stage:** We create the final production image. Instead of running the compilation again, we simply `COPY` the populated `/root/.cache` directory from the "builder" stage.
*   **Result:** When a new Cloud Run instance starts from this image, the vLLM engine finds the pre-compiled kernels in the cache and can skip the ~73-second compilation step, reducing the startup time dramatically.

Here is an example `Dockerfile` and the required priming script:

**1. Create a priming script (`prime_cache.py`):**
This script needs to initialize the engine with the exact same parameters you use in production.

```python
# prime_cache.py
import os
from vllm.engine.arg_utils import AsyncEngineArgs
from vllm.engine.async_llm_engine import AsyncLLMEngine

# These arguments MUST match your production startup command/arguments
# It's crucial that the model, dtype, and other compilation-affecting
# parameters are identical.
model_name = "google/gemma-1.1-2b-it" # Or whichever model you use
tensor_parallel_size = 1 # Match your production setting

print("Starting cache priming process...")

engine_args = AsyncEngineArgs(
    model=model_name,
    tensor_parallel_size=tensor_parallel_size,
    # Add any other relevant args like dtype, max_model_len, etc.
    # For example: dtype="bfloat16"
)

# Initializing the engine will trigger the torch.compile step
engine = AsyncLLMEngine.from_engine_args(engine_args, start_engine_loop=False)

print("Cache priming complete. The cache should be populated in /root/.cache")
```

**2. Modify your `Dockerfile`:**

```dockerfile
# Stage 1: Builder - This stage generates the cache
FROM nvidia/cuda:12.1.1-devel-ubuntu22.04 AS builder

# Install Python, dependencies, etc.
RUN apt-get update && apt-get install -y python3.12 python3-pip && rm -rf /var/lib/apt/lists/*
RUN pip3 install --no-cache-dir torch
RUN pip3 install --no-cache-dir vllm

# Copy the priming script
COPY prime_cache.py /app/prime_cache.py

# Copy your model weights if they are part of the build context
# COPY --from=... your_model_weights /root/.cache/huggingface/hub/...

# Set the environment variable to ensure torch.compile is used
# (This may be on by default in your vLLM version, but it's good to be explicit)
ENV VLLM_TORCH_COMPILE=1

# Run the priming script to generate the cache
# This is the slow step that will now happen during Cloud Build
WORKDIR /app
RUN python3 prime_cache.py

# ---
# Stage 2: Final Image - This stage builds the lean production image
FROM nvidia/cuda:12.1.1-devel-ubuntu22.04

# Install Python, dependencies, etc. (same as builder)
RUN apt-get update && apt-get install -y python3.12 python3-pip && rm -rf /var/lib/apt/lists/*
RUN pip3 install --no-cache-dir torch
RUN pip3 install --no-cache-dir vllm

WORKDIR /app

# The MAGIC step: Copy the populated cache from the builder stage
COPY --from=builder /root/.cache /root/.cache

# Copy the rest of your application code
COPY . .

# Your final command to run the vLLM server
CMD ["python3", "-m", "vllm.entrypoints.openai.api_server", "--model", "google/gemma-1.1-2b-it", "--host", "0.0.0.0", "--port", "8000"]
```

---

#### Strategy 3: Accelerate the Cold Start with a CPU Boost (Complementary)

This feature provides more CPU power during the container startup phase, which can accelerate CPU-bound initialization tasks.

**Action:** Enable Startup CPU Boost.

*   **How:** Set the `--cpu-boost` flag on deployment.
*   **Why it Works:** It temporarily allocates more vCPU to your instance during startup, speeding up tasks like decompressing layers, initializing libraries, and running startup scripts. This can help shave off additional seconds from the already-optimized startup process.

**Command:**
```bash
gcloud run deploy vllm-gemma-3-1b-it \
  --image gcr.io/your-project/your-image:latest \
  --cpu-boost \
  # ... other flags
```

### Summary of Recommendations

1.  **For Immediate Latency Guarantees:** Use `--min-instances=1`. This is the simplest and most effective way to eliminate cold starts for your users, at the cost of having an always-on instance.
2.  **For Long-Term Performance & Cost-Efficiency:** Implement the **multi-stage Docker build (Strategy 2)**. This directly tackles the main 73-second bottleneck and will drastically reduce cold start times for every new instance, making your service more scalable and efficient if you need to scale to zero.
3.  **For an Extra Speed Bump:** Combine Strategy 1 or 2 with **Startup CPU Boost (Strategy 3)** to further shorten the initial provisioning time.

By focusing on the `torch.compile` step identified in the logs, you can achieve a much more significant performance improvement than by focusing on the 3-second CUDA graph capture alone.
--------------------
[2025-10-15 03:25:29] User: for cloud run service vllm-gemma-3-1b-it,is there a way to execute or cache the "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): " part during cloud build stage or during the first cold start, so that subsequent cold starts can be shorter?
[2025-10-15 03:25:29] Bot: Of course. As an expert in Google Cloud Run performance analysis, I can provide a detailed breakdown based on your query and the provided logs.

### Executive Summary

Your analysis is sharp; the "Capturing CUDA graphs" step is indeed part of the cold start initialization. However, based on the logs, it's not the most significant contributor to the total startup time.

1.  **Direct Answer:** No, you **cannot** execute the CUDA graph capturing during the Cloud Build stage. This process requires an active GPU, which is not available in standard Cloud Build environments. Caching the graph from a first cold start for subsequent ones is also not possible due to Cloud Run's stateless natureâ€”each new instance starts from a clean container image.
2.  **Key Log Finding:** The total engine initialization took **141.08 seconds** (over 2.3 minutes). The CUDA graph capture only accounted for **~3 seconds** of this. The largest portion of the startup time is consumed by `torch.compile`, which took **54.25 seconds**.
3.  **Primary Recommendation:** To eliminate cold starts entirely, set the service's minimum instances to 1 (`--min-instances=1`). This keeps one container instance constantly warm and ready to serve requests, bypassing the lengthy initialization process for the first user.
4.  **Secondary Recommendation:** To speed up the initialization when new instances *do* start (e.g., during scaling), you can optimize the `torch.compile` step by pre-warming its cache during your container build process.

Below is a detailed analysis and a set of actionable steps.

---

### 1. Detailed Log Analysis

I've analyzed the startup sequence for instance `0069c7a...` to build a clear timeline of the cold start process.

**Cold Start Timeline:**

*   `19:15:25`: Model loading begins. It completes quickly, taking **~2 seconds** and using 1.91 GiB of memory.
*   `19:15:25` - `19:16:38`: The `torch.compile` process runs. This is the most time-consuming part of the startup:
    *   `19:15:36`: The log mentions using a cache directory: `/root/.cache/vllm/torch_compile_cache/...`
    *   `19:16:20`: A log indicates `Compiling a graph for dynamic shape takes 43.16 s`.
    *   `19:16:38`: The process concludes with a total time of **54.25 seconds**.
*   `19:17:43`: The vLLM engine profiles the hardware and calculates the available KV cache.
*   `19:17:44` - `19:17:46`: **CUDA Graph Capturing begins.** The logs show the progress bar, and this step takes approximately **2-3 seconds**.
    *   `19:17:46`: `Graph capturing finished in 3 secs, took 1.64 GiB`
*   `19:17:46`: The final initialization log appears: `init engine (...) took 141.08 seconds`. This confirms the total startup time from engine initialization to readiness.
*   `19:17:49`: The application server reports `Application startup complete.` and is ready to accept traffic.
*   `19:17:45`: The first request (`/v1/chat/completions`) is logged with a latency of **6.28 seconds**. This request arrived while the instance was in its final startup phase and was likely queued, which is why its latency is lower than the full cold start time.

**Conclusion from Logs:**

Your observation about CUDA graph capturing is correct, but it's only a small part of the problem. The primary bottleneck during a cold start for this vLLM service is the just-in-time compilation performed by `torch.compile`.

---

### 2. Understanding the Technical Constraints

#### Why Can't CUDA Graphs Be Cached?

*   **During Cloud Build:** CUDA graph capturing is not a static compilation. It involves running the model on the actual target GPU to record the sequence of kernel launches. Since Cloud Build runners do not have GPUs, this step is impossible to perform during the image build.
*   **Between Cold Starts:** Cloud Run is fundamentally stateless. When an instance scales to zero, its entire state (CPU memory, GPU memory, and ephemeral filesystem) is discarded. A subsequent request triggering a new instance (a "subsequent cold start") is identical to the very first one. It starts from the pristine container image and must re-initialize everything, including recapturing the CUDA graph.

---

### 3. Actionable Recommendations for Optimization

Here are concrete steps to address the cold start latency, from most impactful to least.

#### Recommendation 1: Eliminate Cold Starts with `min-instances` (Highest Impact)

For latency-sensitive applications like LLM inference, the best solution is to avoid cold starts altogether.

*   **Action:** Configure your Cloud Run service to have a minimum of one instance running at all times.
*   **How:** Use the following `gcloud` command to update your service:
    ```bash
    gcloud run services update vllm-gemma-3-1b-it \
      --region us-central1 \
      --min-instances=1
    ```
*   **Why it Works:** This keeps one container instance "warm." It will have already completed the 141-second initialization, loaded the model into the GPU, and captured the CUDA graph. It will be ready to serve requests with low latency instantly. Subsequent scaling will still involve cold starts, but the first user will never experience one.
*   **Trade-off:** You will be billed for one instance running 24/7. Given the significant startup time, this is a necessary trade-off for a production-level inference service.

#### Recommendation 2: Speed Up Cold Starts with Startup CPU Boost

While `min-instances=1` is the best solution, you can also make the cold start process itself faster for scaling events.

*   **Action:** Enable the "CPU boost" feature for container startup.
*   **How:**
    ```bash
    gcloud run services update vllm-gemma-3-1b-it \
      --region us-central1 \
      --cpu-boost
    ```
*   **Why it Works:** Cloud Run allocates significantly more CPU power to the instance during the startup phase (initialization and first request). Since the `torch.compile` step is CPU-intensive, this can substantially reduce its duration, thereby shortening the overall cold start time.

#### Recommendation 3: Pre-warm the `torch.compile` Cache in Your Dockerfile (Advanced)

The logs show that the `torch.compile` step is the main bottleneck. You can perform this compilation during your `docker build` process and bake the results into your container image. This moves the 54-second delay from runtime to build time.

*   **Action:** Add a step to your `Dockerfile` that runs a lightweight Python script to trigger the model compilation and populate the cache.
*   **1. Create a `warmup.py` script:**
    ```python
    # warmup.py
    # This script initializes the vLLM engine just enough to trigger
    # the torch.compile cache to be created.
    
    from vllm import LLMEngine, EngineArgs
    
    # These arguments should match your service's startup command arguments
    # Ensure the model is baked into the image or accessible during build
    args = EngineArgs(model="google/gemma-3-1b-it", # Or local path
                      enforce_eager=False,
                      enable_torch_compile=True,
                      # Add other relevant args like dtype, tensor_parallel_size, etc.
                      # Use minimal GPU memory to just trigger compilation
                      gpu_memory_utilization=0.1 
                     )
    
    print("Starting engine initialization to warm up torch.compile cache...")
    # This will trigger the compilation and cache population
    engine = LLMEngine.from_engine_args(args, usage_context=0)
    print("Cache warmup complete.")
    
    # We don't need to keep the engine running, just needed to create the cache files.
    ```
*   **2. Modify your `Dockerfile`:**
    You need a builder stage that has access to a GPU to run the warmup script. Unfortunately, standard Cloud Build doesn't provide this. **However, you can often generate the torch compile cache without a GPU**, as it primarily depends on the model's architecture.

    Let's try a no-GPU approach first. If that fails, a local Docker build on a GPU machine would be required.

    ```dockerfile
    # Your base image
    FROM vllm/vllm-openai:latest
    
    # Install any dependencies
    # RUN pip install ...
    
    # Copy your model weights into the image (BEST PRACTICE)
    # This avoids downloading the model on every startup
    COPY ./gemma-3-1b-it /app/gemma-3-1b-it
    
    # --- NEW WARMUP STEP ---
    COPY warmup.py /app/warmup.py
    WORKDIR /app
    
    # Attempt to run the warmup script during the build.
    # This will populate the /root/.cache/vllm/torch_compile_cache/ directory
    # The key is to see if it works without a GPU attached during `docker build`
    RUN python warmup.py
    
    # Your original CMD to start the server
    CMD ["python", "-m", "vllm.entrypoints.openai.api_server", \
         "--model", "/app/gemma-3-1b-it", \
         "--host", "0.0.0.0", \
         "--port", "8000"]
    ```
*   **Why it Works:** When the container starts in Cloud Run, `torch.compile` will find the pre-populated cache directory (`/root/.cache/...`) inside the container image and skip the lengthy compilation step, drastically reducing the startup time.
--------------------
[2025-10-15 03:26:18] User: please create github issue in repository vllm-container-prewarm containing details for solution 1, as a feature enhancement
[2025-10-15 03:26:18] Bot: Of course. As an expert in Google Cloud Run performance analysis, I can provide a detailed breakdown based on your query and the provided logs.

### Executive Summary

Your analysis is sharp; the "Capturing CUDA graphs" step is indeed part of the cold start initialization. However, based on the logs, it's not the most significant contributor to the total startup time.

1.  **Direct Answer:** No, you **cannot** execute the CUDA graph capturing during the Cloud Build stage. This process requires an active GPU, which is not available in standard Cloud Build environments. Caching the graph from a first cold start for subsequent ones is also not possible due to Cloud Run's stateless natureâ€”each new instance starts from a clean container image.
2.  **Key Log Finding:** The total engine initialization took **141.08 seconds** (over 2.3 minutes). The CUDA graph capture only accounted for **~3 seconds** of this. The largest portion of the startup time is consumed by `torch.compile`, which took **54.25 seconds**.
3.  **Primary Recommendation:** To eliminate cold starts entirely, set the service's minimum instances to 1 (`--min-instances=1`). This keeps one container instance constantly warm and ready to serve requests, bypassing the lengthy initialization process for the first user.
4.  **Secondary Recommendation:** To speed up the initialization when new instances *do* start (e.g., during scaling), you can optimize the `torch.compile` step by pre-warming its cache during your container build process.

Below is a detailed analysis and a set of actionable steps.

---

### 1. Detailed Log Analysis

I've analyzed the startup sequence for instance `0069c7a...` to build a clear timeline of the cold start process.

**Cold Start Timeline:**

*   `19:15:25`: Model loading begins. It completes quickly, taking **~2 seconds** and using 1.91 GiB of memory.
*   `19:15:25` - `19:16:38`: The `torch.compile` process runs. This is the most time-consuming part of the startup:
    *   `19:15:36`: The log mentions using a cache directory: `/root/.cache/vllm/torch_compile_cache/...`
    *   `19:16:20`: A log indicates `Compiling a graph for dynamic shape takes 43.16 s`.
    *   `19:16:38`: The process concludes with a total time of **54.25 seconds**.
*   `19:17:43`: The vLLM engine profiles the hardware and calculates the available KV cache.
*   `19:17:44` - `19:17:46`: **CUDA Graph Capturing begins.** The logs show the progress bar, and this step takes approximately **2-3 seconds**.
    *   `19:17:46`: `Graph capturing finished in 3 secs, took 1.64 GiB`
*   `19:17:46`: The final initialization log appears: `init engine (...) took 141.08 seconds`. This confirms the total startup time from engine initialization to readiness.
*   `19:17:49`: The application server reports `Application startup complete.` and is ready to accept traffic.
*   `19:17:45`: The first request (`/v1/chat/completions`) is logged with a latency of **6.28 seconds**. This request arrived while the instance was in its final startup phase and was likely queued, which is why its latency is lower than the full cold start time.

**Conclusion from Logs:**

Your observation about CUDA graph capturing is correct, but it's only a small part of the problem. The primary bottleneck during a cold start for this vLLM service is the just-in-time compilation performed by `torch.compile`.

---

### 2. Understanding the Technical Constraints

#### Why Can't CUDA Graphs Be Cached?

*   **During Cloud Build:** CUDA graph capturing is not a static compilation. It involves running the model on the actual target GPU to record the sequence of kernel launches. Since Cloud Build runners do not have GPUs, this step is impossible to perform during the image build.
*   **Between Cold Starts:** Cloud Run is fundamentally stateless. When an instance scales to zero, its entire state (CPU memory, GPU memory, and ephemeral filesystem) is discarded. A subsequent request triggering a new instance (a "subsequent cold start") is identical to the very first one. It starts from the pristine container image and must re-initialize everything, including recapturing the CUDA graph.

---

### 3. Actionable Recommendations for Optimization

Here are concrete steps to address the cold start latency, from most impactful to least.

#### Recommendation 1: Eliminate Cold Starts with `min-instances` (Highest Impact)

For latency-sensitive applications like LLM inference, the best solution is to avoid cold starts altogether.

*   **Action:** Configure your Cloud Run service to have a minimum of one instance running at all times.
*   **How:** Use the following `gcloud` command to update your service:
    ```bash
    gcloud run services update vllm-gemma-3-1b-it \
      --region us-central1 \
      --min-instances=1
    ```
*   **Why it Works:** This keeps one container instance "warm." It will have already completed the 141-second initialization, loaded the model into the GPU, and captured the CUDA graph. It will be ready to serve requests with low latency instantly. Subsequent scaling will still involve cold starts, but the first user will never experience one.
*   **Trade-off:** You will be billed for one instance running 24/7. Given the significant startup time, this is a necessary trade-off for a production-level inference service.

#### Recommendation 2: Speed Up Cold Starts with Startup CPU Boost

While `min-instances=1` is the best solution, you can also make the cold start process itself faster for scaling events.

*   **Action:** Enable the "CPU boost" feature for container startup.
*   **How:**
    ```bash
    gcloud run services update vllm-gemma-3-1b-it \
      --region us-central1 \
      --cpu-boost
    ```
*   **Why it Works:** Cloud Run allocates significantly more CPU power to the instance during the startup phase (initialization and first request). Since the `torch.compile` step is CPU-intensive, this can substantially reduce its duration, thereby shortening the overall cold start time.

#### Recommendation 3: Pre-warm the `torch.compile` Cache in Your Dockerfile (Advanced)

The logs show that the `torch.compile` step is the main bottleneck. You can perform this compilation during your `docker build` process and bake the results into your container image. This moves the 54-second delay from runtime to build time.

*   **Action:** Add a step to your `Dockerfile` that runs a lightweight Python script to trigger the model compilation and populate the cache.
*   **1. Create a `warmup.py` script:**
    ```python
    # warmup.py
    # This script initializes the vLLM engine just enough to trigger
    # the torch.compile cache to be created.
    
    from vllm import LLMEngine, EngineArgs
    
    # These arguments should match your service's startup command arguments
    # Ensure the model is baked into the image or accessible during build
    args = EngineArgs(model="google/gemma-3-1b-it", # Or local path
                      enforce_eager=False,
                      enable_torch_compile=True,
                      # Add other relevant args like dtype, tensor_parallel_size, etc.
                      # Use minimal GPU memory to just trigger compilation
                      gpu_memory_utilization=0.1 
                     )
    
    print("Starting engine initialization to warm up torch.compile cache...")
    # This will trigger the compilation and cache population
    engine = LLMEngine.from_engine_args(args, usage_context=0)
    print("Cache warmup complete.")
    
    # We don't need to keep the engine running, just needed to create the cache files.
    ```
*   **2. Modify your `Dockerfile`:**
    You need a builder stage that has access to a GPU to run the warmup script. Unfortunately, standard Cloud Build doesn't provide this. **However, you can often generate the torch compile cache without a GPU**, as it primarily depends on the model's architecture.

    Let's try a no-GPU approach first. If that fails, a local Docker build on a GPU machine would be required.

    ```dockerfile
    # Your base image
    FROM vllm/vllm-openai:latest
    
    # Install any dependencies
    # RUN pip install ...
    
    # Copy your model weights into the image (BEST PRACTICE)
    # This avoids downloading the model on every startup
    COPY ./gemma-3-1b-it /app/gemma-3-1b-it
    
    # --- NEW WARMUP STEP ---
    COPY warmup.py /app/warmup.py
    WORKDIR /app
    
    # Attempt to run the warmup script during the build.
    # This will populate the /root/.cache/vllm/torch_compile_cache/ directory
    # The key is to see if it works without a GPU attached during `docker build`
    RUN python warmup.py
    
    # Your original CMD to start the server
    CMD ["python", "-m", "vllm.entrypoints.openai.api_server", \
         "--model", "/app/gemma-3-1b-it", \
         "--host", "0.0.0.0", \
         "--port", "8000"]
    ```
*   **Why it Works:** When the container starts in Cloud Run, `torch.compile` will find the pre-populated cache directory (`/root/.cache/...`) inside the container image and skip the lengthy compilation step, drastically reducing the startup time.
--------------------
[2025-10-15 03:30:00] User: is there a way to execute or cache the "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): " part during cloud build stage or during the first cold start, so that subsequent cold starts can be shorter?
[2025-10-15 03:30:00] Bot: No logs found for service 'None'. It's difficult to propose a solution without logs. Consider checking if the service is running or if the logs are being exported correctly. Based on your query: 'is there a way to execute or cache the "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): " part during cloud build stage or during the first cold start, so that subsequent cold starts can be shorter?', a potential solution could involve optimizing the cold start process by pre-warming the service or caching frequently used resources.
--------------------
[2025-10-15 03:31:35] User: for cloud run service vllm-gemma-3-1b-it, is there a way to execute or cache the "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): " part during cloud build stage or during the first cold start, so that subsequent cold starts can be shorter?
[2025-10-15 03:31:35] Bot: Of course. As an expert in Google Cloud Run performance optimization, I can analyze these logs and provide a detailed solution to your query about caching CUDA graphs and reducing cold start times for your `vllm-gemma-3-1b-it` service.

Let's break this down.

### 1. Analysis of Your Cloud Run Logs

Your query specifically asks about caching the "Capturing CUDA graphs" step. By analyzing the provided logs chronologically during a cold start, we can build a precise timeline and identify the true bottlenecks.

Here is the sequence of events during the container's initialization:

| Timestamp (Local) | Duration | Step | Log Evidence |
| :--- | :--- | :--- | :--- |
| 12:15:25 | ~2 sec | **Model Loading** | `INFO [gpu_model_runner.py:2007] Model loading took 1.9147 GiB and 1.868946 seconds` |
| 12:15:36 | ~11 sec | **TorchDynamo Transform** | `INFO [backends.py:559] Dynamo bytecode transform time: 11.09 s` |
| 12:16:20 | ~43 sec | **Graph Compilation** | `INFO [backends.py:215] Compiling a graph for dynamic shape takes 43.16 s` |
| 12:16:38 | **~54 sec (Total)** | **`torch.compile`** | `INFO [monitor.py:34] torch.compile takes 54.25 s in total` |
| 12:17:44 | **~3 sec** | **CUDA Graph Capturing** | `stderr: Capturing CUDA graphs...` and `INFO [gpu_model_runner.py:2708] Graph capturing finished in 3 secs...` |
| 12:17:46 | **141 sec (Total)** | **vLLM Engine Initialization** | `INFO [core.py:214] init engine (profile, create kv cache, warmup model) took 141.08 seconds` |
| 12:17:49 | - | **Application Ready** | `INFO: Application startup complete.` and `STARTUP TCP probe succeeded...` |

**Key Findings from the Logs:**

1.  **The Real Bottleneck is `torch.compile`:** You are correct to focus on initialization, but the logs clearly show that the `Capturing CUDA graphs` step only takes **3 seconds**. The major contributor to the startup delay is the `torch.compile` process, which takes **~54 seconds**. This process transforms and compiles the PyTorch model into optimized kernels before the CUDA graphs are even captured.
2.  **Total Initialization Time is High:** The entire vLLM engine initialization takes **141 seconds (2 minutes and 21 seconds)**. This is the total cold start time you are experiencing from the application's perspective. Your goal should be to reduce this entire 141-second period, with a primary focus on the 54-second `torch.compile` step.

---

### 2. Answering Your Core Question: Caching CUDA Graphs & Compiled Models

You asked if this process can be moved to the build stage or cached after the first cold start.

#### Can it be done during the Cloud Build stage?

**No, this is not feasible.** Here's why:

*   **No GPU Access:** Standard Cloud Build environments do not have GPUs. CUDA operations, including graph capturing and model compilation with Triton, require a specific NVIDIA GPU to be present.
*   **Hardware Dependency:** The compiled artifacts from `torch.compile` and the captured CUDA graphs are highly specific to the GPU architecture they are created on (e.g., NVIDIA T4, L4, A100). The GPU in a build environment (if you could get one) would not be guaranteed to match the GPU your Cloud Run service is deployed on, making the cache invalid.

#### Can it be cached after the first cold start for subsequent cold starts?

**Yes, this is the correct approach and the recommended solution.**

The output of `torch.compile` can be written to a cache directory. If this directory is on a persistent, shared filesystem, new container instances starting up can read from this cache instead of re-running the expensive compilation process. This directly targets the 54-second bottleneck.

---

### 3. Actionable Recommendations for Optimization

Here are concrete steps to drastically reduce your cold start time.

#### Recommendation 1: Use a Persistent Cache for `torch.compile` (Primary Solution)

This will cache the 54-second compilation step. The *first* container instance will still have a long startup while it populates the cache, but every subsequent cold start will be significantly faster.

You can achieve this by mounting a network filesystem to your Cloud Run service.

**Steps:**

1.  **Set up a Network Filesystem:** You have two main options:
    *   **Google Cloud Filestore (NFS):** High-performance, low-latency. This is the ideal choice for this use case. It requires you to set up a [Serverless VPC Access connector](https://cloud.google.com/run/docs/configuring/connecting-vpc).
    *   **Google Cloud Storage FUSE:** Allows you to mount a GCS bucket as a filesystem. It's easier to set up but has higher latency than Filestore. It may be sufficient here since the cache is mostly read after the first population.

2.  **Mount the Filesystem to Cloud Run:**
    *   When deploying your Cloud Run service, add flags to mount the volume.
    *   **Example using gcloud with Filestore:**
        ```bash
        gcloud run deploy vllm-gemma-3-1b-it \
          --image gcr.io/your-project/your-image \
          --vpc-connector projects/your-project/locations/us-central1/connectors/your-connector \
          --add-volume=name=torch-cache,type=nfs,location=YOUR_FILESTORE_IP:/your-share \
          --add-volume-mount=volume=torch-cache,mount-path=/mnt/cache \
          ... # other flags like --gpu, --region, etc.
        ```
    *   Replace `YOUR_FILESTORE_IP` and `/your-share` with your Filestore instance details. The `mount-path` can be anything, e.g., `/mnt/cache`.

3.  **Set the vLLM Cache Environment Variable:**
    *   Instruct vLLM (and underlying PyTorch) to use this mounted directory for its cache by setting an environment variable in your Cloud Run service.
    *   **Example using gcloud:**
        ```bash
        gcloud run deploy vllm-gemma-3-1b-it \
          --image ... \
          --set-env-vars="VLLM_TORCH_COMPILE_CACHE_DIR=/mnt/cache/torch_compile" \
          ... # other flags
        ```
    *   **Result:** Now, when a new instance starts, it will check `/mnt/cache/torch_compile` for the compiled model. If found, it will skip the ~54-second compilation step.

#### Recommendation 2: Eliminate Cold Starts with `min-instances`

If your application has consistent traffic and you need to guarantee low latency for all users, the most effective solution is to keep at least one container instance warm at all times.

*   **Action:** Set `min-instances` to 1. This means one instance will run continuously, having already completed the 141-second initialization. It will be ready to serve requests instantly.
*   **Trade-off:** This has cost implications, as the instance (and the attached GPU) will be billed for as long as it's running, even when idle.
*   **gcloud command:**
    ```bash
    gcloud run services update vllm-gemma-3-1b-it --min-instances=1
    ```

#### Recommendation 3: Accelerate Initialization with Startup CPU Boost

This feature allocates more CPU resources to your container *only during startup and shutdown*, which can significantly speed up CPU-bound initialization tasks like `torch.compile`.

*   **Action:** Enable Startup CPU Boost for your service. This is a cost-effective way to make the initial cold start faster without paying for higher CPU during normal operation.
*   **gcloud command:**
    ```bash
    gcloud run services update vllm-gemma-3-1b-it --cpu-boost
    ```

### Summary of Solutions

| Solution | What it Solves | Impact on Cold Start | Cost Impact | Recommendation |
| :--- | :--- | :--- | :--- | :--- |
| **1. Persistent Cache** | Re-compilation on new instances. | Drastically reduces startup time for the **2nd instance onward**. | Low (cost of Filestore/GCS). | **Highly Recommended.** This directly addresses the technical bottleneck. |
| **2. `min-instances=1`** | User-facing cold starts. | **Eliminates cold starts** for incoming traffic (as long as one instance can handle the load). | Medium/High (continuous billing for one instance + GPU). | Best for production services requiring consistent low latency. |
| **3. Startup CPU Boost** | Slow CPU-bound initialization. | **Reduces the startup time** for *all* cold starts, including the very first one. | Low (only billed for extra CPU during startup). | **Highly Recommended.** Easy to enable and very effective. |

For the best results, **combine Recommendations 1 and 3**. This will make your first cold start faster (due to CPU boost) and subsequent cold starts dramatically faster (due to the persistent cache). If your budget allows and your service requires it, add Recommendation 2 to eliminate cold starts for your users entirely.
--------------------
[2025-10-15 03:35:12] User: please create github issue in repository vllm-container-prewarm containing details for solution 1, as a feature enhancement
[2025-10-15 03:35:12] Bot: Of course. As an expert in Google Cloud Run performance optimization, I can analyze these logs and provide a detailed solution to your query about caching CUDA graphs and reducing cold start times for your `vllm-gemma-3-1b-it` service.

Let's break this down.

### 1. Analysis of Your Cloud Run Logs

Your query specifically asks about caching the "Capturing CUDA graphs" step. By analyzing the provided logs chronologically during a cold start, we can build a precise timeline and identify the true bottlenecks.

Here is the sequence of events during the container's initialization:

| Timestamp (Local) | Duration | Step | Log Evidence |
| :--- | :--- | :--- | :--- |
| 12:15:25 | ~2 sec | **Model Loading** | `INFO [gpu_model_runner.py:2007] Model loading took 1.9147 GiB and 1.868946 seconds` |
| 12:15:36 | ~11 sec | **TorchDynamo Transform** | `INFO [backends.py:559] Dynamo bytecode transform time: 11.09 s` |
| 12:16:20 | ~43 sec | **Graph Compilation** | `INFO [backends.py:215] Compiling a graph for dynamic shape takes 43.16 s` |
| 12:16:38 | **~54 sec (Total)** | **`torch.compile`** | `INFO [monitor.py:34] torch.compile takes 54.25 s in total` |
| 12:17:44 | **~3 sec** | **CUDA Graph Capturing** | `stderr: Capturing CUDA graphs...` and `INFO [gpu_model_runner.py:2708] Graph capturing finished in 3 secs...` |
| 12:17:46 | **141 sec (Total)** | **vLLM Engine Initialization** | `INFO [core.py:214] init engine (profile, create kv cache, warmup model) took 141.08 seconds` |
| 12:17:49 | - | **Application Ready** | `INFO: Application startup complete.` and `STARTUP TCP probe succeeded...` |

**Key Findings from the Logs:**

1.  **The Real Bottleneck is `torch.compile`:** You are correct to focus on initialization, but the logs clearly show that the `Capturing CUDA graphs` step only takes **3 seconds**. The major contributor to the startup delay is the `torch.compile` process, which takes **~54 seconds**. This process transforms and compiles the PyTorch model into optimized kernels before the CUDA graphs are even captured.
2.  **Total Initialization Time is High:** The entire vLLM engine initialization takes **141 seconds (2 minutes and 21 seconds)**. This is the total cold start time you are experiencing from the application's perspective. Your goal should be to reduce this entire 141-second period, with a primary focus on the 54-second `torch.compile` step.

---

### 2. Answering Your Core Question: Caching CUDA Graphs & Compiled Models

You asked if this process can be moved to the build stage or cached after the first cold start.

#### Can it be done during the Cloud Build stage?

**No, this is not feasible.** Here's why:

*   **No GPU Access:** Standard Cloud Build environments do not have GPUs. CUDA operations, including graph capturing and model compilation with Triton, require a specific NVIDIA GPU to be present.
*   **Hardware Dependency:** The compiled artifacts from `torch.compile` and the captured CUDA graphs are highly specific to the GPU architecture they are created on (e.g., NVIDIA T4, L4, A100). The GPU in a build environment (if you could get one) would not be guaranteed to match the GPU your Cloud Run service is deployed on, making the cache invalid.

#### Can it be cached after the first cold start for subsequent cold starts?

**Yes, this is the correct approach and the recommended solution.**

The output of `torch.compile` can be written to a cache directory. If this directory is on a persistent, shared filesystem, new container instances starting up can read from this cache instead of re-running the expensive compilation process. This directly targets the 54-second bottleneck.

---

### 3. Actionable Recommendations for Optimization

Here are concrete steps to drastically reduce your cold start time.

#### Recommendation 1: Use a Persistent Cache for `torch.compile` (Primary Solution)

This will cache the 54-second compilation step. The *first* container instance will still have a long startup while it populates the cache, but every subsequent cold start will be significantly faster.

You can achieve this by mounting a network filesystem to your Cloud Run service.

**Steps:**

1.  **Set up a Network Filesystem:** You have two main options:
    *   **Google Cloud Filestore (NFS):** High-performance, low-latency. This is the ideal choice for this use case. It requires you to set up a [Serverless VPC Access connector](https://cloud.google.com/run/docs/configuring/connecting-vpc).
    *   **Google Cloud Storage FUSE:** Allows you to mount a GCS bucket as a filesystem. It's easier to set up but has higher latency than Filestore. It may be sufficient here since the cache is mostly read after the first population.

2.  **Mount the Filesystem to Cloud Run:**
    *   When deploying your Cloud Run service, add flags to mount the volume.
    *   **Example using gcloud with Filestore:**
        ```bash
        gcloud run deploy vllm-gemma-3-1b-it \
          --image gcr.io/your-project/your-image \
          --vpc-connector projects/your-project/locations/us-central1/connectors/your-connector \
          --add-volume=name=torch-cache,type=nfs,location=YOUR_FILESTORE_IP:/your-share \
          --add-volume-mount=volume=torch-cache,mount-path=/mnt/cache \
          ... # other flags like --gpu, --region, etc.
        ```
    *   Replace `YOUR_FILESTORE_IP` and `/your-share` with your Filestore instance details. The `mount-path` can be anything, e.g., `/mnt/cache`.

3.  **Set the vLLM Cache Environment Variable:**
    *   Instruct vLLM (and underlying PyTorch) to use this mounted directory for its cache by setting an environment variable in your Cloud Run service.
    *   **Example using gcloud:**
        ```bash
        gcloud run deploy vllm-gemma-3-1b-it \
          --image ... \
          --set-env-vars="VLLM_TORCH_COMPILE_CACHE_DIR=/mnt/cache/torch_compile" \
          ... # other flags
        ```
    *   **Result:** Now, when a new instance starts, it will check `/mnt/cache/torch_compile` for the compiled model. If found, it will skip the ~54-second compilation step.

#### Recommendation 2: Eliminate Cold Starts with `min-instances`

If your application has consistent traffic and you need to guarantee low latency for all users, the most effective solution is to keep at least one container instance warm at all times.

*   **Action:** Set `min-instances` to 1. This means one instance will run continuously, having already completed the 141-second initialization. It will be ready to serve requests instantly.
*   **Trade-off:** This has cost implications, as the instance (and the attached GPU) will be billed for as long as it's running, even when idle.
*   **gcloud command:**
    ```bash
    gcloud run services update vllm-gemma-3-1b-it --min-instances=1
    ```

#### Recommendation 3: Accelerate Initialization with Startup CPU Boost

This feature allocates more CPU resources to your container *only during startup and shutdown*, which can significantly speed up CPU-bound initialization tasks like `torch.compile`.

*   **Action:** Enable Startup CPU Boost for your service. This is a cost-effective way to make the initial cold start faster without paying for higher CPU during normal operation.
*   **gcloud command:**
    ```bash
    gcloud run services update vllm-gemma-3-1b-it --cpu-boost
    ```

### Summary of Solutions

| Solution | What it Solves | Impact on Cold Start | Cost Impact | Recommendation |
| :--- | :--- | :--- | :--- | :--- |
| **1. Persistent Cache** | Re-compilation on new instances. | Drastically reduces startup time for the **2nd instance onward**. | Low (cost of Filestore/GCS). | **Highly Recommended.** This directly addresses the technical bottleneck. |
| **2. `min-instances=1`** | User-facing cold starts. | **Eliminates cold starts** for incoming traffic (as long as one instance can handle the load). | Medium/High (continuous billing for one instance + GPU). | Best for production services requiring consistent low latency. |
| **3. Startup CPU Boost** | Slow CPU-bound initialization. | **Reduces the startup time** for *all* cold starts, including the very first one. | Low (only billed for extra CPU during startup). | **Highly Recommended.** Easy to enable and very effective. |

For the best results, **combine Recommendations 1 and 3**. This will make your first cold start faster (due to CPU boost) and subsequent cold starts dramatically faster (due to the persistent cache). If your budget allows and your service requires it, add Recommendation 2 to eliminate cold starts for your users entirely.
--------------------
[2025-10-15 03:38:32] User: is there a way to execute or cache the "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): " part during cloud build stage or during the first cold start, so that subsequent cold starts can be shorter?
[2025-10-15 03:38:32] Bot: No logs found for service 'None'. It's difficult to propose a solution without logs. Consider checking if the service is running or if the logs are being exported correctly. Based on your query: 'is there a way to execute or cache the "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): " part during cloud build stage or during the first cold start, so that subsequent cold starts can be shorter?', a potential solution could involve optimizing the cold start process by pre-warming the service or caching frequently used resources.
--------------------
[2025-10-15 03:40:08] User: for cloud run service vllm-gemma-3-1b-it, is there a way to execute or cache the "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): " part during cloud build stage or during the first cold start, so that subsequent cold starts can be shorter?
[2025-10-15 03:40:08] Bot: Of course. As an expert in Google Cloud Run log analysis and performance optimization, I can provide a detailed breakdown of your situation and offer actionable solutions.

Here is a comprehensive analysis based on your query and the provided logs.

### Executive Summary

Your core question is whether the `Capturing CUDA graphs` step in vLLM can be cached during the container build or across cold starts to reduce startup time.

**The short answer is no, this is not feasible with a standard Cloud Run setup.**

1.  **Build-Time Caching:** CUDA graph capturing and `torch.compile` are runtime operations that require access to the specific GPU hardware your Cloud Run service will use (e.g., an L4 GPU). Cloud Build environments are CPU-only and do not have GPUs, making it impossible to pre-capture or pre-compile these GPU-specific artifacts.
2.  **Cross-Instance Caching:** Each Cloud Run instance is ephemeral. When an instance scales down to zero, its entire state, including any in-memory or on-disk caches generated during startup, is discarded. The next cold start creates a completely new, clean instance from the container image, which must repeat the entire initialization process.

However, your logs reveal critical details about the cold start process, and there are effective strategies to mitigate this latency. The most significant bottleneck is not the CUDA graph capture itself, but the preceding `torch.compile` step.

---

### 1. Detailed Log Analysis: Identifying the Cold Start Bottlenecks

By analyzing the timestamps in your logs, we can reconstruct the timeline of a cold start for your `vllm-gemma-3-1b-it` service.

**Cold Start Timeline (Instance `0069c7a...`)**

*   `19:15:25`: **Model Loading Begins & Ends.**
    *   `INFO ... [gpu_model_runner.py:2007] Model loading took 1.9147 GiB and 1.868946 seconds`
    *   *Observation:* Loading the model from the container image into GPU memory is very fast (~2 seconds). This is not the bottleneck.

*   `19:15:36`: **PyTorch Compilation (`torch.compile`) Begins.**
    *   `INFO ... [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/...`
    *   *Observation:* vLLM is attempting to use a cache for compiled artifacts. However, on Cloud Run, this `/root/.cache` directory is part of the container's ephemeral filesystem and will not persist after the instance is terminated.

*   `19:16:38`: **PyTorch Compilation (`torch.compile`) Finishes.**
    *   `INFO ... [monitor.py:34] torch.compile takes 54.25 s in total`
    *   **Key Finding:** This is the single largest contributor to your cold start time, taking nearly a minute. Your application is spending most of its startup time compiling the model with PyTorch Inductor for the specific GPU architecture.

*   `19:17:44` - `19:17:46`: **CUDA Graph Capturing Begins & Ends.**
    *   `Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 0%|...`
    *   `INFO ... [gpu_model_runner.py:2708] Graph capturing finished in 3 secs, took 1.64 GiB`
    *   **Key Finding:** While noticeable, the CUDA graph capture only takes **~3 seconds**. It is a much smaller part of the overall cold start compared to `torch.compile`.

*   `19:17:46`: **Engine Initialization Completes.**
    *   `INFO ... [core.py:214] init engine (profile, create kv cache, warmup model) took 141.08 seconds`
    *   *Observation:* The total time from starting the engine initialization to being ready was over 2 minutes. This includes model loading, compilation, graph capture, and other setup.

*   `19:17:49`: **Application Ready.**
    *   `INFO: Application startup complete.`
    *   `STARTUP TCP probe succeeded...`
    *   *Observation:* At this point, the container is fully initialized and Cloud Run marks it as ready to serve traffic.

*   `19:29:27`: **Instance Shutdown.**
    *   `INFO ... Shutting down FastAPI HTTP server.`
    *   *Observation:* After a period of inactivity (~11 minutes), Cloud Run scaled the instance down to zero, leading to the next request experiencing another full cold start.

### 2. Why Caching is Not Straightforward on Cloud Run

*   **Runtime vs. Build-Time:** The compilation and graph capture steps analyze the model's operations and translate them into highly optimized machine code (kernels) specifically for the GPU architecture (e.g., NVIDIA L4) and CUDA version present in the runtime environment. This process cannot be done at build time without access to the identical target hardware.
*   **Ephemeral Filesystem:** As mentioned, any files written to the container's local disk, including the `torch_compile_cache`, are lost when the instance is shut down.

---

### 3. Actionable Solutions and Recommendations

Given these constraints, the best approach is to either prevent the instance from shutting down or to accelerate the startup process itself.

#### Recommendation 1: Configure Minimum Instances (Most Effective Solution)

The most reliable way to eliminate cold starts is to prevent your service from scaling to zero. By keeping at least one instance warm, you pay the initialization cost only once when a new revision is deployed or when scaling up.

*   **Action:** Set the minimum number of instances for your Cloud Run service to `1`.
    ```bash
    gcloud run services update vllm-gemma-3-1b-it \
      --platform managed \
      --region us-central1 \
      --min-instances=1
    ```
*   **Trade-off:** This will incur costs for keeping one GPU instance running 24/7. However, for a production service where low latency is critical, this is often the necessary choice.

#### Recommendation 2: Use Startup CPU Boost

Cloud Run offers a "Startup CPU Boost" feature that allocates a more powerful CPU during the instance startup and for 10 seconds after. This can significantly speed up CPU-bound initialization tasks like code loading, dependency injection, and parts of the `torch.compile` process.

*   **Action:** Enable Startup CPU Boost during deployment.
    ```bash
    gcloud run deploy vllm-gemma-3-1b-it \
      --image gcr.io/your-project/your-image \
      --[other-flags] \
      --cpu-boost
    ```
*   **Benefit:** This can reduce the ~54 seconds spent on `torch.compile` and other CPU-intensive startup tasks.

#### Recommendation 3 (Advanced): Persist the `torch.compile` Cache with a Network Filesystem

You can potentially persist the cache generated by `torch.compile` across cold starts by mounting a network filesystem to the cache directory.

*   **Action:**
    1.  Create a Google Cloud Filestore (NFS) instance in the same VPC network as your Cloud Run service.
    2.  Configure your Cloud Run service to mount this Filestore instance.
    3.  In your `Dockerfile` or startup script, create a symbolic link from `/root/.cache/vllm` to a directory on the mounted Filestore share. For example: `ln -s /mnt/nfs/cache /root/.cache/vllm`.
*   **Benefit:** On the *second* and subsequent cold starts, `torch.compile` should find the cached kernels on the NFS share, potentially reducing the ~54-second compilation time to just a few seconds of file access.
*   **Caveats:**
    *   NFS introduces its own network latency.
    *   This is a more complex setup requiring VPC and Filestore configuration.
    *   This will only cache the `torch.compile` results, not the CUDA graphs which are often tied to the specific GPU session. However, since compilation is the main bottleneck, the benefit can still be substantial.

#### Recommendation 4: Optimize Container Image and Model Loading

Ensure your model is baked directly into your container image. The logs show your model loading is already fast, which suggests you are likely doing this correctly. Avoid downloading models from sources like Hugging Face Hub at runtime, as this adds network latency and variability to your startup time.

*   **Action:** Use a multi-stage `Dockerfile` to download the model assets in a builder stage and copy them into the final, clean runtime image. This keeps the final image as small as possible while including all necessary assets.
--------------------
[2025-10-15 03:41:04] User: "please create github issue in repository vllm-container-prewarm containing details for solution 1, as a feature enhancement"
[2025-10-15 03:41:04] Bot: Of course. As an expert in Google Cloud Run log analysis and performance optimization, I can provide a detailed breakdown of your situation and offer actionable solutions.

Here is a comprehensive analysis based on your query and the provided logs.

### Executive Summary

Your core question is whether the `Capturing CUDA graphs` step in vLLM can be cached during the container build or across cold starts to reduce startup time.

**The short answer is no, this is not feasible with a standard Cloud Run setup.**

1.  **Build-Time Caching:** CUDA graph capturing and `torch.compile` are runtime operations that require access to the specific GPU hardware your Cloud Run service will use (e.g., an L4 GPU). Cloud Build environments are CPU-only and do not have GPUs, making it impossible to pre-capture or pre-compile these GPU-specific artifacts.
2.  **Cross-Instance Caching:** Each Cloud Run instance is ephemeral. When an instance scales down to zero, its entire state, including any in-memory or on-disk caches generated during startup, is discarded. The next cold start creates a completely new, clean instance from the container image, which must repeat the entire initialization process.

However, your logs reveal critical details about the cold start process, and there are effective strategies to mitigate this latency. The most significant bottleneck is not the CUDA graph capture itself, but the preceding `torch.compile` step.

---

### 1. Detailed Log Analysis: Identifying the Cold Start Bottlenecks

By analyzing the timestamps in your logs, we can reconstruct the timeline of a cold start for your `vllm-gemma-3-1b-it` service.

**Cold Start Timeline (Instance `0069c7a...`)**

*   `19:15:25`: **Model Loading Begins & Ends.**
    *   `INFO ... [gpu_model_runner.py:2007] Model loading took 1.9147 GiB and 1.868946 seconds`
    *   *Observation:* Loading the model from the container image into GPU memory is very fast (~2 seconds). This is not the bottleneck.

*   `19:15:36`: **PyTorch Compilation (`torch.compile`) Begins.**
    *   `INFO ... [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/...`
    *   *Observation:* vLLM is attempting to use a cache for compiled artifacts. However, on Cloud Run, this `/root/.cache` directory is part of the container's ephemeral filesystem and will not persist after the instance is terminated.

*   `19:16:38`: **PyTorch Compilation (`torch.compile`) Finishes.**
    *   `INFO ... [monitor.py:34] torch.compile takes 54.25 s in total`
    *   **Key Finding:** This is the single largest contributor to your cold start time, taking nearly a minute. Your application is spending most of its startup time compiling the model with PyTorch Inductor for the specific GPU architecture.

*   `19:17:44` - `19:17:46`: **CUDA Graph Capturing Begins & Ends.**
    *   `Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 0%|...`
    *   `INFO ... [gpu_model_runner.py:2708] Graph capturing finished in 3 secs, took 1.64 GiB`
    *   **Key Finding:** While noticeable, the CUDA graph capture only takes **~3 seconds**. It is a much smaller part of the overall cold start compared to `torch.compile`.

*   `19:17:46`: **Engine Initialization Completes.**
    *   `INFO ... [core.py:214] init engine (profile, create kv cache, warmup model) took 141.08 seconds`
    *   *Observation:* The total time from starting the engine initialization to being ready was over 2 minutes. This includes model loading, compilation, graph capture, and other setup.

*   `19:17:49`: **Application Ready.**
    *   `INFO: Application startup complete.`
    *   `STARTUP TCP probe succeeded...`
    *   *Observation:* At this point, the container is fully initialized and Cloud Run marks it as ready to serve traffic.

*   `19:29:27`: **Instance Shutdown.**
    *   `INFO ... Shutting down FastAPI HTTP server.`
    *   *Observation:* After a period of inactivity (~11 minutes), Cloud Run scaled the instance down to zero, leading to the next request experiencing another full cold start.

### 2. Why Caching is Not Straightforward on Cloud Run

*   **Runtime vs. Build-Time:** The compilation and graph capture steps analyze the model's operations and translate them into highly optimized machine code (kernels) specifically for the GPU architecture (e.g., NVIDIA L4) and CUDA version present in the runtime environment. This process cannot be done at build time without access to the identical target hardware.
*   **Ephemeral Filesystem:** As mentioned, any files written to the container's local disk, including the `torch_compile_cache`, are lost when the instance is shut down.

---

### 3. Actionable Solutions and Recommendations

Given these constraints, the best approach is to either prevent the instance from shutting down or to accelerate the startup process itself.

#### Recommendation 1: Configure Minimum Instances (Most Effective Solution)

The most reliable way to eliminate cold starts is to prevent your service from scaling to zero. By keeping at least one instance warm, you pay the initialization cost only once when a new revision is deployed or when scaling up.

*   **Action:** Set the minimum number of instances for your Cloud Run service to `1`.
    ```bash
    gcloud run services update vllm-gemma-3-1b-it \
      --platform managed \
      --region us-central1 \
      --min-instances=1
    ```
*   **Trade-off:** This will incur costs for keeping one GPU instance running 24/7. However, for a production service where low latency is critical, this is often the necessary choice.

#### Recommendation 2: Use Startup CPU Boost

Cloud Run offers a "Startup CPU Boost" feature that allocates a more powerful CPU during the instance startup and for 10 seconds after. This can significantly speed up CPU-bound initialization tasks like code loading, dependency injection, and parts of the `torch.compile` process.

*   **Action:** Enable Startup CPU Boost during deployment.
    ```bash
    gcloud run deploy vllm-gemma-3-1b-it \
      --image gcr.io/your-project/your-image \
      --[other-flags] \
      --cpu-boost
    ```
*   **Benefit:** This can reduce the ~54 seconds spent on `torch.compile` and other CPU-intensive startup tasks.

#### Recommendation 3 (Advanced): Persist the `torch.compile` Cache with a Network Filesystem

You can potentially persist the cache generated by `torch.compile` across cold starts by mounting a network filesystem to the cache directory.

*   **Action:**
    1.  Create a Google Cloud Filestore (NFS) instance in the same VPC network as your Cloud Run service.
    2.  Configure your Cloud Run service to mount this Filestore instance.
    3.  In your `Dockerfile` or startup script, create a symbolic link from `/root/.cache/vllm` to a directory on the mounted Filestore share. For example: `ln -s /mnt/nfs/cache /root/.cache/vllm`.
*   **Benefit:** On the *second* and subsequent cold starts, `torch.compile` should find the cached kernels on the NFS share, potentially reducing the ~54-second compilation time to just a few seconds of file access.
*   **Caveats:**
    *   NFS introduces its own network latency.
    *   This is a more complex setup requiring VPC and Filestore configuration.
    *   This will only cache the `torch.compile` results, not the CUDA graphs which are often tied to the specific GPU session. However, since compilation is the main bottleneck, the benefit can still be substantial.

#### Recommendation 4: Optimize Container Image and Model Loading

Ensure your model is baked directly into your container image. The logs show your model loading is already fast, which suggests you are likely doing this correctly. Avoid downloading models from sources like Hugging Face Hub at runtime, as this adds network latency and variability to your startup time.

*   **Action:** Use a multi-stage `Dockerfile` to download the model assets in a builder stage and copy them into the final, clean runtime image. This keeps the final image as small as possible while including all necessary assets.
--------------------
[2025-10-15 03:47:55] User: for cloud run service vllm-gemma-3-1b-it, is there a way to execute or cache the "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): " part during cloud build stage or during the first cold start, so that subsequent cold starts can be shorter?
[2025-10-15 03:47:55] Bot: Of course. As an expert in Google Cloud Run performance optimization, I can analyze your logs and provide a detailed solution to your query about caching CUDA graphs and reducing cold start times for your vLLm service.

### Executive Summary

Your observation is correct: a significant portion of your service's cold start is dedicated to JIT (Just-In-Time) compilation and optimization steps, including the "Capturing CUDA graphs" you identified.

The logs reveal that this CUDA graph capture is part of a much larger `torch.compile` process that takes nearly a minute. The key issue is that the output of this compilation is written to an ephemeral cache directory (`/root/.cache/...`) inside the container, which is lost every time a new instance starts.

**The most effective solution is to persist this cache directory on a shared network file system (NFS) using Google Cloud Filestore.** This allows a new instance to reuse the cache created by a previous one, dramatically reducing the startup time for all subsequent cold starts after the very first one.

---

### 1. Detailed Log Analysis

I've analyzed the startup sequence from your logs for instance `0069c7a...`. The timeline below highlights the key performance bottlenecks during a cold start:

| Timestamp (UTC) | Duration | Log Message & Analysis |
| :--- | :--- | :--- |
| `19:15:25` | ~1.9s | `(EngineCore_0) INFO ... Model loading took 1.9147 GiB and 1.868946 seconds` <br> **Analysis:** The initial model weight loading from the container image is very fast. This is not the bottleneck. |
| `19:15:36` | **~11.1s** | `(EngineCore_0) INFO ... Dynamo bytecode transform time: 11.09 s` <br> **Analysis:** This is the first major step of `torch.compile`, where the Python code is analyzed. |
| `19:16:20` | **~43.2s** | `(EngineCore_0) INFO ... Compiling a graph for dynamic shape takes 43.16 s` <br> **Analysis:** This is the longest phase, where PyTorch's Inductor backend compiles the model operations into optimized Triton kernels for the GPU. |
| `19:16:38` | **(~54s total)** | `(EngineCore_0) INFO ... torch.compile takes 54.25 s in total` <br> **Analysis:** This confirms the total time spent in JIT compilation. This is the primary contributor to your long cold start. |
| `19:17:44` - `19:17:46`| **~2.5s** | `(EngineCore_0) Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%` <br> **Analysis:** This is the step you identified. It captures the compiled GPU kernels into a CUDA graph to reduce launch overhead during inference. It's relatively fast compared to the compilation that precedes it. |
| `19:17:46` | (~141s total) | `(EngineCore_0) INFO ... init engine (profile, create kv cache, warmup model) took 141.08 seconds` <br> **Analysis:** The total engine initialization, dominated by `torch.compile`, takes over two minutes. |
| `19:17:49` | - | `(APIServer) INFO: Application startup complete.` <br> **Analysis:** The instance is now ready to serve traffic. |

**Key Findings:**

1.  **`torch.compile` is the main bottleneck:** While the CUDA graph capture takes ~3 seconds, the entire `torch.compile` process takes **~54 seconds**. This compilation happens *inside* the running container when the vLLM engine initializes.
2.  **Caching is happening, but it's ephemeral:** The logs clearly show that a cache is being used:
    > `INFO ... Using cache directory: /root/.cache/vllm/torch_compile_cache/90a1db546d/rank_0_0/backbone for vLLM's torch.compile`
    This is the crucial piece of information. Because Cloud Run instances have ephemeral filesystems, this `/root/.cache` directory is created from scratch and is empty for every new instance, forcing the expensive compilation to run every single time.

### 2. The Core Problem & Proposed Solution

Your request to cache the CUDA graph is on the right track, but we need to cache the entire output of the `torch.compile` process to see a significant improvement. It is **not possible to perform this step during the Cloud Build stage** because Cloud Build workers do not have GPUs, and these compilation artifacts are specific to the GPU hardware and drivers available in the Cloud Run execution environment.

The solution is to make the cache directory (`/root/.cache`) persistent across different container instances.

---

### 3. Actionable Recommendations

Here are two concrete, actionable solutions to drastically reduce your cold start times.

#### Recommendation 1: Persist the Torch Compile Cache with Cloud Filestore (Best for Cost-Efficiency)

This is the most effective and robust solution. By mounting an NFS share to your Cloud Run service, the first instance that starts will perform the slow compilation and save the results to the share. Every subsequent instance will find the cache already populated and will skip the ~54-second compilation step, reducing the startup time to seconds.

**Steps:**

1.  **Create a Filestore Instance:** Create a Basic tier Filestore instance in the same region as your Cloud Run service (`us-central1`). A small 1TB instance is the minimum size, which is sufficient.
    ```bash
    gcloud filestore instances create vllm-cache-store \
        --project=globalbiting-dev \
        --zone=us-central1-a \
        --tier=BASIC_HDD \
        --file-share=name="vllm_cache",capacity=1TB \
        --network=name="default"
    ```

2.  **Create a Serverless VPC Access Connector:** Your Cloud Run service needs this connector to communicate with the Filestore instance on your VPC network.
    ```bash
    gcloud compute networks vpc-access connectors create vllm-connector \
        --project=globalbiting-dev \
        --region=us-central1 \
        --range=10.8.0.0/28
    ```

3.  **Deploy Cloud Run with the NFS Mount:** Re-deploy your service, mounting the Filestore share to the `/root/.cache` directory. Replace `[FILESTORE_IP_ADDRESS]` with the IP address of your Filestore instance.

    ```bash
    gcloud run deploy vllm-gemma-3-1b-it \
        --project=globalbiting-dev \
        --image gcr.io/globalbiting-dev/vllm-gemma-3-1b-it:latest \
        --region us-central1 \
        --vpc-connector vllm-connector \
        --update-env-vars="VLLM_TORCH_COMPILE_CACHE_DIR=/root/.cache/vllm/torch_compile_cache" \
        --add-volume=name=vllm-cache,type=cloud-filestore,location=[FILESTORE_IP_ADDRESS]:/vllm_cache \
        --add-volume-mount=volume=vllm-cache,mount-path=/root/.cache \
        # ... include your other flags like --cpu, --memory, --concurrency, etc.
    ```
    *   `--add-volume`: Defines the NFS volume, pointing to your Filestore IP and share name.
    *   `--add-volume-mount`: Mounts the defined volume to the `/root/.cache` path inside the container.

After deploying this, the first request will still be slow, but all subsequent cold starts will be dramatically faster.

#### Recommendation 2: Set Minimum Instances to 1 (Best for Zero Cold Starts)

If your service has consistent traffic and you want to eliminate cold starts entirely, the simplest solution is to configure a minimum number of instances to always keep warm.

**Steps:**

1.  **Re-deploy your service with the `--min-instances` flag:**
    ```bash
    gcloud run deploy vllm-gemma-3-1b-it \
        --project=globalbiting-dev \
        --image gcr.io/globalbiting-dev/vllm-gemma-3-1b-it:latest \
        --region us-central1 \
        --min-instances=1 \
        # ... include your other flags
    ```

**Trade-off:** This approach is the fastest for users, as there will never be a cold start (unless you scale beyond your warm instances). However, it incurs costs for keeping at least one instance running 24/7, even with no traffic. The Filestore solution (Recommendation 1) allows your service to scale to zero while still providing fast subsequent cold starts. You can also combine both approaches for the best of both worlds.
--------------------
[2025-10-15 03:48:19] User: please create github issue in repository vllm-container-prewarm containing details for solution 1, as a feature enhancement
[2025-10-15 03:48:19] Bot: Of course. As an expert in Google Cloud Run performance optimization, I can analyze your logs and provide a detailed solution to your query about caching CUDA graphs and reducing cold start times for your vLLm service.

### Executive Summary

Your observation is correct: a significant portion of your service's cold start is dedicated to JIT (Just-In-Time) compilation and optimization steps, including the "Capturing CUDA graphs" you identified.

The logs reveal that this CUDA graph capture is part of a much larger `torch.compile` process that takes nearly a minute. The key issue is that the output of this compilation is written to an ephemeral cache directory (`/root/.cache/...`) inside the container, which is lost every time a new instance starts.

**The most effective solution is to persist this cache directory on a shared network file system (NFS) using Google Cloud Filestore.** This allows a new instance to reuse the cache created by a previous one, dramatically reducing the startup time for all subsequent cold starts after the very first one.

---

### 1. Detailed Log Analysis

I've analyzed the startup sequence from your logs for instance `0069c7a...`. The timeline below highlights the key performance bottlenecks during a cold start:

| Timestamp (UTC) | Duration | Log Message & Analysis |
| :--- | :--- | :--- |
| `19:15:25` | ~1.9s | `(EngineCore_0) INFO ... Model loading took 1.9147 GiB and 1.868946 seconds` <br> **Analysis:** The initial model weight loading from the container image is very fast. This is not the bottleneck. |
| `19:15:36` | **~11.1s** | `(EngineCore_0) INFO ... Dynamo bytecode transform time: 11.09 s` <br> **Analysis:** This is the first major step of `torch.compile`, where the Python code is analyzed. |
| `19:16:20` | **~43.2s** | `(EngineCore_0) INFO ... Compiling a graph for dynamic shape takes 43.16 s` <br> **Analysis:** This is the longest phase, where PyTorch's Inductor backend compiles the model operations into optimized Triton kernels for the GPU. |
| `19:16:38` | **(~54s total)** | `(EngineCore_0) INFO ... torch.compile takes 54.25 s in total` <br> **Analysis:** This confirms the total time spent in JIT compilation. This is the primary contributor to your long cold start. |
| `19:17:44` - `19:17:46`| **~2.5s** | `(EngineCore_0) Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%` <br> **Analysis:** This is the step you identified. It captures the compiled GPU kernels into a CUDA graph to reduce launch overhead during inference. It's relatively fast compared to the compilation that precedes it. |
| `19:17:46` | (~141s total) | `(EngineCore_0) INFO ... init engine (profile, create kv cache, warmup model) took 141.08 seconds` <br> **Analysis:** The total engine initialization, dominated by `torch.compile`, takes over two minutes. |
| `19:17:49` | - | `(APIServer) INFO: Application startup complete.` <br> **Analysis:** The instance is now ready to serve traffic. |

**Key Findings:**

1.  **`torch.compile` is the main bottleneck:** While the CUDA graph capture takes ~3 seconds, the entire `torch.compile` process takes **~54 seconds**. This compilation happens *inside* the running container when the vLLM engine initializes.
2.  **Caching is happening, but it's ephemeral:** The logs clearly show that a cache is being used:
    > `INFO ... Using cache directory: /root/.cache/vllm/torch_compile_cache/90a1db546d/rank_0_0/backbone for vLLM's torch.compile`
    This is the crucial piece of information. Because Cloud Run instances have ephemeral filesystems, this `/root/.cache` directory is created from scratch and is empty for every new instance, forcing the expensive compilation to run every single time.

### 2. The Core Problem & Proposed Solution

Your request to cache the CUDA graph is on the right track, but we need to cache the entire output of the `torch.compile` process to see a significant improvement. It is **not possible to perform this step during the Cloud Build stage** because Cloud Build workers do not have GPUs, and these compilation artifacts are specific to the GPU hardware and drivers available in the Cloud Run execution environment.

The solution is to make the cache directory (`/root/.cache`) persistent across different container instances.

---

### 3. Actionable Recommendations

Here are two concrete, actionable solutions to drastically reduce your cold start times.

#### Recommendation 1: Persist the Torch Compile Cache with Cloud Filestore (Best for Cost-Efficiency)

This is the most effective and robust solution. By mounting an NFS share to your Cloud Run service, the first instance that starts will perform the slow compilation and save the results to the share. Every subsequent instance will find the cache already populated and will skip the ~54-second compilation step, reducing the startup time to seconds.

**Steps:**

1.  **Create a Filestore Instance:** Create a Basic tier Filestore instance in the same region as your Cloud Run service (`us-central1`). A small 1TB instance is the minimum size, which is sufficient.
    ```bash
    gcloud filestore instances create vllm-cache-store \
        --project=globalbiting-dev \
        --zone=us-central1-a \
        --tier=BASIC_HDD \
        --file-share=name="vllm_cache",capacity=1TB \
        --network=name="default"
    ```

2.  **Create a Serverless VPC Access Connector:** Your Cloud Run service needs this connector to communicate with the Filestore instance on your VPC network.
    ```bash
    gcloud compute networks vpc-access connectors create vllm-connector \
        --project=globalbiting-dev \
        --region=us-central1 \
        --range=10.8.0.0/28
    ```

3.  **Deploy Cloud Run with the NFS Mount:** Re-deploy your service, mounting the Filestore share to the `/root/.cache` directory. Replace `[FILESTORE_IP_ADDRESS]` with the IP address of your Filestore instance.

    ```bash
    gcloud run deploy vllm-gemma-3-1b-it \
        --project=globalbiting-dev \
        --image gcr.io/globalbiting-dev/vllm-gemma-3-1b-it:latest \
        --region us-central1 \
        --vpc-connector vllm-connector \
        --update-env-vars="VLLM_TORCH_COMPILE_CACHE_DIR=/root/.cache/vllm/torch_compile_cache" \
        --add-volume=name=vllm-cache,type=cloud-filestore,location=[FILESTORE_IP_ADDRESS]:/vllm_cache \
        --add-volume-mount=volume=vllm-cache,mount-path=/root/.cache \
        # ... include your other flags like --cpu, --memory, --concurrency, etc.
    ```
    *   `--add-volume`: Defines the NFS volume, pointing to your Filestore IP and share name.
    *   `--add-volume-mount`: Mounts the defined volume to the `/root/.cache` path inside the container.

After deploying this, the first request will still be slow, but all subsequent cold starts will be dramatically faster.

#### Recommendation 2: Set Minimum Instances to 1 (Best for Zero Cold Starts)

If your service has consistent traffic and you want to eliminate cold starts entirely, the simplest solution is to configure a minimum number of instances to always keep warm.

**Steps:**

1.  **Re-deploy your service with the `--min-instances` flag:**
    ```bash
    gcloud run deploy vllm-gemma-3-1b-it \
        --project=globalbiting-dev \
        --image gcr.io/globalbiting-dev/vllm-gemma-3-1b-it:latest \
        --region us-central1 \
        --min-instances=1 \
        # ... include your other flags
    ```

**Trade-off:** This approach is the fastest for users, as there will never be a cold start (unless you scale beyond your warm instances). However, it incurs costs for keeping at least one instance running 24/7, even with no traffic. The Filestore solution (Recommendation 1) allows your service to scale to zero while still providing fast subsequent cold starts. You can also combine both approaches for the best of both worlds.
--------------------
[2025-10-15 03:50:22] User: for cloud run service vllm-gemma-3-1b-it, is there a way to execute or cache the "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): " part during cloud build stage or during the first cold start, so that subsequent cold starts can be shorter?
[2025-10-15 03:50:22] Bot: Of course. As an expert in Google Cloud Run performance optimization, I can analyze these logs and provide a detailed solution for your `vllm-gemma-3-1b-it` service.

Let's break down your query and the log data.

### 1. Log Analysis and Problem Identification

Your question is spot-on. The "Capturing CUDA graphs" step is a one-time cost per instance startup that optimizes subsequent inference calls by reducing kernel launch overhead. The logs clearly show this process occurring during a cold start.

By analyzing the timestamps in your logs, we can construct a timeline of a new instance starting up:

*   `19:15:25` - Model loading finishes (takes ~2 seconds).
*   `19:15:36` - PyTorch Dynamo bytecode transform finishes (takes **~11 seconds**).
*   `19:16:20` - Graph compilation for dynamic shapes finishes (takes **~43 seconds**).
*   `19:16:38` - The entire `torch.compile` process finishes (total time: **~54 seconds**). This is a major contributor to the cold start time.
*   `19:17:43` - KV Cache initialization begins.
*   `19:17:44` to `19:17:46` - The **"Capturing CUDA graphs"** process runs (takes **~3 seconds**).
*   `19:17:46` - The entire vLLM engine initialization finishes (total time: **141 seconds**, or ~2.3 minutes).
*   `19:17:49` - The application is ready and the startup probe succeeds.

**Key Findings from the Logs:**

1.  **CUDA Graph Capturing is a Minor Factor:** The specific step you asked about, "Capturing CUDA graphs," takes approximately **3 seconds**. While optimizing this is good, it's not the primary bottleneck.
2.  **The Real Bottleneck is `torch.compile`:** The logs clearly show that the `torch.compile` process (which includes the "Dynamo bytecode transform" and "Compiling a graph for dynamic shape") is taking around **54 seconds**. This JIT (Just-In-Time) compilation is the most significant part of your cold start latency.
3.  **Startup is JIT-Bound, Not I/O-Bound:** The model loading itself is very fast (~2 seconds), indicating the model is likely already baked into your container image. The delay is due to computation (compilation and optimization) happening on the instance when it first starts.

### 2. Answering Your Core Question: Caching and the Build Stage

> "...is there a way to execute or cache the 'Capturing CUDA graphs' part during cloud build stage or during the first cold start, so that subsequent cold starts can be shorter?"

This is an excellent question, and the answer reveals a fundamental constraint of GPU-based serverless environments.

**Why it can't be done in Cloud Build:**
Cloud Build provides a CPU-only environment. Both `torch.compile` (for a CUDA backend) and CUDA graph capturing are GPU-specific operations. They require a live NVIDIA GPU and the corresponding CUDA drivers to inspect the hardware architecture and compile optimized code (kernels) for it. Therefore, this step cannot be performed during the image build process.

**Why it's not automatically cached across cold starts:**
Each Cloud Run instance runs in a sandbox with an ephemeral filesystem. When an instance handles requests and then scales down to zero, its entire state, including any in-memory objects or cached files on its local disk, is lost. When a new request comes in, a brand new instance is created, and it must re-run the entire initialization process from scratch.

This means the `torch.compile` cache and the captured CUDA graphs are lost every time an instance shuts down.

---

### 3. Actionable Recommendations for Optimization

Here are concrete, actionable steps to dramatically reduce your cold start times, ordered from simplest to most advanced.

#### Recommendation 1: Mitigate Cold Starts with Minimum Instances (Most Effective)

The most straightforward and effective way to eliminate cold starts is to prevent your service from scaling to zero.

*   **Action:** Configure your Cloud Run service to have a `min-instances` setting of **1**.
*   **How it Works:** This ensures that at least one container instance is always running, warm, and ready to accept requests. This instance will have already gone through the lengthy `torch.compile` and CUDA graph capture process. When traffic arrives, it is served immediately without any initialization delay.
*   **Trade-off:** You will be billed for the time the instance is idle (vCPU, memory, and GPU). However, for latency-sensitive production applications, this is often the necessary and accepted cost.

```bash
# Example gcloud command to set min-instances
gcloud run services update vllm-gemma-3-1b-it \
  --region us-central1 \
  --min-instances=1
```

#### Recommendation 2: Use Startup CPU Boost

This feature allocates more CPU power during instance startup and for the first 10 seconds of request processing, which can speed up the CPU-bound parts of initialization.

*   **Action:** Enable the "CPU boost" option in your Cloud Run service configuration.
*   **How it Works:** While much of the delay is GPU-related, parts of the `torch.compile` process and general application startup (loading Python modules, etc.) are CPU-intensive. A boost here can shorten the overall startup time. This is best used in combination with other strategies.

```bash
# Example gcloud command to enable startup CPU boost
gcloud run services update vllm-gemma-3-1b-it \
  --region us-central1 \
  --[no-]cpu-boost
```

#### Recommendation 3 (Advanced): Persist the `torch.compile` Cache with Cloud Filestore

This is the most direct solution to the root cause of the `torch.compile` delay on *subsequent* cold starts. You can create a persistent, shared cache for the compilation artifacts.

*   **Action:**
    1.  Provision a **Google Cloud Filestore** (NFS) instance in the same VPC and region as your Cloud Run service.
    2.  Mount this Filestore volume to your Cloud Run service (e.g., at `/mnt/cache`).
    3.  Set the following environment variable in your Cloud Run service to tell PyTorch where to store its compilation cache:
        *   `TORCH_COMPILE_CACHE_DIR=/mnt/cache/torch_compile_cache`
*   **How it Works:**
    *   The **very first cold start** will still be slow (141+ seconds) as it runs `torch.compile` and writes the compiled artifacts to the persistent NFS share.
    *   However, every **subsequent cold start** (i.e., when a new instance is created) will find the cache already populated on the shared volume. It will load the pre-compiled artifacts directly, skipping the ~54-second compilation step and dramatically reducing the startup time.
*   **Note:** This adds the cost and complexity of a Filestore instance but directly solves the JIT compilation problem for a scale-to-zero workload.

#### Recommendation 4 (Minor): Set `TORCH_CUDA_ARCH_LIST`

Your logs show the warning: `TORCH_CUDA_ARCH_LIST is not set`. While not a major issue, fixing this is good practice.

*   **Action:** Set the `TORCH_CUDA_ARCH_LIST` environment variable to match the compute capability of the GPU you are using on Cloud Run (e.g., L4 GPUs are `8.9`).
*   **How it Works:** This tells PyTorch to only compile for the specific GPU architecture it will be running on, rather than all possible architectures, which can slightly reduce compilation time.

```bash
# In your Dockerfile or Cloud Run environment variables
ENV TORCH_CUDA_ARCH_LIST="8.9"
```

### Summary of Recommendations

1.  **For Immediate Production Use:** Use **`min-instances=1`**. This is the standard industry practice for eliminating cold starts on latency-sensitive ML models.
2.  **To Optimize the Cold Start Itself:** Implement the **Cloud Filestore caching** strategy (Recommendation 3). This is the technically "correct" way to solve the recurring compilation delay in a serverless environment. Combine this with **Startup CPU Boost** for best results.
3.  **Good Housekeeping:** Set the `TORCH_CUDA_ARCH_LIST` environment variable to silence the warning and ensure optimal compilation.
--------------------
[2025-10-15 03:50:57] User: please create github issue in repository vllm-container-prewarm containing details for solution 1, as a feature enhancement
[2025-10-15 03:50:57] Bot: Of course. As an expert in Google Cloud Run performance optimization, I can analyze these logs and provide a detailed solution for your `vllm-gemma-3-1b-it` service.

Let's break down your query and the log data.

### 1. Log Analysis and Problem Identification

Your question is spot-on. The "Capturing CUDA graphs" step is a one-time cost per instance startup that optimizes subsequent inference calls by reducing kernel launch overhead. The logs clearly show this process occurring during a cold start.

By analyzing the timestamps in your logs, we can construct a timeline of a new instance starting up:

*   `19:15:25` - Model loading finishes (takes ~2 seconds).
*   `19:15:36` - PyTorch Dynamo bytecode transform finishes (takes **~11 seconds**).
*   `19:16:20` - Graph compilation for dynamic shapes finishes (takes **~43 seconds**).
*   `19:16:38` - The entire `torch.compile` process finishes (total time: **~54 seconds**). This is a major contributor to the cold start time.
*   `19:17:43` - KV Cache initialization begins.
*   `19:17:44` to `19:17:46` - The **"Capturing CUDA graphs"** process runs (takes **~3 seconds**).
*   `19:17:46` - The entire vLLM engine initialization finishes (total time: **141 seconds**, or ~2.3 minutes).
*   `19:17:49` - The application is ready and the startup probe succeeds.

**Key Findings from the Logs:**

1.  **CUDA Graph Capturing is a Minor Factor:** The specific step you asked about, "Capturing CUDA graphs," takes approximately **3 seconds**. While optimizing this is good, it's not the primary bottleneck.
2.  **The Real Bottleneck is `torch.compile`:** The logs clearly show that the `torch.compile` process (which includes the "Dynamo bytecode transform" and "Compiling a graph for dynamic shape") is taking around **54 seconds**. This JIT (Just-In-Time) compilation is the most significant part of your cold start latency.
3.  **Startup is JIT-Bound, Not I/O-Bound:** The model loading itself is very fast (~2 seconds), indicating the model is likely already baked into your container image. The delay is due to computation (compilation and optimization) happening on the instance when it first starts.

### 2. Answering Your Core Question: Caching and the Build Stage

> "...is there a way to execute or cache the 'Capturing CUDA graphs' part during cloud build stage or during the first cold start, so that subsequent cold starts can be shorter?"

This is an excellent question, and the answer reveals a fundamental constraint of GPU-based serverless environments.

**Why it can't be done in Cloud Build:**
Cloud Build provides a CPU-only environment. Both `torch.compile` (for a CUDA backend) and CUDA graph capturing are GPU-specific operations. They require a live NVIDIA GPU and the corresponding CUDA drivers to inspect the hardware architecture and compile optimized code (kernels) for it. Therefore, this step cannot be performed during the image build process.

**Why it's not automatically cached across cold starts:**
Each Cloud Run instance runs in a sandbox with an ephemeral filesystem. When an instance handles requests and then scales down to zero, its entire state, including any in-memory objects or cached files on its local disk, is lost. When a new request comes in, a brand new instance is created, and it must re-run the entire initialization process from scratch.

This means the `torch.compile` cache and the captured CUDA graphs are lost every time an instance shuts down.

---

### 3. Actionable Recommendations for Optimization

Here are concrete, actionable steps to dramatically reduce your cold start times, ordered from simplest to most advanced.

#### Recommendation 1: Mitigate Cold Starts with Minimum Instances (Most Effective)

The most straightforward and effective way to eliminate cold starts is to prevent your service from scaling to zero.

*   **Action:** Configure your Cloud Run service to have a `min-instances` setting of **1**.
*   **How it Works:** This ensures that at least one container instance is always running, warm, and ready to accept requests. This instance will have already gone through the lengthy `torch.compile` and CUDA graph capture process. When traffic arrives, it is served immediately without any initialization delay.
*   **Trade-off:** You will be billed for the time the instance is idle (vCPU, memory, and GPU). However, for latency-sensitive production applications, this is often the necessary and accepted cost.

```bash
# Example gcloud command to set min-instances
gcloud run services update vllm-gemma-3-1b-it \
  --region us-central1 \
  --min-instances=1
```

#### Recommendation 2: Use Startup CPU Boost

This feature allocates more CPU power during instance startup and for the first 10 seconds of request processing, which can speed up the CPU-bound parts of initialization.

*   **Action:** Enable the "CPU boost" option in your Cloud Run service configuration.
*   **How it Works:** While much of the delay is GPU-related, parts of the `torch.compile` process and general application startup (loading Python modules, etc.) are CPU-intensive. A boost here can shorten the overall startup time. This is best used in combination with other strategies.

```bash
# Example gcloud command to enable startup CPU boost
gcloud run services update vllm-gemma-3-1b-it \
  --region us-central1 \
  --[no-]cpu-boost
```

#### Recommendation 3 (Advanced): Persist the `torch.compile` Cache with Cloud Filestore

This is the most direct solution to the root cause of the `torch.compile` delay on *subsequent* cold starts. You can create a persistent, shared cache for the compilation artifacts.

*   **Action:**
    1.  Provision a **Google Cloud Filestore** (NFS) instance in the same VPC and region as your Cloud Run service.
    2.  Mount this Filestore volume to your Cloud Run service (e.g., at `/mnt/cache`).
    3.  Set the following environment variable in your Cloud Run service to tell PyTorch where to store its compilation cache:
        *   `TORCH_COMPILE_CACHE_DIR=/mnt/cache/torch_compile_cache`
*   **How it Works:**
    *   The **very first cold start** will still be slow (141+ seconds) as it runs `torch.compile` and writes the compiled artifacts to the persistent NFS share.
    *   However, every **subsequent cold start** (i.e., when a new instance is created) will find the cache already populated on the shared volume. It will load the pre-compiled artifacts directly, skipping the ~54-second compilation step and dramatically reducing the startup time.
*   **Note:** This adds the cost and complexity of a Filestore instance but directly solves the JIT compilation problem for a scale-to-zero workload.

#### Recommendation 4 (Minor): Set `TORCH_CUDA_ARCH_LIST`

Your logs show the warning: `TORCH_CUDA_ARCH_LIST is not set`. While not a major issue, fixing this is good practice.

*   **Action:** Set the `TORCH_CUDA_ARCH_LIST` environment variable to match the compute capability of the GPU you are using on Cloud Run (e.g., L4 GPUs are `8.9`).
*   **How it Works:** This tells PyTorch to only compile for the specific GPU architecture it will be running on, rather than all possible architectures, which can slightly reduce compilation time.

```bash
# In your Dockerfile or Cloud Run environment variables
ENV TORCH_CUDA_ARCH_LIST="8.9"
```

### Summary of Recommendations

1.  **For Immediate Production Use:** Use **`min-instances=1`**. This is the standard industry practice for eliminating cold starts on latency-sensitive ML models.
2.  **To Optimize the Cold Start Itself:** Implement the **Cloud Filestore caching** strategy (Recommendation 3). This is the technically "correct" way to solve the recurring compilation delay in a serverless environment. Combine this with **Startup CPU Boost** for best results.
3.  **Good Housekeeping:** Set the `TORCH_CUDA_ARCH_LIST` environment variable to silence the warning and ensure optimal compilation.
--------------------
[2025-10-15 03:58:51] User: for cloud run service vllm-gemma-3-1b-it, is there a way to execute or cache the "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): " part during cloud build stage or during the first cold start, so that subsequent cold starts can be shorter?
[2025-10-15 03:58:51] Bot: Of course. As an expert in Google Cloud Run performance optimization, I can analyze these logs and provide a detailed solution to your query about caching the CUDA graph capture.

Excellent question. You've correctly identified a specific, technical step in the vLLM startup process that contributes to cold start latency. Let's break down the logs and formulate a comprehensive strategy.

### 1. Log Analysis: Deconstructing the Cold Start

The provided logs give us a clear timeline of a cold start for your `vllm-gemma-3-1b-it` service. By examining the timestamps, we can identify the major contributors to the startup time.

| Timestamp (UTC) | Duration | Log Message | Analysis |
| :--- | :--- | :--- | :--- |
| 19:15:25 | - | `Model loading took ... 1.868946 seconds` | Loading the model weights from disk into memory is very fast. This implies you have likely baked the model into your container image, which is a best practice. |
| 19:15:36 | ~11 sec | `Dynamo bytecode transform time: 11.09 s` | This is the first part of the `torch.compile` process, where PyTorch's Dynamo frontend analyzes the Python bytecode. |
| 19:16:20 | **~43 sec** | `Compiling a graph for dynamic shape takes 43.16 s` | This is the core of `torch.compile`, where the model's operations are Just-In-Time (JIT) compiled into optimized GPU kernels. **This is a major bottleneck.** |
| 19:16:38 | **~54 sec** | `torch.compile takes 54.25 s in total` | This log confirms the total time spent on compilation. This is the single largest contributor to your cold start time. |
| 19:17:44 | - | `Capturing CUDA graphs ... 0%` | This is where the step you asked about begins. |
| 19:17:46 | **~3 sec** | `Graph capturing finished in 3 secs` | The actual CUDA graph capture is relatively quick, taking about 3 seconds. |
| 19:17:46 | **~141 sec** | `init engine ... took 141.08 seconds` | This is the total time for vLLM's engine to become fully initialized, including model loading, `torch.compile`, KV cache allocation, and CUDA graph capture. |
| 19:17:49 | - | `Application startup complete.` | The web server is now ready to accept requests. |

**Key Findings from the Logs:**

1.  **Your observation is correct:** The "Capturing CUDA graphs" step happens during the instance startup (specifically, during the first inference call which acts as a warmup).
2.  **It's not the main bottleneck:** While it takes ~3 seconds, it is dwarfed by the **~54 seconds** required for `torch.compile`.
3.  **The total cold start is long:** From the start of the process to the application being ready takes over two minutes (141 seconds).

### 2. Answering Your Core Question: Can CUDA Graphs Be Cached?

Your question is whether the CUDA graph can be captured during the build stage or cached from the first cold start to benefit subsequent ones.

**The short answer is no, this is not feasible for the following technical reasons:**

*   **Runtime vs. Build Time:** CUDA graphs are runtime artifacts. They are captured by recording a sequence of operations on a *live, initialized GPU*. The Cloud Build environment does not have access to the same powerful GPU (like an L4 or T4) that your Cloud Run service uses. Therefore, there is no GPU context to capture a graph from during the build.
*   **Hardware and Driver Dependency:** A captured CUDA graph is highly specific to the GPU architecture, the CUDA driver version, and the current CUDA context. An artifact generated in one environment cannot be reliably replayed in another.
*   **Ephemeral Nature of Cloud Run:** Each cold start in Cloud Run (when scaling from zero) creates a brand new, clean container instance. The memory, disk, and GPU state from any previous instance is completely wiped. This means that even if a graph were captured on the *first* cold start, it would be lost when that instance shuts down. The *next* cold start would be on a new instance that would have to recapture it.

### 3. Actionable Recommendations for Optimization

While you can't cache the CUDA graph itself across ephemeral instances, we can address the root causes of the long cold start. The goal is to make every step of the 141-second initialization process faster or happen less frequently.

Here is a prioritized list of solutions:

---

#### Recommendation 1: Eliminate Cold Starts with Minimum Instances (Most Effective)

The most direct way to solve the cold start problem is to prevent the service from scaling to zero.

*   **Action:** Set the minimum number of instances for your Cloud Run service to `1`.
    ```bash
    gcloud run services update vllm-gemma-3-1b-it \
      --region us-central1 \
      --min-instances 1
    ```
*   **Why it works:** This keeps one container instance constantly running and "warm." The lengthy initialization (including `torch.compile` and CUDA graph capture) happens only once when this instance is first provisioned. All subsequent requests are served immediately by this warm instance.
*   **Trade-off:** This has cost implications, as you will be billed for one GPU instance continuously, even when it's idle. This is the classic performance-vs-cost trade-off.

---

#### Recommendation 2: Persist the `torch.compile` Cache with a Network File System (Best for Reducing Cold Start Time)

The logs show that the `torch.compile` step is your biggest bottleneck (~54 seconds). Fortunately, PyTorch allows you to cache the results of this compilation. We can persist this cache across cold starts using a network file system.

*   **Action:**
    1.  **Provision a Google Cloud Filestore (NFS) instance** in the same region as your Cloud Run service. A basic tier instance is sufficient.
    2.  **Mount the Filestore instance** to your Cloud Run service. You can mount the `fileserver` share to a path like `/cache`.
    3.  **Set an environment variable** in your Cloud Run service to tell PyTorch where to store its cache. The log indicates vLLM uses a specific path, but you can override it. Set the environment variable `TORCH_COMPILE_CACHE_DIR` to a directory on your new mount:
        *   **Environment Variable:** `TORCH_COMPILE_CACHE_DIR=/cache/torch_compile_cache`
*   **Why it works:**
    *   On the very first cold start, the ~54 second compilation will still occur, and the resulting optimized kernels will be saved to the persistent Filestore volume.
    *   On **all subsequent cold starts**, the container will find the pre-compiled kernels on the NFS mount and load them directly, skipping the entire compilation step. This will reduce your startup time by nearly a minute.

---

#### Recommendation 3: Ensure Model Weights are Baked into the Container

Your model loading time is already fast (1.8s), which suggests you are likely doing this already. However, it's crucial to confirm.

*   **Action:** In your `Dockerfile`, ensure you download the model from Hugging Face during the build process and copy it into the final image. Then, point your vLLM start command to the local path.
    ```dockerfile
    # Dockerfile
    ...
    # Download the model during the build
    RUN huggingface-cli download google/gemma-3-1b-it --local-dir /app/gemma-3-1b-it

    # Your CMD or ENTRYPOINT should use the local path
    CMD ["python", "-m", "vllm.entrypoints.openai.api_server", "--model", "/app/gemma-3-1b-it", ...]
    ```
*   **Why it works:** This completely eliminates any network latency from downloading the model on startup, making the process faster and more reliable.

---

#### Recommendation 4: Enable Startup CPU Boost

This is a general Cloud Run feature that can help speed up any CPU-bound initialization tasks.

*   **Action:** Enable the "CPU boost" option when deploying your service.
    ```bash
    gcloud run services update vllm-gemma-3-1b-it \
      --region us-central1 \
      --cpu-boost
    ```
*   **Why it works:** Cloud Run allocates a larger amount of CPU during the container's startup phase (before it starts serving traffic). While `torch.compile` is GPU-intensive, many parts of the initialization process are CPU-bound and can benefit from this boost, shaving off a few valuable seconds.

### Summary of Solutions

| Solution | Impact on Cold Start | Implementation Effort | Cost Impact |
| :--- | :--- | :--- | :--- |
| **Set `min-instances=1`** | **Eliminates** cold starts entirely. | Low | High (always-on GPU cost) |
| **Cache `torch.compile` on NFS** | **Reduces** cold start time by ~54 seconds. | Medium | Medium (Filestore cost) |
| **Bake Model into Image** | **Reduces** cold start by eliminating download time. | Low | Negligible |
| **Enable Startup CPU Boost** | **Slightly Reduces** cold start time. | Low | None |

By implementing these recommendations, especially **caching the `torch.compile` output** and **setting `min-instances=1`**, you can drastically improve the performance and responsiveness of your vLLM service on Cloud Run.
--------------------
[2025-10-15 03:59:32] User: please create github issue in repository vllm-container-prewarm containing details for solution 1, as a feature enhancemen
[2025-10-15 03:59:32] Bot: Of course. As an expert in Google Cloud Run performance optimization, I can analyze these logs and provide a detailed solution to your query about caching the CUDA graph capture.

Excellent question. You've correctly identified a specific, technical step in the vLLM startup process that contributes to cold start latency. Let's break down the logs and formulate a comprehensive strategy.

### 1. Log Analysis: Deconstructing the Cold Start

The provided logs give us a clear timeline of a cold start for your `vllm-gemma-3-1b-it` service. By examining the timestamps, we can identify the major contributors to the startup time.

| Timestamp (UTC) | Duration | Log Message | Analysis |
| :--- | :--- | :--- | :--- |
| 19:15:25 | - | `Model loading took ... 1.868946 seconds` | Loading the model weights from disk into memory is very fast. This implies you have likely baked the model into your container image, which is a best practice. |
| 19:15:36 | ~11 sec | `Dynamo bytecode transform time: 11.09 s` | This is the first part of the `torch.compile` process, where PyTorch's Dynamo frontend analyzes the Python bytecode. |
| 19:16:20 | **~43 sec** | `Compiling a graph for dynamic shape takes 43.16 s` | This is the core of `torch.compile`, where the model's operations are Just-In-Time (JIT) compiled into optimized GPU kernels. **This is a major bottleneck.** |
| 19:16:38 | **~54 sec** | `torch.compile takes 54.25 s in total` | This log confirms the total time spent on compilation. This is the single largest contributor to your cold start time. |
| 19:17:44 | - | `Capturing CUDA graphs ... 0%` | This is where the step you asked about begins. |
| 19:17:46 | **~3 sec** | `Graph capturing finished in 3 secs` | The actual CUDA graph capture is relatively quick, taking about 3 seconds. |
| 19:17:46 | **~141 sec** | `init engine ... took 141.08 seconds` | This is the total time for vLLM's engine to become fully initialized, including model loading, `torch.compile`, KV cache allocation, and CUDA graph capture. |
| 19:17:49 | - | `Application startup complete.` | The web server is now ready to accept requests. |

**Key Findings from the Logs:**

1.  **Your observation is correct:** The "Capturing CUDA graphs" step happens during the instance startup (specifically, during the first inference call which acts as a warmup).
2.  **It's not the main bottleneck:** While it takes ~3 seconds, it is dwarfed by the **~54 seconds** required for `torch.compile`.
3.  **The total cold start is long:** From the start of the process to the application being ready takes over two minutes (141 seconds).

### 2. Answering Your Core Question: Can CUDA Graphs Be Cached?

Your question is whether the CUDA graph can be captured during the build stage or cached from the first cold start to benefit subsequent ones.

**The short answer is no, this is not feasible for the following technical reasons:**

*   **Runtime vs. Build Time:** CUDA graphs are runtime artifacts. They are captured by recording a sequence of operations on a *live, initialized GPU*. The Cloud Build environment does not have access to the same powerful GPU (like an L4 or T4) that your Cloud Run service uses. Therefore, there is no GPU context to capture a graph from during the build.
*   **Hardware and Driver Dependency:** A captured CUDA graph is highly specific to the GPU architecture, the CUDA driver version, and the current CUDA context. An artifact generated in one environment cannot be reliably replayed in another.
*   **Ephemeral Nature of Cloud Run:** Each cold start in Cloud Run (when scaling from zero) creates a brand new, clean container instance. The memory, disk, and GPU state from any previous instance is completely wiped. This means that even if a graph were captured on the *first* cold start, it would be lost when that instance shuts down. The *next* cold start would be on a new instance that would have to recapture it.

### 3. Actionable Recommendations for Optimization

While you can't cache the CUDA graph itself across ephemeral instances, we can address the root causes of the long cold start. The goal is to make every step of the 141-second initialization process faster or happen less frequently.

Here is a prioritized list of solutions:

---

#### Recommendation 1: Eliminate Cold Starts with Minimum Instances (Most Effective)

The most direct way to solve the cold start problem is to prevent the service from scaling to zero.

*   **Action:** Set the minimum number of instances for your Cloud Run service to `1`.
    ```bash
    gcloud run services update vllm-gemma-3-1b-it \
      --region us-central1 \
      --min-instances 1
    ```
*   **Why it works:** This keeps one container instance constantly running and "warm." The lengthy initialization (including `torch.compile` and CUDA graph capture) happens only once when this instance is first provisioned. All subsequent requests are served immediately by this warm instance.
*   **Trade-off:** This has cost implications, as you will be billed for one GPU instance continuously, even when it's idle. This is the classic performance-vs-cost trade-off.

---

#### Recommendation 2: Persist the `torch.compile` Cache with a Network File System (Best for Reducing Cold Start Time)

The logs show that the `torch.compile` step is your biggest bottleneck (~54 seconds). Fortunately, PyTorch allows you to cache the results of this compilation. We can persist this cache across cold starts using a network file system.

*   **Action:**
    1.  **Provision a Google Cloud Filestore (NFS) instance** in the same region as your Cloud Run service. A basic tier instance is sufficient.
    2.  **Mount the Filestore instance** to your Cloud Run service. You can mount the `fileserver` share to a path like `/cache`.
    3.  **Set an environment variable** in your Cloud Run service to tell PyTorch where to store its cache. The log indicates vLLM uses a specific path, but you can override it. Set the environment variable `TORCH_COMPILE_CACHE_DIR` to a directory on your new mount:
        *   **Environment Variable:** `TORCH_COMPILE_CACHE_DIR=/cache/torch_compile_cache`
*   **Why it works:**
    *   On the very first cold start, the ~54 second compilation will still occur, and the resulting optimized kernels will be saved to the persistent Filestore volume.
    *   On **all subsequent cold starts**, the container will find the pre-compiled kernels on the NFS mount and load them directly, skipping the entire compilation step. This will reduce your startup time by nearly a minute.

---

#### Recommendation 3: Ensure Model Weights are Baked into the Container

Your model loading time is already fast (1.8s), which suggests you are likely doing this already. However, it's crucial to confirm.

*   **Action:** In your `Dockerfile`, ensure you download the model from Hugging Face during the build process and copy it into the final image. Then, point your vLLM start command to the local path.
    ```dockerfile
    # Dockerfile
    ...
    # Download the model during the build
    RUN huggingface-cli download google/gemma-3-1b-it --local-dir /app/gemma-3-1b-it

    # Your CMD or ENTRYPOINT should use the local path
    CMD ["python", "-m", "vllm.entrypoints.openai.api_server", "--model", "/app/gemma-3-1b-it", ...]
    ```
*   **Why it works:** This completely eliminates any network latency from downloading the model on startup, making the process faster and more reliable.

---

#### Recommendation 4: Enable Startup CPU Boost

This is a general Cloud Run feature that can help speed up any CPU-bound initialization tasks.

*   **Action:** Enable the "CPU boost" option when deploying your service.
    ```bash
    gcloud run services update vllm-gemma-3-1b-it \
      --region us-central1 \
      --cpu-boost
    ```
*   **Why it works:** Cloud Run allocates a larger amount of CPU during the container's startup phase (before it starts serving traffic). While `torch.compile` is GPU-intensive, many parts of the initialization process are CPU-bound and can benefit from this boost, shaving off a few valuable seconds.

### Summary of Solutions

| Solution | Impact on Cold Start | Implementation Effort | Cost Impact |
| :--- | :--- | :--- | :--- |
| **Set `min-instances=1`** | **Eliminates** cold starts entirely. | Low | High (always-on GPU cost) |
| **Cache `torch.compile` on NFS** | **Reduces** cold start time by ~54 seconds. | Medium | Medium (Filestore cost) |
| **Bake Model into Image** | **Reduces** cold start by eliminating download time. | Low | Negligible |
| **Enable Startup CPU Boost** | **Slightly Reduces** cold start time. | Low | None |

By implementing these recommendations, especially **caching the `torch.compile` output** and **setting `min-instances=1`**, you can drastically improve the performance and responsiveness of your vLLM service on Cloud Run.
--------------------
[2025-10-15 04:05:19] User: for cloud run service vllm-gemma-3-1b-it, is there a way to execute or cache the "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): " part during cloud build stage or during the first cold start, so that subsequent cold starts can be shorter?
[2025-10-15 04:05:19] Bot: Of course. As an expert in Google Cloud Run log analysis and performance optimization, I can provide a detailed breakdown and actionable solutions based on your query and the provided logs.

### Executive Summary (TL;DR)

Your analysis is correctâ€”the "Capturing CUDA graphs" step is part of the cold start process. However, the logs reveal a much larger bottleneck: **`torch.compile` is taking over 54 seconds**, which is the primary contributor to your long cold start time.

Unfortunately, you **cannot execute or cache the CUDA graph capturing during the Cloud Build stage** because Cloud Build environments (without special configuration) lack GPUs. Caching the graph *between* cold starts is also not feasible because each new Cloud Run instance is ephemeral and starts from a clean state.

Here are the most effective solutions:

1.  **Primary Solution (Immediate Fix):** Use the `--min-instances=1` setting on your Cloud Run service. This keeps one instance constantly warm and ready to serve requests, effectively eliminating cold starts for your baseline traffic at the cost of keeping one instance running.
2.  **Advanced Solution (Targeted Optimization):** The biggest win will come from **pre-populating the `torch.compile` cache during your container build process**. By using a GPU-enabled Cloud Build worker, you can run a dummy inference during the build, which generates the compilation cache. This cache is then baked into your container image, slashing over 54 seconds from every subsequent cold start.

---

### Detailed Log Analysis

Let's break down the cold start sequence for instance `0069c7a988fa...` using the timestamps in your logs. This reveals the true bottlenecks.

| Timestamp | Duration | Log Message | Analysis |
| :--- | :--- | :--- | :--- |
| `19:15:25` | ~1.9s | `Model loading took 1.9147 GiB and 1.868946 seconds` | Model weights are loaded quickly. This indicates they are likely already included in your container image, which is excellent practice. |
| `19:15:36` | **~11.1s** | `Dynamo bytecode transform time: 11.09 s` | This is the first part of the PyTorch 2.0+ compilation process (`torch.compile`) that vLLM uses for optimization. |
| `19:16:20` | **~43.2s** | `Compiling a graph for dynamic shape takes 43.16 s` | This is the main graph compilation step. It is extremely time-consuming and the **primary bottleneck** in your cold start. |
| `19:16:38` | **(Total: 54.2s)** | `torch.compile takes 54.25 s in total` | This log confirms the total time spent just on model compilation. This happens *before* the CUDA graph capture. |
| `19:17:44` | **~2-3s** | `Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):` | This is the step you identified. vLLM is warming up the GPU by running sample inputs to capture the CUDA kernels for faster execution later. While part of the startup, it's minor compared to the compilation. |
| `19:17:46` | **(Total: 141s)** | `init engine (profile, create kv cache, warmup model) took 141.08 seconds` | This is the total time from starting the vLLM engine to it being ready. The `torch.compile` step accounts for a significant portion of this ~2.3-minute startup. |
| `19:17:49` | | `Application startup complete.` | The instance is now ready to accept requests. |

**Conclusion from Logs:** Your intuition about optimizing startup is correct, but the target should be the `torch.compile` step, not just the CUDA graph capture. The ~3 seconds for graph capture is dwarfed by the **~54 seconds** for compilation.

---

### Why Caching the CUDA Graph is Not Feasible in this Context

1.  **During Cloud Build:** Standard Cloud Build workers do not have GPUs. CUDA graph capturing is a runtime operation that requires initializing the model on a physical GPU and running operations on it. This cannot be done in a CPU-only build environment.
2.  **Between Cold Starts:** Cloud Run instances are ephemeral. When an instance scales down to zero and a new request arrives, a brand-new instance is started from the original container image. Any state generated during the previous instance's runtime (like a captured graph in GPU memory or a cache file on the temporary filesystem) is lost.

---

### Actionable Solutions & Recommendations

Here are concrete steps to dramatically improve your service's cold start performance.

#### Recommendation 1: Keep an Instance Warm with `min-instances` (The Standard Solution)

This is the most direct way to eliminate cold starts. By setting a minimum number of instances, you ensure that at least one container is always running, fully initialized, and ready to serve requests instantly.

*   **Pros:** Completely eliminates cold starts for incoming traffic. Simple to configure.
*   **Cons:** Incurs costs for keeping the instance(s) running 24/7, even with no traffic.

**How to implement:**
Use the following `gcloud` command to update your service:

```bash
gcloud run services update vllm-gemma-3-1b-it \
  --platform managed \
  --region us-central1 \
  --min-instances=1
```

#### Recommendation 2: Pre-populate the `torch.compile` Cache in Cloud Build (The Advanced Solution)

This is the most powerful optimization for reducing the cold start time itself. The logs show vLLM uses a cache directory: `/root/.cache/vllm/torch_compile_cache/...`. We can populate this cache during the image build process by using a Cloud Build worker with a GPU.

*   **Pros:** Drastically reduces cold start time (by ~54 seconds in your case) for every new instance without the 24/7 cost of `min-instances`.
*   **Cons:** More complex setup; requires using a GPU-enabled machine type for your Cloud Build, which has an associated cost.

**How to implement:**

1.  **Create a Warmup Script:**
    Create a simple Python script named `warmup.py` to initialize the vLLM engine. This will trigger the compilation and caching.

    ```python
    # warmup.py
    from vllm import LLM

    print("Starting vLLM engine warmup to populate torch.compile cache...")

    # Initialize the LLM engine with the same parameters you use at runtime.
    # This MUST match your service's entrypoint arguments.
    # The model will be downloaded by this step if not present.
    llm = LLM(model="google/gemma-2-9b-it", trust_remote_code=True)
    
    # Optional: You can perform a single dummy inference to be thorough,
    # but engine initialization is usually sufficient to trigger compilation.
    # llm.generate("Warmup prompt")

    print("Warmup complete. Cache should be populated.")
    ```

2.  **Modify your `Dockerfile`:**
    Add a `RUN` step in your Dockerfile to execute this warmup script. This ensures the cache is generated and added as a layer to your final image.

    ```dockerfile
    # Your existing Dockerfile setup...
    # (e.g., FROM nvidia/cuda:12.1.1-devel-ubuntu22.04)
    # (COPY requirements.txt, RUN pip install ...)

    COPY warmup.py .

    # This is the key step: Run the warmup script to generate the cache.
    # The resulting files in /root/.cache/ will be saved in the container image.
    RUN python3 warmup.py

    # Your existing CMD or ENTRYPOINT to start the vLLM server
    CMD ["python3", "-m", "vllm.entrypoints.openai.api_server", "--model", "google/gemma-2-9b-it"]
    ```

3.  **Submit the Build using a GPU Worker:**
    When you build this image with Cloud Build, specify a machine type that includes a GPU.

    ```bash
    gcloud builds submit . --config=cloudbuild.yaml
    ```
    
    And your `cloudbuild.yaml` file would look like this:

    ```yaml
    steps:
    - name: 'gcr.io/cloud-builders/docker'
      args: ['build', '-t', 'us-central1-docker.pkg.dev/YOUR-PROJECT/YOUR-REPO/vllm-gemma-3-1b-it', '.']
    
    images:
    - 'us-central1-docker.pkg.dev/YOUR-PROJECT/YOUR-REPO/vllm-gemma-3-1b-it'
    
    options:
      # Specify a machine type with a GPU to run the build
      machineType: 'N1_HIGHCPU_8_NVIDIA_TESLA_T4'
    ```

#### Recommendation 3: Enable Startup CPU Boost

This feature allocates more CPU power to an instance during its startup and for 10 seconds after, which can speed up CPU-bound initialization tasks like loading libraries and decompressing data.

*   **Pros:** Can slightly reduce overall startup time. No extra cost.
*   **Cons:** Won't affect the GPU-bound `torch.compile` or CUDA graph capture steps, so its impact will be limited but still positive.

**How to implement:**

```bash
gcloud run services update vllm-gemma-3-1b-it \
  --platform managed \
  --region us-central1 \
  --cpu-boost
```

By combining these strategies, particularly **#1** or **#2**, you can significantly improve the performance and responsiveness of your `vllm-gemma-3-1b-it` service.
--------------------
[2025-10-15 04:06:04] User: please create github issue in repository vllm-container-prewarm containing details for solution 1, as a feature enhancemen
[2025-10-15 04:06:04] Bot: Of course. As an expert in Google Cloud Run log analysis and performance optimization, I can provide a detailed breakdown and actionable solutions based on your query and the provided logs.

### Executive Summary (TL;DR)

Your analysis is correctâ€”the "Capturing CUDA graphs" step is part of the cold start process. However, the logs reveal a much larger bottleneck: **`torch.compile` is taking over 54 seconds**, which is the primary contributor to your long cold start time.

Unfortunately, you **cannot execute or cache the CUDA graph capturing during the Cloud Build stage** because Cloud Build environments (without special configuration) lack GPUs. Caching the graph *between* cold starts is also not feasible because each new Cloud Run instance is ephemeral and starts from a clean state.

Here are the most effective solutions:

1.  **Primary Solution (Immediate Fix):** Use the `--min-instances=1` setting on your Cloud Run service. This keeps one instance constantly warm and ready to serve requests, effectively eliminating cold starts for your baseline traffic at the cost of keeping one instance running.
2.  **Advanced Solution (Targeted Optimization):** The biggest win will come from **pre-populating the `torch.compile` cache during your container build process**. By using a GPU-enabled Cloud Build worker, you can run a dummy inference during the build, which generates the compilation cache. This cache is then baked into your container image, slashing over 54 seconds from every subsequent cold start.

---

### Detailed Log Analysis

Let's break down the cold start sequence for instance `0069c7a988fa...` using the timestamps in your logs. This reveals the true bottlenecks.

| Timestamp | Duration | Log Message | Analysis |
| :--- | :--- | :--- | :--- |
| `19:15:25` | ~1.9s | `Model loading took 1.9147 GiB and 1.868946 seconds` | Model weights are loaded quickly. This indicates they are likely already included in your container image, which is excellent practice. |
| `19:15:36` | **~11.1s** | `Dynamo bytecode transform time: 11.09 s` | This is the first part of the PyTorch 2.0+ compilation process (`torch.compile`) that vLLM uses for optimization. |
| `19:16:20` | **~43.2s** | `Compiling a graph for dynamic shape takes 43.16 s` | This is the main graph compilation step. It is extremely time-consuming and the **primary bottleneck** in your cold start. |
| `19:16:38` | **(Total: 54.2s)** | `torch.compile takes 54.25 s in total` | This log confirms the total time spent just on model compilation. This happens *before* the CUDA graph capture. |
| `19:17:44` | **~2-3s** | `Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):` | This is the step you identified. vLLM is warming up the GPU by running sample inputs to capture the CUDA kernels for faster execution later. While part of the startup, it's minor compared to the compilation. |
| `19:17:46` | **(Total: 141s)** | `init engine (profile, create kv cache, warmup model) took 141.08 seconds` | This is the total time from starting the vLLM engine to it being ready. The `torch.compile` step accounts for a significant portion of this ~2.3-minute startup. |
| `19:17:49` | | `Application startup complete.` | The instance is now ready to accept requests. |

**Conclusion from Logs:** Your intuition about optimizing startup is correct, but the target should be the `torch.compile` step, not just the CUDA graph capture. The ~3 seconds for graph capture is dwarfed by the **~54 seconds** for compilation.

---

### Why Caching the CUDA Graph is Not Feasible in this Context

1.  **During Cloud Build:** Standard Cloud Build workers do not have GPUs. CUDA graph capturing is a runtime operation that requires initializing the model on a physical GPU and running operations on it. This cannot be done in a CPU-only build environment.
2.  **Between Cold Starts:** Cloud Run instances are ephemeral. When an instance scales down to zero and a new request arrives, a brand-new instance is started from the original container image. Any state generated during the previous instance's runtime (like a captured graph in GPU memory or a cache file on the temporary filesystem) is lost.

---

### Actionable Solutions & Recommendations

Here are concrete steps to dramatically improve your service's cold start performance.

#### Recommendation 1: Keep an Instance Warm with `min-instances` (The Standard Solution)

This is the most direct way to eliminate cold starts. By setting a minimum number of instances, you ensure that at least one container is always running, fully initialized, and ready to serve requests instantly.

*   **Pros:** Completely eliminates cold starts for incoming traffic. Simple to configure.
*   **Cons:** Incurs costs for keeping the instance(s) running 24/7, even with no traffic.

**How to implement:**
Use the following `gcloud` command to update your service:

```bash
gcloud run services update vllm-gemma-3-1b-it \
  --platform managed \
  --region us-central1 \
  --min-instances=1
```

#### Recommendation 2: Pre-populate the `torch.compile` Cache in Cloud Build (The Advanced Solution)

This is the most powerful optimization for reducing the cold start time itself. The logs show vLLM uses a cache directory: `/root/.cache/vllm/torch_compile_cache/...`. We can populate this cache during the image build process by using a Cloud Build worker with a GPU.

*   **Pros:** Drastically reduces cold start time (by ~54 seconds in your case) for every new instance without the 24/7 cost of `min-instances`.
*   **Cons:** More complex setup; requires using a GPU-enabled machine type for your Cloud Build, which has an associated cost.

**How to implement:**

1.  **Create a Warmup Script:**
    Create a simple Python script named `warmup.py` to initialize the vLLM engine. This will trigger the compilation and caching.

    ```python
    # warmup.py
    from vllm import LLM

    print("Starting vLLM engine warmup to populate torch.compile cache...")

    # Initialize the LLM engine with the same parameters you use at runtime.
    # This MUST match your service's entrypoint arguments.
    # The model will be downloaded by this step if not present.
    llm = LLM(model="google/gemma-2-9b-it", trust_remote_code=True)
    
    # Optional: You can perform a single dummy inference to be thorough,
    # but engine initialization is usually sufficient to trigger compilation.
    # llm.generate("Warmup prompt")

    print("Warmup complete. Cache should be populated.")
    ```

2.  **Modify your `Dockerfile`:**
    Add a `RUN` step in your Dockerfile to execute this warmup script. This ensures the cache is generated and added as a layer to your final image.

    ```dockerfile
    # Your existing Dockerfile setup...
    # (e.g., FROM nvidia/cuda:12.1.1-devel-ubuntu22.04)
    # (COPY requirements.txt, RUN pip install ...)

    COPY warmup.py .

    # This is the key step: Run the warmup script to generate the cache.
    # The resulting files in /root/.cache/ will be saved in the container image.
    RUN python3 warmup.py

    # Your existing CMD or ENTRYPOINT to start the vLLM server
    CMD ["python3", "-m", "vllm.entrypoints.openai.api_server", "--model", "google/gemma-2-9b-it"]
    ```

3.  **Submit the Build using a GPU Worker:**
    When you build this image with Cloud Build, specify a machine type that includes a GPU.

    ```bash
    gcloud builds submit . --config=cloudbuild.yaml
    ```
    
    And your `cloudbuild.yaml` file would look like this:

    ```yaml
    steps:
    - name: 'gcr.io/cloud-builders/docker'
      args: ['build', '-t', 'us-central1-docker.pkg.dev/YOUR-PROJECT/YOUR-REPO/vllm-gemma-3-1b-it', '.']
    
    images:
    - 'us-central1-docker.pkg.dev/YOUR-PROJECT/YOUR-REPO/vllm-gemma-3-1b-it'
    
    options:
      # Specify a machine type with a GPU to run the build
      machineType: 'N1_HIGHCPU_8_NVIDIA_TESLA_T4'
    ```

#### Recommendation 3: Enable Startup CPU Boost

This feature allocates more CPU power to an instance during its startup and for 10 seconds after, which can speed up CPU-bound initialization tasks like loading libraries and decompressing data.

*   **Pros:** Can slightly reduce overall startup time. No extra cost.
*   **Cons:** Won't affect the GPU-bound `torch.compile` or CUDA graph capture steps, so its impact will be limited but still positive.

**How to implement:**

```bash
gcloud run services update vllm-gemma-3-1b-it \
  --platform managed \
  --region us-central1 \
  --cpu-boost
```

By combining these strategies, particularly **#1** or **#2**, you can significantly improve the performance and responsiveness of your `vllm-gemma-3-1b-it` service.
--------------------
[2025-10-15 04:14:03] User: for cloud run service vllm-gemma-3-1b-it, is there a way to execute or cache the "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): " part during cloud build stage or during the first cold start, so that subsequent cold starts can be shorter?
[2025-10-15 04:14:03] Bot: Of course. As an expert in Google Cloud Run performance optimization, I can analyze these logs and provide a detailed solution.

### Executive Summary

Based on your query and a thorough analysis of the logs, here is the key finding:

Your assumption that "Capturing CUDA graphs" is the primary bottleneck is incorrect. While it is part of the startup process, the logs clearly show it only takes **~3 seconds**. The actual major bottleneck is the `torch.compile` step, which took **~54 seconds**. The entire vLLM engine initialization took **141 seconds (over 2 minutes)**.

You cannot cache the CUDA graph state across cold starts because it's stored in the GPU memory of an ephemeral container instance. However, you can **eliminate the 54-second `torch.compile` delay** by pre-compiling the CUDA kernels during your container build process. This, combined with setting a minimum number of instances, will dramatically reduce cold start times.

---

### Detailed Log Analysis

Let's break down the relevant log entries in chronological order to understand the cold start process for your `vllm-gemma-3-1b-it` service.

1.  **`INFO 10-14 12:15:36 ... Using cache directory: /root/.cache/vllm/torch_compile_cache/...`**
    *   **What it means:** vLLM is attempting to use a cache for `torch.compile`. However, because this path is inside the container's ephemeral filesystem, this cache is empty on a cold start and is lost when the instance shuts down.

2.  **`INFO 10-14 12:16:38 [monitor.py:34] torch.compile takes 54.25 s in total`**
    *   **This is the primary bottleneck.** Before vLLM can capture the CUDA graph, PyTorch's `torch.compile` feature is JIT (Just-In-Time) compiling the model's Python code into optimized low-level CUDA kernels. This compilation is happening at runtime and is taking nearly a minute.

3.  **`stderr Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|...| 67/67 [00:02<00:00, 26.86it/s]`**
    *   **What it means:** This is the step you asked about. vLLM is running dummy inputs through the compiled model to capture the sequence of kernel launches. This allows for faster execution during actual inference by replaying the graph instead of launching kernels one by one.

4.  **`INFO 10-14 12:17:46 [gpu_model_runner.py:2708] Graph capturing finished in 3 secs, took 1.64 GiB`**
    *   **This is a crucial data point.** The actual CUDA graph capturing process is extremely fast, taking only **3 seconds**. This confirms it is not the cause of your long cold starts.

5.  **`INFO 10-14 12:17:46 [core.py:214] init engine (profile, create kv cache, warmup model) took 141.08 seconds`**
    *   **This is the total cold start duration from the application's perspective.** From the moment vLLM started initializing the engine until it was ready, it took over two minutes. The 54-second `torch.compile` is the largest single contributor to this time.

6.  **`INFO STARTUP TCP probe succeeded after 1 attempt...` at `19:17:49`**
    *   This log entry from Cloud Run confirms that the container is now listening on its port and considered "ready" to serve traffic, approximately 3 seconds after the vLLM engine finished its initialization.

### Answering Your Core Question

> "...is there a way to execute or cache the "Capturing CUDA graphs" part during cloud build stage or during the first cold start, so that subsequent cold starts can be shorter?"

The direct answer is **no, for two main reasons**:

1.  **No GPU in Cloud Build:** Cloud Build, the service that builds your container images, does not provide GPU access. Capturing a CUDA graph requires initializing the model on a GPU, which is not possible during the build step.
2.  **Ephemeral State:** The captured CUDA graph resides in the GPU memory of the specific Cloud Run instance. When Cloud Run scales your service down to zero, the instance is terminated, and its entire state (including GPU memory) is lost. The next request will trigger a new cold start on a fresh instance, which must recapture the graph.

---

### Actionable Recommendations for Optimization

While you can't cache the graph itself across instances, you can solve the root problem: the long initialization time.

#### Recommendation 1: Eliminate Runtime Compilation with `TORCH_CUDA_ARCH_LIST` (High Impact)

The 54-second `torch.compile` delay can be moved from runtime to build time. You need to tell PyTorch which GPU architecture to build for. For Cloud Run GPUs, this is typically the NVIDIA L4.

**Action:** Set the `TORCH_CUDA_ARCH_LIST` environment variable in your `Dockerfile` before installing vLLM. The compute capability for an L4 GPU is `8.9`.

**Example `Dockerfile` modification:**

```dockerfile
# Set this before your 'pip install vllm' command
# This tells the build process to pre-compile kernels for the L4 GPU architecture (sm_89)
ENV TORCH_CUDA_ARCH_LIST="8.9"

# Force vLLM to reinstall to trigger the build with the new ENV VAR
# Use --force-reinstall and --no-cache-dir to ensure it rebuilds the kernels
RUN pip install --upgrade --force-reinstall --no-cache-dir vllm

# Your other Dockerfile commands (COPY, CMD, etc.)
...
```

By doing this, the CUDA kernels will be pre-compiled and included in your container image. When the container starts, `torch.compile` will find the cached kernels and load them instantly, saving you ~54 seconds on every cold start.

#### Recommendation 2: Use `min-instances` to Avoid Cold Starts (Highest Impact for User Experience)

The most effective way to eliminate cold start latency for your users is to ensure there is always at least one container instance running.

**Action:** Configure your Cloud Run service to have a minimum number of instances.

```bash
gcloud run services update vllm-gemma-3-1b-it \
  --region us-central1 \
  --min-instances 1
```

*   **Pros:** Guarantees that at least one instance is always warm and ready to serve requests instantly. This completely hides the cold start problem from users as long as traffic doesn't exceed the capacity of the warm instances.
*   **Cons:** You will be billed for the time the instance is idle (vCPU, memory, and GPU). For a GPU service, this is a significant cost consideration, but it provides the best performance.

#### Recommendation 3: Enable Startup CPU Boost

This feature allocates more CPU power during the instance startup phase, which can accelerate CPU-bound initialization tasks like loading libraries and decompressing the model before it's moved to the GPU.

**Action:** Deploy your service with the `--cpu-boost` flag.

```bash
gcloud run deploy vllm-gemma-3-1b-it \
  --image gcr.io/your-project/your-image \
  --region us-central1 \
  --cpu-boost
  # ... other flags
```

This will likely only shave a few seconds off your total initialization time since the main bottleneck is GPU-related, but it's a simple and effective optimization.

### Summary of Steps

1.  **Primary Action:** Modify your `Dockerfile` to set `ENV TORCH_CUDA_ARCH_LIST="8.9"` before installing vLLM to pre-compile the kernels. This will target the root cause of the delay identified in the logs.
2.  **Secondary Action:** For the best user experience and to completely eliminate cold starts, set `--min-instances 1` on your Cloud Run service, being mindful of the cost implications.
3.  **Tertiary Action:** Enable `--cpu-boost` for a small but easy performance gain during initialization.
--------------------
[2025-10-15 04:14:34] User: please create github issue in repository vllm-container-prewarm containing details for solution 1, as a feature enhancemen
[2025-10-15 04:14:34] Bot: Of course. As an expert in Google Cloud Run performance optimization, I can analyze these logs and provide a detailed solution.

### Executive Summary

Based on your query and a thorough analysis of the logs, here is the key finding:

Your assumption that "Capturing CUDA graphs" is the primary bottleneck is incorrect. While it is part of the startup process, the logs clearly show it only takes **~3 seconds**. The actual major bottleneck is the `torch.compile` step, which took **~54 seconds**. The entire vLLM engine initialization took **141 seconds (over 2 minutes)**.

You cannot cache the CUDA graph state across cold starts because it's stored in the GPU memory of an ephemeral container instance. However, you can **eliminate the 54-second `torch.compile` delay** by pre-compiling the CUDA kernels during your container build process. This, combined with setting a minimum number of instances, will dramatically reduce cold start times.

---

### Detailed Log Analysis

Let's break down the relevant log entries in chronological order to understand the cold start process for your `vllm-gemma-3-1b-it` service.

1.  **`INFO 10-14 12:15:36 ... Using cache directory: /root/.cache/vllm/torch_compile_cache/...`**
    *   **What it means:** vLLM is attempting to use a cache for `torch.compile`. However, because this path is inside the container's ephemeral filesystem, this cache is empty on a cold start and is lost when the instance shuts down.

2.  **`INFO 10-14 12:16:38 [monitor.py:34] torch.compile takes 54.25 s in total`**
    *   **This is the primary bottleneck.** Before vLLM can capture the CUDA graph, PyTorch's `torch.compile` feature is JIT (Just-In-Time) compiling the model's Python code into optimized low-level CUDA kernels. This compilation is happening at runtime and is taking nearly a minute.

3.  **`stderr Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|...| 67/67 [00:02<00:00, 26.86it/s]`**
    *   **What it means:** This is the step you asked about. vLLM is running dummy inputs through the compiled model to capture the sequence of kernel launches. This allows for faster execution during actual inference by replaying the graph instead of launching kernels one by one.

4.  **`INFO 10-14 12:17:46 [gpu_model_runner.py:2708] Graph capturing finished in 3 secs, took 1.64 GiB`**
    *   **This is a crucial data point.** The actual CUDA graph capturing process is extremely fast, taking only **3 seconds**. This confirms it is not the cause of your long cold starts.

5.  **`INFO 10-14 12:17:46 [core.py:214] init engine (profile, create kv cache, warmup model) took 141.08 seconds`**
    *   **This is the total cold start duration from the application's perspective.** From the moment vLLM started initializing the engine until it was ready, it took over two minutes. The 54-second `torch.compile` is the largest single contributor to this time.

6.  **`INFO STARTUP TCP probe succeeded after 1 attempt...` at `19:17:49`**
    *   This log entry from Cloud Run confirms that the container is now listening on its port and considered "ready" to serve traffic, approximately 3 seconds after the vLLM engine finished its initialization.

### Answering Your Core Question

> "...is there a way to execute or cache the "Capturing CUDA graphs" part during cloud build stage or during the first cold start, so that subsequent cold starts can be shorter?"

The direct answer is **no, for two main reasons**:

1.  **No GPU in Cloud Build:** Cloud Build, the service that builds your container images, does not provide GPU access. Capturing a CUDA graph requires initializing the model on a GPU, which is not possible during the build step.
2.  **Ephemeral State:** The captured CUDA graph resides in the GPU memory of the specific Cloud Run instance. When Cloud Run scales your service down to zero, the instance is terminated, and its entire state (including GPU memory) is lost. The next request will trigger a new cold start on a fresh instance, which must recapture the graph.

---

### Actionable Recommendations for Optimization

While you can't cache the graph itself across instances, you can solve the root problem: the long initialization time.

#### Recommendation 1: Eliminate Runtime Compilation with `TORCH_CUDA_ARCH_LIST` (High Impact)

The 54-second `torch.compile` delay can be moved from runtime to build time. You need to tell PyTorch which GPU architecture to build for. For Cloud Run GPUs, this is typically the NVIDIA L4.

**Action:** Set the `TORCH_CUDA_ARCH_LIST` environment variable in your `Dockerfile` before installing vLLM. The compute capability for an L4 GPU is `8.9`.

**Example `Dockerfile` modification:**

```dockerfile
# Set this before your 'pip install vllm' command
# This tells the build process to pre-compile kernels for the L4 GPU architecture (sm_89)
ENV TORCH_CUDA_ARCH_LIST="8.9"

# Force vLLM to reinstall to trigger the build with the new ENV VAR
# Use --force-reinstall and --no-cache-dir to ensure it rebuilds the kernels
RUN pip install --upgrade --force-reinstall --no-cache-dir vllm

# Your other Dockerfile commands (COPY, CMD, etc.)
...
```

By doing this, the CUDA kernels will be pre-compiled and included in your container image. When the container starts, `torch.compile` will find the cached kernels and load them instantly, saving you ~54 seconds on every cold start.

#### Recommendation 2: Use `min-instances` to Avoid Cold Starts (Highest Impact for User Experience)

The most effective way to eliminate cold start latency for your users is to ensure there is always at least one container instance running.

**Action:** Configure your Cloud Run service to have a minimum number of instances.

```bash
gcloud run services update vllm-gemma-3-1b-it \
  --region us-central1 \
  --min-instances 1
```

*   **Pros:** Guarantees that at least one instance is always warm and ready to serve requests instantly. This completely hides the cold start problem from users as long as traffic doesn't exceed the capacity of the warm instances.
*   **Cons:** You will be billed for the time the instance is idle (vCPU, memory, and GPU). For a GPU service, this is a significant cost consideration, but it provides the best performance.

#### Recommendation 3: Enable Startup CPU Boost

This feature allocates more CPU power during the instance startup phase, which can accelerate CPU-bound initialization tasks like loading libraries and decompressing the model before it's moved to the GPU.

**Action:** Deploy your service with the `--cpu-boost` flag.

```bash
gcloud run deploy vllm-gemma-3-1b-it \
  --image gcr.io/your-project/your-image \
  --region us-central1 \
  --cpu-boost
  # ... other flags
```

This will likely only shave a few seconds off your total initialization time since the main bottleneck is GPU-related, but it's a simple and effective optimization.

### Summary of Steps

1.  **Primary Action:** Modify your `Dockerfile` to set `ENV TORCH_CUDA_ARCH_LIST="8.9"` before installing vLLM to pre-compile the kernels. This will target the root cause of the delay identified in the logs.
2.  **Secondary Action:** For the best user experience and to completely eliminate cold starts, set `--min-instances 1` on your Cloud Run service, being mindful of the cost implications.
3.  **Tertiary Action:** Enable `--cpu-boost` for a small but easy performance gain during initialization.
--------------------
